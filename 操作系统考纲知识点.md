# 操作系统考纲知识点

<aside>
🔖 目录

</aside>

# 一、总体要求

主要考察学生对操作系统基本概念、原理的理解程度，重点考察操作系统的设计方法与实现技术， 同时能够具备运用所学的操作系统原理、方法与技术分析问题和解决问题的能力。 

# 二、内容

## 1. 操作系统的基本概念

### 1.1 批处理与多道程序设计 ★

- **1.1.1 单道批处理系统**
    
    单道批处理系统是在解决人机矛盾和  CPU 与 I/O 设备速度不匹配矛盾的过程中形成的。系统对作业的处理是成批进行的，但内存中始终保持一道作业。
    
    特征：自动性、顺序性、单道性。
    
    缺点：内存中仅有一道程序，系统中的资源得不到充分的利用。
    
    ---
    
- **1.1.3 多道程序设计和多道批处理系统**
    1. 多道程序设计
        
        为了进一步提高资源的利用率和系统的吞吐量，引入了多道程序技术。多道程序设计技术允许多个程序同时进入内存并允许它们在 CPU 中交替地运行，这些程序共享系统中的各种硬/软件资源。当一道程序因 I/O 请求而暂停运行时，CPU 便立即转去运行另一 道程序。它不采用某些机制来提高某一技术方面的瓶颈问题 ，而让系统的各个组成部分都尽量去“忙”，因此切换任务所花费的时间很少，可实现系统各部件之间的并行工作，使其整体在单位时间内的效率翻倍。
        
        当然，多道批处理系统的设计和实现要比单道系统复杂很多，因为要充分利用各种资源，就要涉及各种资源的调度问题。
        
        注意：引入多道程序技术的前提是系统要具有中断功能。
        
        多道程序设计的特点是多道、宏观上并行、微观上串行。
        
        - ① 多道。计算机内存中同时存放多道相互独立的程序。
        - ② 宏观上并行。同时进入系统的多道程序都处于运行过程中，即它们先后开始各自的运行，但都未运行完毕。
        - ③ 微观上串行。内存中的多道程序轮流占有 CPU，交替执行。
        
        多道程序设计技术的实现需要解决下列问题： 
        
        - ① 如何分配处理器。
        - ② 多道程序的内存分配问题。
        - ③ I/O 设备如何分配。
        - ④ 如何组织和存放大量的程序和数据，以方便用户使用并保证其安全性与一致性。
    2. 多道批处理系统
        
        在批处理系统中采用多道程序设计技术就形成了多道批处理操作系统 。该系统把用户提交的 作业成批地送入计算机内存，然后由作业调度程序自动地选择作业运行。
        
        多道批处理系统的优缺点如下：
        
        - 资源利用率高。引入多道批处理能使多道程序交替运行，以保持 CPU 处于忙碌状态；在内存中装入多道程序可提高内存的利用率；此外还可以提高 I/O 设备的利用率。
        - 系统吞吐量大。能提高系统吞吐量的主要原因可归结为：CPU 和其它资源保持 “忙碌”状态； 仅当作业完成时或运行不下去时才进行切换，系统开销小。
        - 平均周转时间长。由于作业要排队依次进行处理，因而作业的周转时间较长，通常需几个小时，甚至几天。
        - 无交互能力。用户一旦把作业提交给系统后，直至作业完成，用户都不能与自己的作业进行交互，修改和调试程序极不方便。
    
    ---
    

### 1.2 分时系统与实时系统 ★★

- **1.2.1 分时系统**
    
    分时操作系统是指多个用户通过终端同时共享一台主机，这些终端连接在主机上，用户可以同时与主机进行交互操作而互不干扰。
    
    主要特征：
    
    - 多路性（同时性）：指允许多个终端用户同时使用一台计算机，即一台计算机 与若干台终端相连接，终端上的这些用户可以同时或基本同时使用计算机。
    - 交互性：用户能够方便地与系统进行人机对话 ，即用户通过终端采用人机对话的方式直接控制程序运行，与程序进行交互。
    - 独立性：系统中多个用户可以彼此独立地进行操作，互不干扰，单个用户感觉不到别人也在使用这台计算机，好像只有自己单独使用这台计算机一样。
    - 及时性：用户请求能在很短时间内获得响应。分时系统采用时间片轮转方式使一台计算 机同时为多个终端服务，使用户能够对系统的及时响应感到满意 。
    
    ---
    
- **1.2.2 实时系统**
    
    实时系统是指系统能及时响应外部事件的请求，在规定的时间内完成对该事件的处理，并控制所有实时任务协调一致地运行。
    
    若某个动作必须绝对地在规定的时刻（或规定的时间范围）发生，称为硬实时系统（硬实时任务），如飞行器的飞行自动控制系统。若能够接受偶尔违反时间规定且不会引起任何永久性的损害，则称为软实时系统（软实时任务），如飞机订票系统、银行管理系统。
    
    主要特征：及时性、可靠性。
    
    ---
    

### 1.3 操作系统的基本类型与特征 ★

- **1.3.1 操作系统基本类型**
    
    批处理系统、分时系统和实时系统
    
    ---
    
- **1.3.2 操作系统的特征**
    
    操作系统具有并发、共享、虚拟和异步四个基本特征。
    
    - 并发：指两个或多个事件在同一时间间隔内发生。操作系统的并发性是指计算机系统中同时存在多个运行的程序，因此它具有处理和调度多个程序同时执行的能力。在操作系统中，引入进程的目的是使程序能并发执行。
    - 共享：资源共享即共享，是指系统中的资源可供内存中多个并发执行的进程共同使用。实现方式有两种：互斥共享方式、同时访问方式。
        
        并发和共享是操作系统两个最基本的特征，两者之间互为存在的条件：
        
        - ① 资源共享是以程序的并发为条件的，若系统不允许程序并发执行，则自然不存在资源共享问题；
        - ② 若系统不能对资源共享实施有效的管理，则必将影响到程序的并发执行，甚至根本无法并发执行。
    - 虚拟：指把一个物理实体变为若干个逻辑上的对应物。实现方式：时分复用（如处理器的分时共享）、空分复用（如虚拟存储器）。
    - 异步：多道程序环境允许多个程序并发执行，但由于资源有限，进程的执行并不是一贯到底的，而是走走停停的，它以不可预知的速度向前推进，这就是进程的异步性。
    
    ---
    
- **1.3.3 操作系统的目标和功能**
    
    为了给多道程序提供良好的运行环境，操作系统应具有以下几方面的功能：处理机管理、存储器管理、设备管理和文件管理。为了方便用户使用操作系统，还必须向用户提供接口。同时， 操作系统可用来扩充机器，以提供更方便的服务、更高的资源利用率。
    
    1. 操作系统的目标
        
        在计算机系统上配置操作系统，其主要目标是：方便性、有效性、可扩充性和开放性。其中方便性和有效性是设计 OS 时最重要的两个目标。
        
    2. 操作系统的功能
        1. 处理机管理
            
            在多道程序环境下，处理机的分配和运行都以进程（或线程）为基本单位，因而对处理机的管理可归结为对进程的管理。并发是指在计算机内同时运行多个进程，因此进程何时创建、何时撤销、如何管理、如何避免冲突、合理共享就是进程管理的最主要的任务。进程管理的主要功能包括进程控制、进程同步、进程通信、死锁处理、处理机调度等。
            
        2. 存储器管理
            
            存储器管理是为了给多道程序的运行提供良好的环境 ，方便用户使用及提高内存的利用率， 主要包括内存分配与回收、地址映射、内存保护与共享和内存扩充等功能。
            
        3. 文件管理
            
            计算机中的信息都是以文件的形式存在的 ，操作系统中负责文件管理的部分称为文件系统。 文件管理包括文件存储空间的管理、目录管理及文件读写管理和保护等。 
            
        4. 设备管理
            
            设备管理的主要任务是完成用户的 I/O 请求，方便用户使用各种设备，并提高设备的利用率, 主要包括缓冲管理、设备分配、设备处理和虚拟设备等功能。
            
    3. 操作系统与用户之间的接口
        
        为了让用户方便、快捷、可靠地操纵计算机硬件并运行自己的程序，操作系统还提供了用户接口。操作系统提供的接口主要分为两类：一类是命令接口，用户利用这些操作命令来组织和控制作业的执行；另一类是程序接口，编程人员可以使用它们来请求操作系统服务。
        
        ![操作系统作为接口的示意图](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled.png)
        
        操作系统作为接口的示意图
        
        1. 命令接口
            
            使用命令接口进行作业控制的主要方式有两种，即联机控制方式和脱机控制方式。按作业控制方式的不同，可将命令接口分为联机命令接口和脱机命令接口。
            
            联机命令接口又称交互式命令接口，适用于分时或实时系统的接口。它由一组键盘操作命令组成。用户通过控制台或终端输入操作命令，向系统提出各种服务要求。用户每输入一条命令， 控制权就转给操作系统的命令解释程序，然后由命令解释程序解释并执行输入的命令，完成指定 的功能。之后，控制权转回控制台或终端，此时用户又可输入下一条命令。
            
            脱机命令接口又称批处理命令接口，适用于批处理系统，它由一组作业控制命令组成。脱机用户不能直接干预作业的运行，而应事先用相应的作业控制命令写成一份作业操作说明书，连同作业一起提交给系统。系统调度到该作业时，由系统中的命令解释程序逐条解释执行作业说明书 上的命令，从而间接地控制作业的运行。
            
        2. 程序接口
            
            程序接口由一组系统调用（也称广义指令）组成 。用户通过在程序中使用这些系统调用来请求操作系统为其提供服务，如使用各种外部设备、申请分配和回收内存及其他各种要求。
            
            当前最为流行的是图形用户界面（GUI），即图形接口。GUI 最终是通过调用程序接口实现的， 用户通过鼠标和键盘在图形界面上单击或使用快捷键，就能很方便地使用操作系统。严格来说， 图形接口不是操作系统的一部分，但图形接口所调用的系统调用命令是操作系统的一部分。
            
    
    ---
    

### 1.4 并发与并行的概念 ★★★

并发：指两个或多个事件在同一时间间隔内发生。

并行：指两个或多个事件在同一时刻发生。

在多道程序环境下，并发性是指在一段时间内宏观上有多个程序在同时运行，但在单处理机系统中，每一时刻却仅能有一道程序执行，故微观上这些程序只能是分时地交替执行。倘若在计算机系统中有多个处理机，这些可以并发执行的程序便可被分配到多个处理机上，实现并行执行，即利用每个处理机来处理一个可并发执行的程序。这样，多个程序便可同时执行。

### 1.5 操作系统的层次结构与功能模块 ★

- **1.5.1 功能模块（模块化结构 OS、模块化）**
    
    将操作系统按功能精心地划分为若干个具有一定独立性和大 小的模块。每个模块具有某方面的管理功能，如进程管理模块、存储器管理模块、I/0 设备 管理模块等，并仔细地规定好各模块间的接口，使各模块之间能通过接口实现交互。然后再进一步将各模块细分为若干个具有一定功能的子模块，如把进程管理模块又分为进程控制、进程同步等子模块，同样也规定好各子模块之间的接口。若子模块较大，可再进一步将它细分。我们把这种设计方法称为模块－接口法，由此构成的操作系统就是具有模块化结构的操作系统。
    
    优点：提高了操作系统设计的正确性、可理解性和可维护性；增强 OS 的可适应性；加速 OS 的开发过程。
    
    缺点：模块间的接口规定很难满足对接口的实际需求；各模块设计者齐头并进，每个决定无法建立在上一个已验证的正确决定的基础上，因此无法找到一个可靠的决定顺序（无序性）。
    
    衡量模块的独立性的标准：内聚性、耦合度。（模块内部高内聚，模块之间低耦合说明模块独立性好）
    
    ---
    
- **1.5.2 层次结构（分层式结构 OS、分层法）**
    
    为了将模块－接口法中“决定顺序”的无序性变为有序性，引入了有序分层法。分层法是将操作系统分为若干层，最底层（层0）为硬件，最高层（层N）为用户接口，每层只能调用紧邻它的低层的功能和服务（单向依赖）。
    
    优点：易保证系统的正确性；易扩充和易维护性；便于系统的调试和验证，简化了系统的设计和实现。
    
    缺点：系统效率低、合理定义各层比较困难。
    
    ---
    

### 1.6 程序的并发执行与顺序执行 ★

- **1.6.1 程序的顺序执行**
    
    通常，一个应用程序由若干个程序段组成，每一个程序段完成特定的功能，它们在执行时，都需要按照某种先后次序顺序执行，仅当前一程序段执行完后，才运行后一程序段。
    
    程序顺序执行时的特征：
    
    - 顺序性：指处理机严格地按照程序所规定的顺序执行，即每一操作必须在下一个操作开始之前结束；
    - 封闭性：指程序在封闭的环境下运行，即程序运行时独占全机资源，资源的状态（除初始状态外）只有本程序才能改变它，程序一旦开始执行，其执行结果不受外界因素影响；
    - 可再现性：指只要程序执行时的环境和初始条件相同，当程序重复执行时可获得相同的结果。
    
    ---
    
- **1.6.2 程序的并发执行**
    
    程序顺序执行时，系统资源的利用率很低。因而系统引入多道程序技术，使程序或程序段间能并发执行。注意，只有在不存在前趋关系的程序之间才有可能并发执行。
    
    程序并发执行时的特征：
    
    - 间断性：程序在并发执行时，由于它们共享系统资源，以及为完成同一项任务而相互合作，致使在这些并发执行的程序之间形成了相互制约的关系，从而导致并发程序具有“执行——暂停——执行”这种间断性的活动规律。
    - 失去封闭性：当系统中存在着多个可以并发执行的程序时，系统中的各种资源将为它们所共享，而这些资源的状态也由这些程序来改变，致使其中任一程序在运行时，其环境都必然会受到其它程序的影响。
    - 不可再现性：程序在并发执行时，由于失去了封闭性，也将导致其失去可再现性。即程序经过多次执行后， 虽然它们执行时的环境和初始条件相同，但得到的结果却各不相同。
    
    ---
    

## 2. 进程管理

### 2.1 进程：进程控制块、进程的几种基本状态与状态转换（进程的创建、进程的终止、进程的阻塞与唤醒、进程的挂起与激活等）★★★

- **2.1.1 进程的概念和特征**
    1. 进程的概念
        
        在多道程序环境下，程序的执行属于并发执行，此时它们将失去其封闭性，并具有间断性，以及其运行结果不可再现性的特征。由此，决定了通常的程序是不能参与并发执行的，否则，程序的运行也就失去了意义。为了能使程序并发执行，并且可以对并发执行的程序加以描述和控制，由此引入了“进程”的概念。
        
         为了使参与并发执行的每个程序（含数据）都能独立地运行，必须为之配置一个专门的数据结构，称为[进程控制块（Process Control Block，PCB）](https://www.notion.so/d6789a488c934a009190634bba8e1811)。系统利用 PCB 来描述进程的基本情况和运行状态，进而控制和管理进程（PCB 是进程存在的唯一标志）。相应地，由程序段、相关数据段和 PCB 三部分构成了进程实体 （又称进程映像）。所谓创建进程，实质上是创建进程实体中的 PCB；而撤销进程，实质上是撤销进程的 PCB。值得注意的是，进程映像是静态的，进程则是动态的。
        
        从不同的角度，进程可以有不同的定义，比较典型的定义有： 
        
        - 进程是程序的一次执行过程。
        - 进程是一个程序及其数据在处理机上顺序执行时所发生的活动 。
        - 进程是具有独立功能的程序在一个数据集合上运行的过程，它是系统进行资源分配和调度的一个独立单位。
        
        引入进程实体的概念后，可以把传统操作系统中的进程定义为：进程是进程实体的运行过程，是系统进行资源分配和调度的一个独立单位。
        
    2. 进程的特征
        
        进程是由多道程序的并发执行而引出的，它和程序是两个截然不同的概念。进程的基本特征是对比单个程序的顺序执行提出的，也是对进程管理提出的基本要求。
        
        - 动态性：进程是程序的一次执行，它有着创建、活动、暂停、终止等过程，具有一定的生命周期，是动态地产生、变化和消亡的。动态性是进程最基本的特征。
        - 并发性：指多个进程实体同存于内存中，能在一段时间内同时运行。引入进程的目的就是使进程能和其他进程并发执行。并发性是进程的重要特征，也是操作系统的重要特征。
        - 独立性：指进程实体是一个能独立运行、独立获得资源和独立接受调度的基本单位。凡未建立 PCB 的程序，都不能作为一个独立的单位参与运行。
        - 异步性：由于进程的相互制约，使得进程按各自独立的、不可预知的速度向前推进。异步性会导致执行结果的不可再现性，为此在操作系统中必须配置相应的进程同步机制。
    
    ---
    
- **2.1.2 进程控制块（PCB）**
    
    进程创建时，操作系统为它新建一个 PCB，该结构之后常驻内存，任意时刻都可以存取，并在进程结束时删除。PCB 是进程实体的一部分，是进程存在的唯一标志。
    
    进程执行时，系统通过其 PCB 了解进程的现行状态信息，以便操作系统对其进行控制和管理；进程结束时，系统收回其 PCB，该进程随之消亡。
    
    当操作系统欲调度某进程运行时，要从该进程的 PCB 中查出其现行状态及优先级；在调度到某进程后，要根据其 PCB 中所保存的处理机状态信息，设置该进程恢复运行的现场，并根据其 PCB 中的程序和数据的内存始址，找到其程序和数据。
    
    进程在运行过程中，当需要和与之合作的进程实现同步、通信或访问文件时，也需要访问 PCB；当进程由于某种原因而暂停运行时，又需将其断点的处理机环境保存在 PCB 中。可见，在进程的整个生命期中，系统总是通过PCB对进程进行控制的，亦即系统唯有通过进程的 PCB 才能感知到该进程的存在。
    
    PCB 主要包括进程描述信息（含进程标识符）、进程控制和管理信息（含进程调度信息）、资源分配清单和处理机相关信息（含处理机状态信息）等：
    
    ![PCB通常包含的内容](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%201.png)
    
    PCB通常包含的内容
    
    1. 进程描述信息
        1. 进程标识符：标志各个进程，每个进程都有一个唯一的标识号。
        2. 用户标识符：进程归属的用户，用户标识符主要为共享和保护服务。 
    2.  进程控制和管理信息
        1. 进程当前状态：描述进程的状态信息，作为处理机分配调度的依据。
        2. 进程优先级：描述进程抢占处理机的优先级，优先级高的进程可优先获得处理机。 
    3. 资源分配清单
        
        用于说明有关内存地址空间或虚拟地址空间的状况，所打开文件的列表和所使用的输入/输出设备信息。 
        
    4. 处理机相关信息（处理机状态）
        
        也称处理机的上下文，主要指处理机中各寄存器的值，包括通用寄存器、指令计数器 PC、程序状态字 PSW、用户栈指针等。当进程处于执行态时，处理机的许多信息都在寄存器中。当进程被切换时，处理机状态信息都必须保存在相应的PCB中，以便在该进程重新执行时，能从断点继续执行。
        
    
    ![PCB 组成概览](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%202.png)
    
    PCB 组成概览
    
    在一个系统中，通常存在着许多进程的 PCB，有的处于就绪态，有的处于阻塞态，而且阻塞的原因各不相同。为了方便进程的调度和管理，需要将各进程的 PCB 用适当的方法组织起来。目前，常用的组织方式有链接方式和索引方式两种。链接方式将同一状态的 PCB 链接成一个队列， 不同状态对应不同的队列，也可把处于阻塞态的进程的 PCB，根据其阻塞原因的不同，排成多个阻塞队列。索引方式将同一状态的进程组织在一个索引表中，索引表的表项指向相应的 PCB，不同状态对应不同的索引表，如就绪索引表和阻塞索引表等。
    
    - Q：PCB 中主要存储的内容是什么？为什么说 PCB 是进程存在的唯一标志？
        
        PCB 中存储的内容主要包括进程描述信息（含进程标识符）、进程控制和管理信息（含进程调度信息）、资源分配清单和处理机相关信息（含处理机状态信息）。
        
        进程创建时，操作系统为它新建一个 PCB，该结构之后常驻内存。进程结束时，系统收回其 PCB，该进程随之消亡。在进程的整个生命期中，系统总是通过 PCB 对进程进行控制的，亦即系统唯有通过进程的 PCB 才能感知到该进程的存在。所以 PCB 是进程存在的唯一标志。
        
    
    ---
    
- **2.1.3 进程的状态与转换**
    
    进程的状态转换是在硬件和操作系统的相互配合下完成的，其中其主要作用的是中断系统。
    
    1. 进程的基本状态与转换
        
        进程在其生命周期内，由于系统中各进程之间的相互制约及系统的运行环境的变化 ，使得进程的状态也在不断地发生变化。通常进程有以下 5 种状态，前 3 种是进程的基本状态。 
        
        - ① 执行态（运行态）：进程正在处理机上运行。在单处理机中，每个时刻只有一个进程处于运行态。
        - ② 就绪态。进程获得了除处理机外的一切所需资源，一旦得到处理机，便可立即运行。系统中处于就绪状态的进程可能有多个，通常将它们排成一个队列，称为就绪队列。
        - ③ 阻塞态（等待态）：进程正在等待某一事件而暂停运行，如等待某资源为可用（不包括处理机）或等待输入/输出完成。即使处理机空闲，该进程也不能运行。系统通常将处 于阻塞态的进程也排成一个队列，甚至根据阻塞原因的不同，设置多个阻塞队列。
        - ④ 创建态：进程正在被创建，尚未转到就绪态。创建进程需要多个步骤：首先申请一个空白 PCB，并向 PCB 中填写用于控制和管理进程的信息；然后为该进程分配运行时所必须的资源；最后把该进程转入就绪态并插入就绪队列。但是，如果进程所需的资源尚不能得到满足，如内存不足，则创建工作尚未完成，进程此时所处的状态称为创建态。
        - ⑤ 终止态（结束态）：进程正从系统中消失，可能是进程正常结束或其他原因退出运行。进程需要结束运行时，系统首先将该进程置为结束态，然后进一步处理资源释放和回收等工作。
        
        注意区别就绪态和阻塞态：就绪态是指进程仅缺少处理器，只要获得处理机资源就立即运行；而阻塞态是指进程需要其他资源（除了处理机）或等待某一事件。之所以把处理机和其他资源划分开，是因为在分时系统的时间片轮转机制中，每个进程分到的时间片是若干毫秒。也就是说， 进程得到处理机的时间很短且非常频繁，进程在运行过程中实际上是频繁地转换到就绪态的；而其他资源（如外设）的使用和分配或某一事件的发生（如 I/O 操作的完成）对应的时间相对来说很长，进程转换到阻塞态的次数也相对较少。这样来看，就绪态和等待态是进程生命周期中两个完全不同的状态，显然需要加以区分。
        
        ![进程的五种基本状态及转换](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%203.png)
        
        进程的五种基本状态及转换
        
        上图说明了 5 种进程状态的转换，而 3 种基本状态之间的转换如下：
        
        - 就绪态 → 运行态：处于就绪态的进程被调度后，获得处理机资源（分派处理机时间片），于是进程由就绪态转换为运行态。
        - 运行态 → 就绪态：处于运行态的进程在时间片用完后，不得不让出处理机，从而进程由运行态转换为就绪态。此外，在可剥夺的操作系统中，当有更高优先级的进程就绪时，调度程序将正在执行的进程转换为就绪态，让更高优先级的进程执行。
        - 运行态 → 阻塞态：进程请求某一资源（如外设）的使用和分配或等待某一事件的发生 （如 I/O 操作的完成）时，它就从运行态转换为阻塞态。进程以系统调用的形式请求操作系统提供服务，这是一种特殊的、由运行用户态程序调用操作系统内核过程的形式。
        - 阻塞态 → 就绪态：进程等待的事件到来时，如 I/O 操作结束或中断结束时，中断处理程序必须把相应进程的状态由阻塞态转换为就绪态。
        
        需要注意的是，一个进程从运行态变成阻塞态是主动的行为，而从阻塞态变成就绪态是被动的行为，需要其他相关进程的协助。
        
    2. 挂起操作和进程状态的转换
        
        许多系统还引入了进程挂起操作，当该操作作用于某个进程时，该进程将被挂起，意味着此时该进程处于静止状态。如果进程正在执行，它将暂停执行。 若原本处于就绪状态，则该进程此时暂不接受调度。与挂起操作对应的操作是激活操作。
        
        引入挂起操作的原因有：终端用户的需要；父进程请求；负荷调节的需要；操作系统的需要。
        
        在引入挂起状态、挂起原语 Suspend 和激活原语 Active 后，在它们的作用下，进程将可能发生以下几种状态的转换：
        
        - ① 创建 → 活动就绪：在当前系统的性能和内存的容量均允许的情况下，完成对进程创建的必要操作后，相应的系统进程将进程的状态转换为活动就绪状态。
        - ② 创建 → 静止就绪：考虑到系统当前资源状况和性能的要求，不分配给新建进程所需资源，主要是主存，相应的系统将进程状态转为静止就绪状态，被安置在外存，不参与调度，此时进程创建工作尚未完成。
        - ③ 活动就绪 → 静止就绪：当进程处于未被挂起的就绪状态时，称此为活动就绪状态，此时进程可以接受调度。当用挂起原语 Suspend 将该进程挂起后，该进程便转变为静止就绪状态，此时进程不再被调度执行。
        - ④ 活动阻塞 → 静止阻塞：当进程处于未被挂起的阻塞状态时，称它是处于活动阻塞状态。当用 Suspend 原语将它挂起后，进程便转变为静止阻塞状态。处于该状态的进程在其所期待的事件出现后，它将从静止阻塞变为静止就绪状态。
        - ⑤ 静止就绪 → 活动就绪：处于静止就绪状态的进程若用激活原语 Active 激活后，该进程将转变为活动就绪状态。
        - ⑥ 静止阻塞 → 活动阻塞：处于静止阻塞状态的进程若用激活原语 Active 激活后，进程将转变为活动就绪状态。
        
        ![具有创建、终止和挂起状态的进程状态图](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%204.png)
        
        具有创建、终止和挂起状态的进程状态图
        
    
    ---
    
- **2.1.4 进程控制**
    
    进程控制的主要功能是对系统中的所有进程实施有效的管理，它具有创建新进程、撤销已有进程、实现进程状态转换等功能。在操作系统中，一般把进程控制用的程序段称为原语，原语的特点是执行期间不允许中断，它是一个不可分割的基本单位。
    
    1. 进程的创建
        
        允许一个进程创建另一个进程，此时创建者称为父进程，被创建的进程称为子进程。子进程可以继承父进程所拥有的资源。当子进程被撤销时，应将其从父进程那里获得的资源归还给父进 程。此外，在撤销父进程时，通常也会同时撤销其所有的子进程（若父进程退出，而它的子进程还在运行，那么这些子进程将成为孤儿进程）。
        
        在操作系统中，终端用户登录系统、作业调度、系统提供服务、用户程序的应用请求等都会引起进程的创建。
        
        操作系统创建一个新进程的过程如下（创建原语）：
        
        - ① 为新进程分配一个唯一的进程标识号，并申请一个空白 PCB。PCB 是有限的，若 PCB 申请失败，则创建失败。
        - ② 为进程分配其运行所需的资源，如内存、文件、I/O 设备和 CPU 时间等（在 PCB 中体现）。这些资源或从操作系统获得，或仅从其父进程获得。如果资源不足（如内存），则并不是创建失败，而是处于创建态，等待内存资源。
        - ③ 初始化 PCB，主要包括初始化标志信息、初始化处理机状态信息和初始化处理机控制信息，以及设置进程的优先级等。
        - ④ 若进程就绪队列能够接纳新进程，则将新进程插入就绪队列，等待被调度运行。
    2. 进程的终止
        
        引起进程终止的事件主要有：
        
        - ① 正常结束，表示进程的任务已完成并准备退岀运行。
        - ② 异常结束，表示进程在运行时，发生了某种异常事件，使程序无法继续运行，如存储区越界、保护错、非法指令、特权指令错、运行超时、算术运算错、I/O 故障等。
        - ③ 外界干预，指进程应外界的请求而终止运行，如操作员或操作系统干预、父进程请求和父进程终止。
        
        操作系统终止进程的过程如下（终止原语）：
        
        - ① 根据被终止进程的标识符，检索出该进程的 PCB，从中读出该进程的状态。
        - ② 若被终止进程处于执行状态，立即终止该进程的执行，将处理机资源分配给其他进程。
        - ③ 若该进程还有子孙进程，则应将其所有子孙进程终止。
        - ④ 将该进程所拥有的全部资源，或归还给其父进程，或归还给操作系统。
        - ⑤ 将该 PCB 从所在队列（链表）中删除。
    3. 进程的阻塞与唤醒
        
        正在执行的进程，由于期待的某些事件未发生，如请求系统资源失败、等待某种操作的完成、新数据尚未到达或无新任务可做等，进程便通过调用阻塞原语（Block），使自己由运行态变为阻塞态。可见，阻塞是进程自身的一种主动行为，也因此只有处于运行态的进程 （获得 CPU），才可能转为阻塞态。阻塞原语的执行过程如下：
        
        - ① 找到将要被阻塞进程的标识号对应的 PCB。
        - ② 若该进程为运行态，则保护其现场，将其状态转为阻塞态，停止运行。
        - ③ 把该 PCB 插入相应事件的等待队列，将处理机资源调度给其他就绪进程。
        
        当被阻塞进程所期待的事件出现时，如它所启动的 I/O 操作已完成或其所期待的数据已到达，由有关进程（比如，释放该 I/O 设备的进程，或提供数据的进程）调用唤醒原语（Wakeup），将等待该事件的进程唤醒。唤醒原语的执行过程如下：
        
        - ① 在该事件的等待队列中找到相应进程的 PCB。
        - ② 将其从等待队列中移出，并置其状态为就绪态。
        - ③ 把该 PCB 插入就绪队列，等待调度程序调度。
        
        应当注意，Block 原语和 Wakeup 原语是一对作用刚好相反的原语，必须成对使用。如果在某进程中调用了 Block 原语，则必须在与之合作的或其他相关的进程中安排一条相应的 Wakeup 原语，以便唤醒阻塞进程；否则，阻塞进程将会因不能被唤醒而永久地处于阻塞状态。
        
    4. 进程的挂起与激活
        
        当系统中出现了引起进程挂起的事件时， OS 将利用挂起原语（Suspend）将指定进程或处于阻塞状态的进程挂起。 Suspend 的执行过程是：首先检查被挂起进程的状态，若处于活动就绪状态，便将其改为静止就绪；对于活动阻塞状态的进程，则将之改为静止阻塞；为了方便用户或父进程考查该进程的运行情况，而把该进程的 PCB 复制到某指定的内存区域；最后，若被挂起的进程正在执行，则转向调度程序重新调度。
        
        当系统中发生激活进程的事件时， OS 将利用激活原语（Active），将指定进程激活。激活原语先将进程从外存调入内存，检查该进程的现行状态，若是静止就绪，便将之改为活动就绪；若为静止阻塞，便将之改为活动阻塞。假如采用的是抢占调度策略，则每当有静止就绪进程被激活而插入就绪队列时，便应检查是否要进行重新调度，即由调度程序将被激活的进程与当前进程两者的优先级进行比较，如果被激活进程的优先级低，就不必重新调度；否则，立即剥夺当前进程的运行，把处理机分配给刚刚被激活的进程。
        
    
    ---
    

### 2.2 进程的同步与互斥：临界资源、临界区、进程同步与互斥问题、信号量机制以及 P、V 操作、管程机制 ★★★★★

- **2.2.1 同步与互斥的基本概念**
    
    在多道程序环境下，进程是并发执行的，不同进程之间存在着不同的相互制约关系。为了协调进程之间的相互制约关系，引入了进程同步的概念。
    
    1. 临界资源和临界区
        
        虽然多个进程可以共享系统中的各种资源，但其中许多资源一次只能为一个进程所用，我们将一次仅允许一个进程使用的资源称为临界资源（必须互斥使用的资源）。许多物理设备都属于临界资源，如打印机等。 此外，还有许多变量、数据等都可以被若干进程共享，也属于临界资源。
        
        对临界资源的访问，必须互斥地进行，在每个进程中，访问临界资源的那段代码称为临界区。 为了保证临界资源的正确使用，可把临界资源的访问过程分成 4 个部分：
        
        - 进入区：为了进入临界区使用临界资源，在进入区要检查可否进入临界区 ，若能进入临界区，则应设置正在访问临界区的标志，以阻止其他进程同时进入临界区。
        - 临界区：进程中访问临界资源的那段代码，又称临界段。
        - 退出区：将正在访问临界区的标志清除。
        - 剩余区：代码中的其余部分。
    2. 同步
        
        同步亦称直接制约关系，是指为完成某种任务而建立的两个或多个进程，这些进程因为需要在某些位置上协调它们的工作次序而等待、传递信息所产生的制约关系。进程间的直接制约关系源于它们之间的相互合作。 
        
        例如，输入进程 A 通过单缓冲向进程 B 提供数据。当该缓冲区空时，进程 B 不能获得所需数据而阻塞，一旦进程 A 将数据送入缓冲区，进程 B 就被唤醒。反之，当缓冲区满时，进程 A 被阻塞，仅当进程 B 取走缓冲数据时，才唤醒进程 A。
        
    3. 互斥
        
        互斥也称间接制约关系。当一个进程进入临界区使用临界资源时，另一个进程必须等待，当占用临界资源的进程退出临界区后，另一进程才允许去访问此临界资源。
        
        例如，在仅有一台打印机的系统中，有两个进程 A 和进程 B，若进程 A 需要打印时，系统已将打印机分配给进程 B，则进程 A 必须阻塞。一旦进程 B 将打印机释放，系统便将进程 A 唤醒，并将其由阻塞态变为就绪态。
        
        为禁止两个进程同时进入临界区，同步机制应遵循以下准则： 
        
        - 空闲让进：临界区空闲时，可以允许一个请求进入临界区的进程立即进入临界区 。
        - 忙则等待：当已有进程进入临界区时，其他试图进入临界区的进程必须等待。
        - 有限等待：对请求访问的进程，应保证能在有限时间内进入临界区。
        - 让权等待：当进程不能进入临界区时，应立即释放处理器，防止进程忙等待。
    
    ---
    
- **2.2.2 实现临界区互斥的基本方法**
    1. 软件实现方法 - Peterson 算法
        
        为了防止两个进程为进入临界区而无限期等待，设置变量 turn，每个进程在先设置自己的标志后再设置 turn 标志。这时，再同时检测另一个进程状态标志和允许进入标志，以便保证两个进程同时要求进入临界区时，只允许一个进程进入临界区。
        
        ```c
        // Pi进程：                   // Pj进程：
        flag[i]=TRUE;turn=j;         flag[j]=TRUE; turn=i;         //进入区
        while(flag[j] && turn==j);   while(flag[i] && turn==i) ;   //进入区
        critical section;            critical section;             //临界区
        flag[i]=FALSE;               flag[j]=FALSE;                //退出区 
        remainder section;           remainder section;            //剩余区
        ```
        
        具体如下：考虑进程 $\small P_i$，一旦设置 `flag[i]= true`，就表示它想要进入临界区，同时 `turn=j` ，此时若进程 $\small P_j$ 已在临界区中，符合进程 $\small P_i$ 中的 while 循环条件，则 $\small P_i$ 不能进入临界区。若 $\small P_j$ 不想要进入临界区，即 `flag[j]= false`，循环条件不符合，则 $\small P_i$ 可以顺利进入，反之亦然。
        
        本算法的基本思想是利用 flag 标志解决临界资源的互斥访问， 而利用 turn 解决“饥饿”现象。故该算法满足解决临界区问题的三个必须标准：空闲让进、忙则等待、有限等待。
        
    2. 硬件实现方法
        1. 中断屏蔽（关中断）
            
            当一个进程正在执行它的临界区代码时，防止其他进程进入其临界区的最简方法是关中断。 因为 CPU 只在发生中断时引起进程切换，因此屏蔽中断能够保证当前运行的进程让临界区代码顺利地执行完，进而保证互斥的正确实现，然后执行开中断。其典型模式为
            
            ```c
            ...
            关中断； 
            临界区； 
            开中断；
            ...
            ```
            
            这种方法限制了处理机交替执行程序的能力，因此执行的效率会明显降低。对内核来说，在它执行更新变量或列表的几条指令期间，关中断是很方便的，但将关中断的权力交给用户则很不明智，若一个进程关中断后不再开中断，则系统可能会因此终止。关中断方法也不适用于多 CPU 系统，因为在一个处理器上关中断并不能防止进程在其它处理器上执行相同的临界段代码。
            
        2. 硬件指令方法
            - TestAndSet 指令：这条指令是原子操作，即执行该代码时不允许被中断。其功能是读出指定标志后把该标志设置为真。指令的功能描述如下：
                
                ```c
                boolean TestAndSet(boolean *lock){ 
                	boolean old; 
                	old=*lock; 
                	*lock=true; 
                	return old;
                }
                ```
                
                可以为每个临界资源设置一个共享布尔变量 lock，表示资源的两种状态：true 表示正被占用， 初值为 false。进程在进入临界区之前，利用 TestAndSet 检查标志 lock，若无进程在临界区，则其值为 false，可以进入，关闭临界资源，把 lock 置为 true，使任何进程都不能进入临界区；若有进程在临界区，则循环检查，直到进程退出。利用该指令实现互斥的过程描述如下：
                
                ```c
                while TestAndSet(&1ock); 
                进程的临界区代码段；
                lock=false; 
                进程的其他代码；
                ```
                
            - Swap 指令：该指令的功能是交换两个字(字节)的内容。其功能描述如下：
                
                ```c
                Swap(boolean boolean *b){ 
                	boolean temp; 
                	Temp=*a; 
                	*a=*b; 
                	*b=temp;
                }
                ```
                
                用 Swap 指令可以简单有效地实现互斥，为每个临界资源设置一个共享布尔变量 lock，初值为 false；在每个进程中再设置一个局部布尔变量 key，用于与 lock 交换信息。在进入临界区前， 先利用 Swap 指令交换 lock 与 key 的内容，然后检查 key 的状态；有进程在临界区时，重复交换和检查过程，直到进程退出。其处理过程描述如下：
                
                ```c
                key=true;
                while(key!=false) 
                	Swap(&lock, &key);
                进程的临界区代码段；
                lock=false; 
                进程的其他代码；
                ```
                
            
            注意：以上对 TestAndSet 和 Swap 指令的描述仅是功能实现，而并非软件实现的定义。事实 上，它们是由硬件逻辑直接实现的，不会被中断。
            
            硬件方法的优点：适用于任意数目的进程，而不管是单处理机还是多处理机；简单、容易验证其正确性。可以支持进程内有多个临界区，只需为每个临界区设立一个布尔变量。
            
            硬件方法的缺点：进程等待进入临界区时要耗费处理机时间，不能实现让权等待。从等待进程中随机选择一个进入临界区，有的进程可能一直选不上，从而导致“饥饿”现象。
            
    
    ---
    
- **2.2.3 信号量机制及 P、V 操作**
    
    注：P、V 操作应用及习题参考 [2.6 综合应用：生产者消费者问题、读者和写者问题、哲学家进餐问题等](https://www.notion.so/d6789a488c934a009190634bba8e1811)
    
    信号量机制可用来解决互斥与同步问题，与一般整型数据不同，它只能被两个标准的原语 wait(S) 和 signal(S) 访问（可分别记为 P、V 操作）。原语是指完成某种功能且不被分割、不被中断执行的操作序列，通常可由硬件来实现。
    
    1. 整型信号量
        
        整型信号量被定义为一个用于表示资源数目的整型量 S，wait 和 signal 操作可描述为
        
        ```c
        wait(S) { 
        	while(S<=0); 
        	S--; 
        } 
        
        signal(S) { 
        	S++; 
        }
        ```
        
        在整型信号量机制中的 wait 操作，只要信号量 S ≤ 0,就会不断地测试。因此，该机制并未遵循“让权等待”的准则，而是使进程处于“忙等”的状态。
        
    2. 记录型信号量
        
        记录型信号量机制是一种不存在“忙等”现象的进程同步机制。除了需要一个用于代表资源数目的整型变量 value 外，还应增加一个进程链表指针 list，用于链接上述的所有等待进程。记录型信号量是由于它采用了记录型的数据结构而得名的。
        
        ```c
        typedef struct { 
        	int value;
        	struct process_controblock *list;
        } semaphore;
        ```
        
        相应地，wait(S) 和  signal(S) 操作可描述如下：
        
        ```c
        wait(semaphore *S) {
        	S->value--;
        	if (S->value < 0) block(S->list);
        }
        ```
        
        wait 操作，S->value-- 表示进程请求一个该类资源，当 S->value < 0 时，表示该类资源已分配完毕，因此进程应调用 block 原语，进行自我阻塞，放弃处理机，并插入该类资源的等待队列 S->list，可见该机制遵循了 “让权等待”的准则。
        
        ```c
        signal(semaphore *S) {
        	S->value++:
        	if (S->value<=O) wakeup (S->list);
        }
        ```
        
        signal 操作，表示进程释放一个资源，使系统中可供分配的该类资源数增1，因此有 S -> value ++。 若加 1 后仍是 S -> value < 0，则表示在 S -> list 中仍有等待该资源的进程被阻塞，因此还应调用 wakeup 原语，将 S -> list 中的第一个等待进程唤醒。如果 S -> value 的初值为 l，表示只允许一个进程访问临界资源，此时的信号量转化为互斥信号量，用于进程互斥。
        
    3. AND 型信号量
        
        AND 同步机制的基本思想是：将进程在整个运行过程中需要的所有资源，一次性全部地分配给进程，待进程使用完后再一起释放。只要尚有一个资源未能分配给进程，其它所有可能为之分配的资源也不分配给它。亦即，对若干个临界资源的分配采取原子操作方式：要么把它所请求的资源全部分配到进程，要么一个也不分配。由死锁理论可知，这样就可避免死锁情况的发生。为此，在 wait 操作中增加了一个 “AND” 条件，故称为 AND 同步，或称为同时 wait 操作。
        
    4. 利用信号量实现同步
        
        信号量机制能用于解决进程间的各种同步问题 。设 S 为实现进程 P1、P2 同步的公共信号量， 初值为 0。进程 P2 中的语句 y 要使用进程 P1 中语句 x 的运行结果，所以只有当语句 x 执行完成之后语句 y 才可以执行。其实现进程同步的算法如下：
        
        ```c
        semaphore S=0;   // 初始化信号量
        
        P1() {
        	...
        	x;             // 语句 x
        	V(S);          // 告诉进程 P2，语句 x 己经完成
        	...
        } 
        
        P2() {
        	...
        	P(S);          // 检査语句 x 是否运行完成 
        	y;             // 检查无误，运行 y 语句
        	...
        }
        ```
        
        若 P2 先执行到 P(S) 时，S 为 0，执行 P 操作会把进程 P2 阻塞，并放入阻塞队列；当进程 P1 中的 x 执行完后，执行 V 操作，把 P2 从阻塞队列中放回就绪队列，当 P2 得到处理机时，就得以继续执行。
        
    5. 利用信号量实现进程互斥
        
        信号量机制也能很方便地解决进程互斥问题。设 S 为实现进程 P1、P2 互斥的信号量，由于每次只允许一个进程进入临界区 ，所以 S 的初值应为 1（即可用资源数为 1）。只需把临界区置于 P(S) 和 V(S) 之间，即可实现两个进程对临界资源的互斥访问。其算法如下：
        
        ```c
        semaphore S=1;    // 初始化信号量，值为 1 相当于互斥锁
        
        P1() {
        	...
        	P(S);           // 准备开始访问临界资源，加锁
        	进程P1的临界区; 
        	V(S);           // 访问结束，解锁
        	...
        }
        
        P2() {
        	...
        	P(S);           // 准备开始访问临界资源，加锁
        	进程P2的临界区; 
        	V(S);           // 访问结束，解锁
        	...
        }
        ```
        
        当没有进程在临界区时，任意一个进程要进入临界区，就要执行 P 操作，把 S 的值减为 0，然后进入临界区；当有进程存在于临界区时，S 的值为 0，再有进程要进入临界区，执行 P 操作时将会被阻塞，直至在临界区中的进程退出，这样便实现了临界区的互斥。 
        
        互斥是不同进程对同一信号量进行 P、V 操作实现的，一个进程成功对信号量执行了 P 操作后进入临界区，并在退出临界区后，由该进程本身对该信号量执行 V 操作，表示当前没有进程进入临界区，可以让其他进程进入。
        
    6. 利用信号量实现前驱关系
        
        信号量也可用来描述程序之间或语句之间的前驱关系。下图给出了一个前驱图，其中 S1 ~ S6 是最简单的程序段(只有一 条语句)。为使各程序段能正确执行，应设置若干初始值为“0”的 信号量。例如，为保证 S1→S2，S1→S3 的前驱关系，应分别设置信号量 a1、a2。同样，为保证 S2→S4，S2→S5，S3→S6，S4→S6，S5→S6，应设置信号量 b1、b2、c、d、e。
        
        ![前驱图举例](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%205.png)
        
        前驱图举例
        
        实现算法如下：
        
        ```c
        semaphore a1=a2=b1=b2=c=d=e=0; // 初始化信号量
        
        S1() {
        	...;
        	V(a1); V(a2);                // S1 已经运行完成
        }
        
        S2() {
        	P(a1);                       // 检查 S1 是否运行完成
        	...;
        	V(b1); V(b2);                // S2 已经运行完成
        }
        
        S3() {
        	P(a2);                       // 检查 S1 是否运行完成
        	...;
        	V(c);                        // S3 已经运行完成
        }
        
        S4() {
        	P(b1);                       // 检查 S2 是否运行完成
        	...;
        	V(d);                        // S4 已经运行完成
        }
        
        S5() {
        	P(b2);                       // 检查 S2 是否运行完成
        	...;
        	V(e);                        // S5 已经运行完成
        }
        
        S6() {
        	P(c);                        // 检查 S3 是否运行完成
        	P(d);                        // 检查 S4 是否运行完成
        	P(e);                        // 检查 S5 是否运行完成
        	...;
        }
        ```
        
    7. 分析进程同步和互斥问题的方法步骤
        - 关系分析：找出问题中的进程数，并分析它们之间的同步和互斥关系。同步、互斥、前驱关系直接按照上面例子中的经典范式改写。
        - 整理思路：找岀解决问题的关键点，并根据做过的题目找岀求解的思路。根据进程的操作流程确定P操作、V操作的大致顺序。
        - 设置信号量：根据上面的两步，设置需要的信号量，确定初值，完善整理。
    
    ---
    
- **2.2.4 管程机制**
    
    在信号量机制中，每个要访问临界资源的进程都必须自备同步的 P V 操作，大量分散的同步操作给系统管理带来了麻烦，且容易因同步操作不当而导致系统死锁。于是，便产生了一种新的进程同步工具一一管程。管程的特性保证了进程互斥，无须程序员自己实现互斥，从而降低了死锁发生的可能性。同时管程提供了条件变量，可以让程序员灵活地实现进程同步。
    
    1. 管程的定义
        
        系统中的各种硬件资源和软件资源，均可用数据结构抽象地描述其资源特性，即用少量信息和对资源所执行的操作来表征该资源，而忽略它们的内部结构和实现细节。
        
        利用共享数据结构抽象地表示系统中的共享资源，而把对该数据结构实施的操作定义为一组过程。进程对共享资源的申请、释放等操作，都通过这组过程来实现，这组过程还可以根据资源情况，或接受或阻塞进程的访问，确保每次仅有一个进程使用共享资源，这样就可以统一管理对共享资源的所有访问，实现进程互斥。这个代表共享资源的数据结构，以及由对该共享数据结构实施操作的一组过程所组成的资源管理程序，称为管程（monitor）。管程定义了一个数据结构和能为并发进程所执行（在该数据结构上）的一组操作，这组操作能同步进程和改变管程中的数据。
        
        由上述定义可知，管程由 4 部分组成：
        
        - ① 管程的名称；
        - ② 局部于管程内部的共享数据结构说明；
        - ③ 对该数据结构进行操作的一组过程(或函数)；
        - ④ 对局部于管程内部的共享数据设置初始值的语句。
        
        管程的定义描述举例如下：
        
        ```c
        monitor Demo{ // ① 定义一个名称为"Demo"的管程 
            
            共享数据结构 S; // ② 定义共享数据结构，对应系统中的某种共享资源
        
            // ④ 对共享数据结构初始化的语句
            init_code(){
                S = 5; // 初始资源数等于 5
            } 
        
            // ③ 过程1：申请一个资源
            take_away(){ 
                对共享数据结构x的一系列处理； 
                S--; // 可用资源数 -1
                ...
            }
        
            // ③ 过程2：归还一个资源 
            give_back(){
                对共享数据结构x的一系列处理；
                S++; // 可用资源数 +1
                ...
            }
        }
        ```
        
        熟悉面向对象程序设计的读者看到管程的组成后，会立即联想到管程很像一个类（class）。 
        
        - 管程把对共享资源的操作封装起来，管程内的共享数据结构只能被管程内的过程所访问。一个进程只有通过调用管程内的过程才能进入管程访问共享资源。对于上例，外部 进程只能通过调用 take_away( ) 过程来申请一个资源；归还资源也一样。
        - 每次仅允许一个进程进入管程，从而实现进程互斥。若多个进程同时调用 take_away( )，give_back( ),则只有某个进程运行完它调用的过程后，下个进程才能开始运行它调用的过程。也就是说，各个进程只能串行执行管程内的过程，这一特性保证了进程“互斥”访问共享数据结构 S。
    2. 条件变量
        
        当一个进程进入管程后被阻塞，直到阻塞的原因解除时，在此期间，如果该进程不释放管程，那么其他进程无法进入管程。为此，将阻塞原因定义为条件变量 condition。通常，一个进程被阻塞的原因可以有多个，因此在管程中设置了多个条件变量。每个条件变量保存了一个等待队列， 用于记录因该条件变量而阻塞的所有进程 ，对条件变量只能进行两种操作，即 wait 和 signal。
        
        x.wait：当 x 对应的条件不满足时，正在调用管程的进程调用 x.wait 将自己插入 x 条件的等待队列，并释放管程。此时其他进程可以使用该管程。
        
        x.signal：x 对应的条件发生了变化，则调用 x.signal，唤醒一个因 x 条件而阻塞的进程。
        
        下面给出条件变量的定义和使用：
        
        ```c
        monitor Demo{
            共享数据结构 S;
            condition x; // 定义一个条件变量 x
            init_code(){ ... }
            take_away(){ 
                if (S <= 0) x.wait (); // 资源不够，在条件变量 x 上阻塞等待 
                资源足够，分配资源，做一系列相应处理； 
            }
        
            give_back(){ 
                归还资源，做一系列相应处理；
                if(有进程在等待) x.signal(); // 唤醒一个阻塞进程
            }
        }
        ```
        
        条件变量和信号量的比较： 
        
        相似点：条件变量的 wait/signal 操作类似于信号量的 P/V 操作，可以实现进程的阻塞/唤醒。
        
        不同点：条件变量是“没有值”的，仅实现了 “排队等待”功能；而信号量是“有值”的， 信号量的值反映了剩余资源数，而在管程中，剩余资源数用共享数据结构记录。
        
    
    ---
    

### 2.3 进程间通信：进程通信的类型（直接通信和间接通信方式）、消息传递系统中的几个问题、 消息缓冲队列通信机制 ★

- **2.3.1 进程通信的类型**
    
    进程通信是指进程之间的信息交换。P V 操作是低级通信方式，高级通信方式是指以较高的效率传输大量数据的通信方式。高级通信方法主要有以下三类。
    
    1. 共享存储
        
        在通信的进程之间存在一块可直接访问的共享空间 ，通过对这片共享空间进行写/读操作实现进程之间的信息交换，如图所示。在对共享空间进行写/读操作时，需要使用同步互斥工具（如 P 操作、V 操作），对共享空间的写/读进行控制。共享存储又分为两种：低级方式的共享是基于数据结构的共享；高级方式的共享则是基于存储区的共享。操作系统只负责为通信进程提供可共享使用的存储空间和同步互斥工具，而数据交换则由用户自己安排读/写指令完成。 
        
        注意，进程空间一般都是独立的，进程运行期间一般不能访问其他进程的空间，想让两个进程共享空间，必须通过特殊的系统调用实现，而进程内的线程是自然共享进程空间的。
        
        ![共享存储](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%206.png)
        
        共享存储
        
    2. 消息传递
        
        在消息传递系统中，进程间的数据交换以格式化的消息（Message）为单位。若通信的进程之间不存在可直接访问的共享空间，则必须利用操作系统提供的消息传递方法实现进程通信。进程通过系统提供的发送消息和接收消息两个原语进行数据交换。这种方式隐藏了通信实现细节，使通信过程对用户透明，简化了通信程序的设计，是当前应用最广泛的进程间通信机制。在微内核操作系统中，微内核与服务器之间的通信就采用了消息传递机制。由于该机制能很好地支持多处理机系统、分布式系统和计算机网络，因此也成为这些领域最主要的通信工具。
        
        - 直接通信方式。发送进程直接把消息发送给接收进程，并将它挂在接收进程的消息缓冲队列上，接收进程从消息缓冲队列中取得消息，如图所示。
            
            ![消息传递](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%207.png)
            
            消息传递
            
        - 间接通信方式。发送进程把消息发送到某个中间实体，接收进程从中间实体取得消息。这种中间实体一般称为信箱。该通信方式广泛应用于计算机网络中。
    3. 管道通信
        
        管道通信是消息传递的一种特殊方式。所谓“管道”，是指用于连接一个读进程和一个写进程以实现它们之间的通信的一个共享文件，又名 pipe 文件。向管道（共享文件）提供输入的发送进程（即写进程），以字符流形式将大量的数据送入（写）管道；而接收管道输出的接收进程（即读进程）则从管道中接收（读）数据。为了协调双方的通信，管道机制必须提供以下三方面的协调能力：互斥、同步和确定对方的存在。
        
        在 Linux 中，管道是一种使用非常频繁的通信机制。从本质上说，管道也是一种文件，但它又和一般的文件有所不同，管道可以克服使用文件进行通信的两个问题，具体表现如下： 
        
        - ① 限制管道的大小。实际上，管道是一个固定大小的缓冲区。在 Linux 中，该缓冲区的大小为 4KB，这使得它的大小不像文件那样不加检验地增长。使用单个固定缓冲区也会带来问题，比如在写管道时可能变满，这种情况发生时，随后对管道的 write( ) 调用将默认地被阻塞，等待某些数据被读取，以便腾出足够的空间供 write( ) 调用写。
        - ② 读进程也可能工作得比写进程快。当所有当前进程数据已被读取时，管道变空。当这种情况发生时，一个随后的 read( ) 调用将默认地被阻塞，等待某些数据被写入，这解决了 read( ) 调用返回文件结束的问题。
        
        注意：从管道读数据是一次性操作，数据一旦被读取，就释放空间以便写更多数据。管道只能采用半双工通信，即某一时刻只能单向传输。要实现父子进程互动通信，需定义两个管道。
        
        管道可以理解为共享存储的优化和发展，因为在共享存储中，若某进程要访问共享存储空间, 则必须没有其他进程在该共享存储空间中进行写操作，否则访问行为就会被阻塞。而管道通信中，存储空间进化成了缓冲区，缓冲区只允许一边写入、另一边读出，因此只要缓冲区中有数据，进 程就能从缓冲区中读出，而不必担心会因为其他进程在其中进行写操作而遭到阻塞，因为写进程 会先把缓冲区写满，然后才让读进程读，当缓冲区中还有数据时，写进程不会往缓冲区写数据。 当然，这也决定了管道通信必然是半双工通信。
        
    
    ---
    
- **2.3.2 消息传递系统中的几个问题**
    
    TODO
    
    ---
    
- **2.3.3 消息缓冲队列通信机制**
    
    TODO
    
    在这种通信机制中，发送进程利用 Send 原语将消息直接发送给接收进程；接收进程则利用 Receive 原语接收消息。
    
    ---
    

### 2.4 线程与进程的调度：线程与进程的基本概念，调度的类型、调度队列模型、调度方式、进程调度算法（先来先服务、短进程优先、时间片轮转、基于优先级的调度算法等）★★★★

- **2.4.1 线程和进程的基本概念**
    
    引入进程的目的是更好地使多道程序并发执行，提高资源利用率和系统吞吐量。而引入线程的目的则是减小程序在并发执行时所付出的时空开销，提高操作系统的并发性能。
    
    线程最直接的理解就是“轻量级进程”，它是一个基本的 CPU 执行单元，也是程序执行流的最小单元，由线程 ID、程序计数器、寄存器集合和堆栈组成。线程是进程中的一个实体，是被系统独立调度和分派的基本单位，线程自己不拥有系统资源，只拥有一点儿在运行中必不可少的资 源，但它可与同属一个进程的其他线程共享进程所拥有的全部资源。一个线程可以创建和撤销另 一个线程，同一进程中的多个线程之间可以并发执行。由于线程之间的相互制约，致使线程在运 行中呈现出间断性。线程也有就绪、阻塞和运行三种基本状态。
    
    引入线程后，进程的内涵发生了改变，进程只作为除 CPU 外的系统资源的分配单元，而线程则作为处理机的分配单元。由于一个进程内部有多个线程，若线程的切换发生在同一个进程内部，则只需要很少的时空开销。
    
    ---
    
- **2.4.2 调度的类型、调度队列模型**
    1. 调度的基本概念
        
        在多道程序系统中，进程的数量往往多于处理机的个数，因此进程争用处理机的情况在所难 免。处理机调度是对处理机进行分配，即从就绪队列中按照一定的算法（公平、高效的原则）选择一个进程并将处理机分配给它运行 ，以实现进程并发地执行。 处理机调度是多道程序操作系统的基础，是操作系统设计的核心问题。
        
    2. 调度的层次（调度类型、调度队列模型）
        
        一个作业从提交开始直到完成 ，往往要经历以下三级调度，如图所示。
        
        ![具有三级调度时的调度队列模型](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/%25E5%2585%25B7%25E6%259C%2589%25E4%25B8%2589%25E7%25BA%25A7%25E8%25B0%2583%25E5%25BA%25A6%25E6%2597%25B6%25E7%259A%2584%25E8%25B0%2583%25E5%25BA%25A6%25E9%2598%259F%25E5%2588%2597%25E6%25A8%25A1%25E5%259E%258B.png)
        
        具有三级调度时的调度队列模型
        
        - 高级调度（作业调度、长程调度）
            
            按照一定的原则从外存上处于后备队列的作业中挑选一个 （或多个），给它（们）分配内存、 输入/输岀设备等必要的资源，并建立相应的进程，以使它（们）获得竞争处理机的权利。简言之，作业调度就是内存与辅存之间的调度。对于每个作业只调入一次、调出一次。 多道批处理系统中大多配有作业调度，而其他系统中通常不需要配置作业调度。 
            
        - 中级调度（内存调度、中程调度）
            
            引入中极调度的目的是提高内存利用率和系统吞吐量。为此，将那些暂时不能运行的进程调至外存等待，此时进程的状态称为挂起态。当它们已具备运行条件且内存又稍有空闲时 ，由中级调度来决定把外存上的那些已具备运行条件的就绪进程再重新调入内存，并修改其状态为就绪 态，挂在就绪队列上等待。中级调度实际上是存储器管理中的对换功能。
            
        - 低级调度（进程调度、短程调度）
            
            按照某种算法从就绪队列中选取一个进程，将处理机分配给它。进程调度是最基本的一种调度，在各种操作系统中都必须配置这级调度。进程调度的频率很高，一般几十毫秒一次。
            
    3. 三级调度的联系
        
        作业调度从外存的后备队列中选择一批作业进入内存 ，为它们建立进程，这些进程被送入就绪队列，进程调度从就绪队列中选出一个进程，并把其状态改为运行态，把 CPU 分配给它。中级调度是为了提高内存的利用率，系统将那些暂时不能运行的进程挂起来。
        
        1. 作业调度为进程活动做准备，进程调度使进程正常活动起来。
        2. 中级调度将暂时不能运行的进程挂起，中级调度处于作业调度和进程调度之间。 
        3. 作业调度次数少，中级调度次数略多，进程调度频率最高。 
        4. 进程调度是最基本的，不可或缺。
    
    ---
    
- **2.4.3 进程调度方式、调度的目标**
    1. 进程调度方式
        
        所谓进程调度方式，是指当某个进程正在处理机上执行时，若有某个更为重要或紧迫的进程需要处理，即有优先权更高的进程进入就绪队列，此时应如何分配处理机。 
        
        通常有以下两种进程调度方式： 
        
        - 非抢占调度方式，又称非剥夺方式。是指当一个进程正在处理机上执行时 ，即使有某个更为重要或紧迫的进程进入就绪队列，仍然让正在执行的进程继续执行，直到该进程运 行完成或发生某种事件而进入阻塞态时，才把处理机分配给其他进程。 非抢占调度方式的优点是实现简单、系统开销小，适用于大多数的批处理系统，但它不 能用于分时系统和大多数的实时系统。
        - 抢占调度方式，又称剥夺方式。是指当一个进程正在处理机上执行时，若有某个更为重要或紧迫的进程需要使用处理机，则允许调度程序根据某种原则去暂停正在执行的进程, 将处理机分配给这个更为重要或紧迫的进程 。 抢占调度方式对提高系统吞吐率和响应效率都有明显的好处。但“抢占”不是一种任意性行为，必须遵循一定的原则，主要有优先权、短进程优先和时间片原则等。
    2. 调度的目标
        
        不同的调度算法具有不同的特性，在选择调度算法时，必须考虑算法的特性。为了比较处理 机调度算法的性能，人们提出了很多评价标准，下面介绍其中主要的几种：
        
        - CPU利用率。CPU是计算机系统中最重要和昂贵的资源之一，所以应尽可能使CPU保持“忙”状态，使这一资源利用率最高。CPU利用率的计算方法：${\small CPU 的利用率 = \frac{CPU 有效工作时间}{CPU 有效工作时间 + CPU 空闲等待时间} }$
        - 系统吞吐量。表示单位时间内CPU完成作业的数量。长作业需要消耗较长的处理机时间，因此会降低系统的吞吐量。而对于短作业，需要消耗的处理机时间较短，因此能提高系统的吞吐量。调度算法和方式的不同，也会对系统的吞吐量产生较大的影响。
        - 周转时间。指从作业提交到作业完成所经历的时间，是在处理机上运行及输入/输出操作所花费时间的总和。
            - 周转时间的计算方法：周转时间 = 作业完成时间 - 作业提交时间
            - 平均周转时间是指多个作业周转时间的平均值：${\small 平均周转时间 = \frac{作业1的周转时间 + \dots + 作业 n 的周转时间}{n}
            }$
            - 带权周转时间是指作业周转时间与作业实际运行时间的比值：${\small 带权周转时间 = \frac{作业周转时间}{作业实际运行时间}
            }$
            - 平均带权周转时间是指多个作业带权周转时间的平均值：${\small 平均带权周转时间 = \frac{作业1的带权周转时间 + \dots + 作业 n 的带权周转时间}{n}
            }$
        - 等待时间。指进程处于等处理机的时间之和，等待时间越长，用户满意度越低。处理机 调度算法实际上并不影响作业执行或输入/输出操作的时间，只影响作业在就绪队列中等 待所花的时间。因此，衡量一个调度算法的优劣，常常只需简单地考察等待时间。
        - 响应时间。指从用户提交请求到系统首次产生响应所用的时间。在交互式系统中，周转 时间不是最好的评价准则，一般釆用响应时间作为衡量调度算法的重要准则之一。从用 户角度来看，调度策略应尽量降低响应时间，使响应时间处在用户能接受的范围之内。
    
    ---
    
- **2.4.4 进程调度算法**
    1. 先来先服务（First-come First-served，FCFS）调度算法
        
        FCFS 调度算法是一种最简单的调度算法，它既可用于作业调度，又可用于进程调度。在作业调度中，算法每次从后备作业队列中选择最先进入该队列的一个或几个作业，将它们调入内存, 分配必要的资源，创建进程并放入就绪队列。
        
        在进程调度中，FCFS 调度算法每次从就绪队列中选择最先进入该队列的进程 ，将处理机分 配给它，使之投入运行，直到运行完成或因某种原因而阻塞时才释放处理机 。
        
        FCFS 调度算法属于不可剥夺算法。从表面上看，它对所有作业都是公平的，但若一个长作业先到达系统，就会使后面的许多短作业等待很长时间，因此它不能作为分时系统和实时系统的 主要调度策略。但它常被结合在其他调度策略中使用。例如，在使用优先级作为调度策略的系统中，往往对多个具有相同优先级的进程按 FCFS 原则处理。
        
        FCFS 调度算法的特点是算法简单，但效率低；对长作业比较有利，但对短作业不利（相对 SJF 和高响应比）；有利于 CPU 繁忙型作业，而不利于 I/O 繁忙型作业。
        
    2. 短作业（进程）优先（Short Job First，SJF）调度算法
        
        短作业（进程）优先调度算法是指对短作业（进程）优先调度的算法。短作业优先（SJF） 调度算法从后备队列中选择一个或若干估计运行时间最短的作业，将它们调入内存运行；短进程优先（SPF）调度算法从就绪队列中选择一个估计运行时间最短的进程，将处理机分配给它，使之立即执行，直到完成或发生某事件而阻塞时，才释放处理机。
        
        SJF 调度算法的缺点：
        
        - 必须预知作业的运行时间。在采用这种算法时，要先知道每个作业的运行时间。 即使是程序员也很难准确估计作业的运行时间，如果估计过低，系统就可能按估计的时间 终止作业的运行，但此时作业并未完成，故一般都会偏长估计。
        - 对长作业非常不利，长作业的周转时间会明显地增长。更严重的是，该算法完全 忽视作业的等待时间，可能使作业等待时间过长，出现饥饿现象。
        - 该调度算法完全未考虑作业的紧迫程度，故不能保证紧迫性作业能得到及时处理。
        
        注意，SJF 调度算法的平均等待时间、平均周转时间最少。
        
    3. 时间片轮转（Round Robin，RR）调度算法
        
        时间片轮转调度算法主要适用于分时系统。在这种算法中，系统将所有就绪进程按 FCFS 策略排成一个就绪队列，调度程序总是选择就绪队列中的第一个进程执行 ，但仅能运行一个时间片，如 50ms。在使用完一个时间片后，即使进程并未运行完成，它也必须释放出（被剥夺）处理机给下一个就绪进程，而被剥夺的进程返回到就绪队列的末尾重新排队 ，等候再次运行。
        
        在时间片轮转调度算法中，时间片的大小对系统性能的影响很大。若时间片足够大，以至于所有进程都能在一个时间片内执行完毕，则时间片轮转调度算法就退化为先来先服务调度算法。 若时间片很小，则处理机将在进程间过于频繁地切换，使处理机的开销增大，而真正用于运行用 户进程的时间将减少。因此，时间片的大小应选择适当，时间片的长短通常由以下因素确定：系统的响应时间、就绪队列中的进程数目和系统的处理能力。一个较为可取的时间片大小是略大于一次典型的交互所需要的时间，使大多数交互式进程能在一个时间片内完成，从而可以获得很小的响应时间。
        
    4. 基于优先级的调度算法
        
        优先级调度算法既可用于作业调度，又可用于进程调度。该算法中的优先级用于描述作业的紧迫程度。在作业调度中，优先级调度算法每次从后备作业队列中选择优先级最高的一个或几个作业，将它们调入内存，分配必要的资源，创建进程并放入就绪队列。在进程调度中，优先级调度算法每次从就绪队列中选择优先级最高的进程，将处理机分配给它，使之投入运行。
        
        根据新的更高优先级进程能否抢占正在执行的进程，可将该调度算法分为如下两种： 
        
        - 非抢占式优先级调度算法。当一个进程正在处理机上运行时，即使有某个优先级更高的进程进入就绪队列，仍让正在运行的进程继续运行，直到由于其自身的原因而让出处理机时（任务完成或等待事件），才把处理机分配给就绪队列中优先级最高的进程。
        - 抢占式优先级调度算法。当一个进程正在处理机上运行时，若有某个优先级更高的进程进入就绪队列，则立即暂停正在运行的进程，将处理机分配给优先级更高的进程。
        
        而根据进程创建后其优先级是否可以改变，可以将进程优先级分为以下两种：
        
        - 静态优先级。优先级是在创建进程时确定的，且在进程的整个运行期间保持不变。确定静态优先级的主要依据有进程类型、进程对资源的要求、用户要求。
        - 动态优先级。在进程运行过程中，根据进程情况的变化动态调整优先级 。动态调整优先级的主要依据有进程占有 CPU 时间的长短、就绪进程等待 CPU 时间的长短。
        
        一般来说，进程优先级的设置可以参照以下原则：
        
        - 系统进程 ＞ 用户进程。
        - 交互型进程 ＞ 非交互型进程（或前台进程〉后台进程）。
        - I/O 型进程 ＞ 计算型进程。
    5. 高响应比优先调度算法
        
        高响应比优先调度算法主要用于作业调度，是对 FCFS 调度算法和 SJF 调度算法的一种综合平衡，同时考虑了每个作业的等待时间和估计的运行时间。在每次进行作业调度时，先计算后备作业队列中每个作业的响应比，从中选出响应比最高的作业投入运行。 响应比的变化规律可描述为：${\small 响应比 = \frac{等待时间＋要求服务时间}{要求服务时间} = \frac{响应时间}{要求服务时间}}$
        
        根据公式可知：
        
        - 作业的等待时间相同时，要求服务时间越短，响应比越高，有利于短作业因而类似于 SJF。
        - 要求服务时间相同时，作业的响应比由其等待时间决定，等待时间越长，其响应比越高，因而类似于 FCFS。
        - 对于长作业，作业的响应比可以随等待时间的增加而提高，当其等待时间足够长时，也可获得处理机，克服了 “饥饿”现象。
    6. 多级反馈队列（Multilevel Feedback Queue）调度算法
        
        多级反馈队列调度算法是时间片轮转调度算法和优先级调度算法的综合与发展。通过动态调整进程优先级和时间片大小 ，多级反馈队列调度算法可以兼顾多方面的系统目标 。 例如，为提高系统吞吐量和缩短平均周转时间而照顾短进程；为获得较好的 I/O 设备利用率和缩短响应时间而照顾 I/O 型进程；同时，也不必事先估计进程的执行时间。
        
        多级反馈队列调度算法的实现思想如下：
        
        - 设置多个就绪队列，并为每个队列赋予不同的优先级。第1级队列的优先级最高，第2级队列的优先级次之，其余队列的优先级逐个降低。
        - 赋予各个队列的进程运行时间片的大小各不相同。在优先级越高的队列中，每个进程的时间片就越小。例如，第 i+1 级队列的时间片要比第 i 级队列的时间片长1倍。
        - 每个队列都采用 FCFS 算法。当新进程进入内存后，首先将它放入第 1 级队列的末尾，按 FCFS 原则等待调度。当轮到该进程执行时，如它能在该时间片内完成，便可撤离系统。若它在一个时间片结束时尚未完成，调度程序将其转入第 2 级队列的末尾等待调度；若它在第 2 级队列中运行一个时间片后仍未完成，再将它放入第 3 级队列……，依此类推。当进程最后被降到第 n 级队列后，在第 n 级队列中便采用时间片轮转方式运行。
        - 按队列优先级调度。仅当第 1 级队列为空时，才调度第 2 级队列中的进程运行；仅当第 1 ~ i-1 级队列均为空时，才会调度第 i 级队列中的进程运行。若处理机正在执行第 i 级队列 中的某进程时，又有新进程进入任一优先级较高的队列，此时须立即把正在运行的进程 放回到第 i 级队列的末尾，而把处理机分配给新到的高优先级进程。
        
        多级反馈队列的优势有以下几点：
        
        - 终端型作业用户：短作业优先。
        - 短批处理作业用户：周转时间较短。
        - 长批处理作业用户：经过前面几个队列得到部分执行，不会长期得不到处理。
    7. 进程调度算法对比（待补充）
        
        
        | 调度算法 | 调度方式 | 优点 | 缺点 | 适用于 | 是否存在进程饥饿现象 |
        | --- | --- | --- | --- | --- | --- |
        | 先来先服务 | 非抢占式 | 公平，实现简单 | 不利于短作业 | 无 | 否。不管等多久，所有进程最后都会运行，不存在某个进程永远得不到处理机的情况 |
        | 短作业优先 | 抢占式或非抢占式（默认） | 平均等待时间最少，效率最高 | 长作业会饥饿，估计时间不易确定 | 作业调度，批处理系统 | 是。忽视长作业的等待时间，可能使长作业等待时间过长，出现饥饿现象。 |
        | 高响应比优先 | 抢占式或非抢占式（默认） | 兼顾长短作业。
        有利于短小作业，兼顾到长作业。 | 计算响应比的开销大 | 无 | 否。响应比会随着等待时间增加而变大，优先级会提高，能够避免饥饿现象。 |
        | 时间片轮转 | 抢占式 | 兼顾长短作业 | 平均等待时间较长，上下文切换浪费时间 | 分时系统 | 否。时间片一到自然就会切换到其它进程，不存在某个进程永远无法被调度的情况。 |
        | 多级反馈队列 | 抢占式 | 兼顾长短作业，有较好的响应时间，可行性强 | 可能会导致饥饿 | 相当通用 | 是。可能会导致饥饿。若有源源不断的短进程到达第一队列，那么这些进程会持续被调度，使得下面一级的那些进程一直得不到调度，导致饥饿现象的发生。 |
    
    ---
    

### 2.5 死锁：死锁的基本概念，死锁定理、死锁预防、死锁避免与处理死锁的基本方法、银行家算法 ★★★★

- **2.5.1 死锁的基本概念**
    1. 死锁的定义
        
        死锁是指两个或两个以上的进程在执行过程中，由于竞争资源或者由于彼此通信而造成的一种阻塞的现象，若无外力作用，它们都将无法推进下去。
        
    2. 死锁产生的原因
        
        竞争资源（不可抢占性及可消耗性资源）、进程推进顺序不当。
        
    3. 产生死锁的必要条件
        - 互斥条件：进程要求对所分配的资源（如打印机）进行排他性使用，即在一段时间内某资源仅为一个进程所占有。此时若有其他进程请求该资源，则请求进程只能等待。
        - 请求和保持条件：进程已经保持了至少一个资源，但又提出了新的资源请求，而该资源已被其它进程占有，此时请求进程被阻塞，但对自己已获得的资源保持不放。
        - 不可抢占（剥夺）条件：进程已获得的资源在未使用完之前不能被抢占，只能在进程使用完时由自己释放。
        - 循环等待条件：在发生死锁时，必然存在一个进程——资源的循环链，即进程集合 ${\small \{P_0 ,P_1, P_2, \dots ,P_n\}}$ 中的 ${\small P_0}$ 正在等待一个 ${\small P_1}$ 占用的资源，${\small P_1}$ 正在等待 ${\small P_2}$ 占用的资源…… ${\small P_n}$ 正在等待被 ${\small P_0}$ 占用的资源。
    4. 处理死锁的方法
        
        为使系统不发生死锁，必须设法破坏产生死锁的 4 个必要条件之一，或允许死锁产生，但当死锁发生时能检测出死锁，并有能力实现恢复。
        
        - 死锁预防：设置某些限制条件，破坏产生死锁的 4 个必要条件中的一个或几个。其属于事先预防策略，限制条件比较严格，实现起来较为简单，但往往导致系统的效率低，资源利用率低。
        - 死锁避免：在资源的动态分配过程中，用某种方法防止系统进入不安全状态。属于事先预防策略，限制条件相对宽松，资源分配后需要通过算法来判断是否进入不安全状态，实现起来较为复杂。
        - 死锁检测与解除：这种方法无须采取任何限制性措施，允许进程在运行过程中发生死锁。通过系统的检测机构及时地检测出死锁的发生 ，然后采取某种措施解除死锁。常用的死锁解除方法是撤消一些进程，回收它们的资源，并分配给已处于阻塞状态的进程，使其能继续运行。
    
    ---
    
- **2.5.2 死锁预防**
    
    防止死锁的发生只需破坏死锁产生的 4 个必要条件之一即可。
    
    1. 破坏“互斥”条件
        
        若允许系统资源都能共享使用，则系统不会进入死锁状态。但有些资源根本不能同时访问， 如打印机等临界资源只能互斥使用。所以，破坏互斥条件而预防死锁的方法不太可行，而且在有的场合应该保护这种互斥性。
        
    2. 破坏“请求和保持”条件
        
        采用预先静态分配方法，即进程在运行前一次申请完它所需要的全部资源，在它的资源未满足前，不把它投入运行。一旦投入运行，这些资源就一直归它所有，不再提出其他资源请求，这样就可以保证系统不会发生死锁。 这种方式实现简单，但系统资源被严重浪费，其中有些资源可能仅在运行初期或运行快结束时才使用，甚至根本不使用。而且还会导致“饥饿”现象，由于个别资源长期被其他进程占用时，将致使等待该资源的进程迟迟不能开始运行。
        
        第二种方法是对上述方法的改进，它允许一个进程只获得运行初期所需的资源后，便开始运行。进程运行过程中再逐步释放已分配给自己的、且已用毕的全部资源，然后再请求新的所需资源。
        
    3. 破坏“不可抢占（剥夺）”条件
        
        当一个已保持了某些不可抢占资源的进程请求新的资源而得不到满足时 ，它必须释放已经保持的所有资源，待以后需要时再重新申请。这意味着，一个进程已占有的资源会被暂时释放（被抢占），或从而破坏了不可抢占条件。 该策略实现起来比较复杂，释放已获得的资源可能造成前一阶段工作的失效，反复地申请和释放资源会增加系统开销，降低系统吞吐量。这种方法常用于状态易于保存和恢复的资源，如 CPU 的寄存器及内存资源，一般不能用于打印机之类的资源。
        
    4. 破坏“循环等待”条件
        
        采用资源有序分配策略（或称顺序资源分配法），给系统中的资源编号，规定每个进程必须按编号递增的顺序请求资源。如果需要多个同类资源单元， 则必须一起请求。也就是说，只要进程提出申请分配资源 $\small R_i$ 则该进程在以后的资源申请中就只能申请编号大于 $\small R_i$ 的资源。这种方法存在的问题是，编号必须相对稳定。这就限制了新类型设备的增加；尽管在为资源编号时已考虑到大多数作业实际使用这些资源的顺序，但也经常会发生作业使用资源的顺序与系统规定顺序不同的情况，造成资源的浪费；此外，这种按规定次序申请资源的方法，也必然会给用户的编程带来麻烦。
        
    
    ---
    
- **2.5.3 死锁避免、银行家算法**
    1. 死锁避免
        
        避免死锁属于事先预防策略，但并不是事先采取某种限制措施破坏死锁的必要条件，而是在资源动态分配过程中，防止系统进入不安全状态，以避免发生死锁。这种方法所施加的限制条件较弱，可以获得较好的系统性能。
        
        在该方法中，允许进程动态地申请资源，但系统在进行资源分配之前，应先计算此次分配的安全性。若此次分配不会导致系统进入不安全状态，则允许分配；否则让进程等待。
        
        所谓安全状态，是指系统能按某种进程推进顺序 $\small (P_1, P_2, \dots , P_n)$ 为每个进程 $\small P_i$，分配其所需资源，直至满足每个进程对资源的最大需求，使每个进程都可顺利地完成。此时称 $\small (P_1, P_2, \dots , P_n)$ 为安全序列。如果系统无法找到这样一个安全序列，则称系统处于不安全状态。虽然并非所有不安全状态都必然会转为死锁状态，但当系统进入不安全状态后，就有可能进入死锁状态。反之，只要系统处于安全状态，系统便不会进入死锁状态。
        
    2. 银行家算法
        
        银行家算法是最著名的死锁避免算法，其思想是：把操作系统视为银行家，操作系统管理的资源相当于银行家管理的资金，进程向操作系统请求分配资源相当于用户向银行家贷款。操作系统按照银行家制定的规则为进程分配资源。进程运行之前先声明对各种资源的最大需求量，当进程在执行中继续申请资源时，先测试该进程已占用的资源数与本次申请的资源数之和是否超过该 进程声明的最大需求量。若超过则拒绝分配资源，若未超过则再测试系统现存的资源能否满足该进程尚需的最大资源量，若能满足则按当前的申请量分配资源，否则也要推迟分配。
        
        1. 数据结构描述
            
            可利用资源向量 Available：含有 m 个元素的数组，其中每个元素代表一类可用的资源数目。 Available[j] = K 表示系统中现有 $\small \mathrm{R_j}$ 类资源 K 个。
            
            最大需求矩阵 Max：n × m 矩阵，定义系统中 n 个进程中的每个进程对 m 类资源的最大需求。简单来说，一行代表一个进程，一列代表一类资源。Max[i, j] = K 表示进程 i 需要 $\small \mathrm{R_j}$ 类资源的最大数目为 K。
            
            分配矩阵 Allocation：n × m 矩阵，定义系统中每类资源当前已分配给每个进程的资源数。Allocation[i, j] =K 表示进程 i 当前已分得 $\small \mathrm{R_j}$ 类资源的数目为 K。
            
            需求矩阵 Need： n × m 矩阵，表示每个进程接下来最多还需要多少资源。Need[i, j] = K 表示进程 i 还需要 $\small \mathrm{R_j}$ 类资源的数目为 K。
            
            上述三个矩阵间的关系：Need = Max- Allocation
            
            一般情况下，在银行家算法的题目中，Max 矩阵和 Allocation 矩阵是已知条件，而求岀 Need 矩阵是解题的第一步。
            
        2. 银行家算法描述
            
            设 $\small \mathrm{Request_i}$,是进程 $\small \mathrm{P_i}$ 的请求向量，$\small \mathrm{Request_i[j]=K}$ 表示进程 $\small \mathrm{P_i}$ 需要 $\small \mathrm{R_j}$ 类资源 K 个。当 $\small \mathrm{P_i}$ 发出资源请求后，系统按下述步骤进行检查：
            
            - ① 若 $\small \mathrm{Request_i[j] \le Need[i,j]}$，则转向步骤 ②；否则认为出错，因为它所需要的资源数已超过它所宣布的最大值。
            - ② 若 $\small \mathrm{Request_i[j] \le Available[j]}$，则转向步骤 ③；否则，表示尚无足够资源，$\small \mathrm{P_i}$ 须等待。
            - ③ 系统试探着把资源分配给进程 $\small \mathrm{P_i}$ 并修改下面数据结构中的数值：
                - $\small \mathrm{Available[j] = Available[j] - Request_i[j]}$；
                - $\small \mathrm{Allocation[i,j]= Allocation[i,j]+Request_i[j]}$；
                - $\small \mathrm{Need[i,j] = Need[i, j] - Request_i[j]}$；
            - ④ 系统执行安全性算法，检查此次资源分配后，系统是否处于安全状态。若安全，才正式将资源分配给进程 $\small \mathrm{P_i}$ 以完成本次分配；否则，将本次的试探分配作废，恢复原来的资源分配状态，让进程 $\small \mathrm{P_i}$ 等待。
        3. 安全性算法
            
            设置工作向量 Work，有 m 个元素，表示系统中的剩余可用资源数目。在执行安全性算法开始时，Work = Available。
            
            - ① 初始时安全序列为空。
            - ② 从 Need 矩阵中找出符合下面条件的行：该行对应的进程不在安全序列中，而且该行小于等于 Work 向量，找到后，把对应的进程加入安全序列；若找不到，则执行步骤 ④。
            - ③ 进程 R 进入安全序列后，可顺利执行，直至完成，并释放分配给它的资源，因此应执行 Work = Work + Allocation[i]，其中 Allocation[i] 表示进程 $\small \mathrm{P_i}$ 代表的在 Allocation 矩阵中对应的行。返回步骤 ②。
            - ④ 若此时安全序列中已有所有进程，则系统处于安全状态，否则系统处于不安全状态。
        4. 安全性算法举例
            
            假定系统中有 5 个进程 $\small \{P_0, P_1, P_2, P_3, P_4\}$ 和三类资源 {A, B, C}，各种资源的数量分别为 10，5，7。在 $\small T_0$ 时刻的资源分配情况如下表。
            
            ![ $\small T_0$ 时刻的资源分配表](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%208.png)
            
             $\small T_0$ 时刻的资源分配表
            
            利用安全性算法对 $\small T_0$ 时刻的资源分配进行分析。
            
            - ① 从题目中我们可以提取 Max 矩阵和 Allocation 矩阵，这两个矩阵相减可得到 Need 矩阵：
                
                ![Untitled](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%209.png)
                
            - ② 然后，将 Work 向量 (3, 3, 2) 与 Need 矩阵的各行进行比较，找出比 Work 矩阵小的行。
                
                例如，在初始时：(3, 3, 2) > (1, 2, 2)；(3, 3, 2) > (0, 1, 1)。
                
                对应的两个进程分别为 $\small \mathrm{P_1}$ 和 $\small \mathrm{P_3}$，这里我们选择 $\small \mathrm{P_1}$（也可以选择 $\small \mathrm{P_1}$）暂时加入安全序列。
                
            - ③ 释放 $\small \mathrm{P_1}$ 所占的资源，即把 $\small \mathrm{P_1}$ 进程对应的 Allocation 矩阵中的一行与 Work 向量相加：Work = (3, 3, 2) + (2, 0, 0) = (5, 3, 2)。此时需求矩阵更新为(去掉了 $\small \mathrm{P_1}$ 对应的一行)：
                
                ![Untitled](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2010.png)
                
            - 再用更新的 Work 向量和 Need 矩阵重复步骤 ②。利用安全性算法分析 $\small T_0$ 时刻的资源分配情况如下表所示，最后得到一个安全序列 $\small \{P_1, P_3, P_4, P_2, P_0\}$。
                
                ![ $\small T_0$ 时刻安全序列的分析](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2011.png)
                
                 $\small T_0$ 时刻安全序列的分析
                
        5. 银行家算法举例
            
            安全性算法是银行家算法的核心，在银行家算法的题目中，一般会有某个进程的一个资源请求向量，只要执行上面所介绍的银行家算法的前三步 ，马上就会得到更新的 Allocation 矩阵和 Need 矩阵，再按照上例的安全性算法判断，就能知道系统能否满足进程提出的资源请求。 
            
            假设当前系统中资源的分配和剩余情况如下表所示。
            
            ![ $\small T_0$ 时刻的资源分配表](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%208.png)
            
             $\small T_0$ 时刻的资源分配表
            
            - ① $\small \mathrm{P_1}$ 请求资源：$\small \mathrm{P_1}$ 发出请求向量 $\small \mathrm{Request_1(1,0,2)}$
                - 系统按银行家算法进行检查
                    - $\small \mathrm{Request_1(1,0,2) \le Need_1(l,2,2)}$
                    - $\small \mathrm{Request_1(l, 0, 2) \le Available(3, 3, 2)}$
                - 系统先假定可为 $\small \mathrm{P_1}$ 分配资源，并修改有关数据
                    - $\small \mathrm{Available = Available-Request_1 = (2, 3, 0) }$
                    - $\small \mathrm{Allocation_1 = Allocation_1 + Request_1 = (3, 0, 2)}$
                    - $\small \mathrm{Need_1 = Need_1 - Request_1 = (0, 2, 0)}$
                    
                    由此形成的资源变化情况如上表中的圆括号所示。 
                    
                - 令 $\small \mathrm{Work = Available = (2,3,0)}$，再利用安全性算法检查此时系统是否安全，如下表所示。
                    
                    ![$\small \mathrm{P_1}$ 申请资源时的安全性检查](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2012.png)
                    
                    $\small \mathrm{P_1}$ 申请资源时的安全性检查
                    
                    由所进行的安全性检查得知，可找到一个安全序列 $\small \mathrm{\{P_1,P_3,P_4,P_0,P_2\}}$。因此，系统是安全的，可以立即将 $\small \mathrm{P_1}$ 所申请的资源分配给它。分配后系统中的资源情况如下表所示。
                    
                    ![为 $\small \mathrm{P_1}$ 分配资源后的有关资源数据](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2013.png)
                    
                    为 $\small \mathrm{P_1}$ 分配资源后的有关资源数据
                    
            - ② $\small \mathrm{P_4}$ 请求资源：$\small \mathrm{P_4}$ 发出请求向量 $\small \mathrm{Request_4(3,3,0)}$
                - 系统按银行家算法进行检查
                    - $\small \mathrm{Request_4(3, 3, 0) \le Need_4(4, 3, 1)}$
                    - $\small \mathrm{Request_4(3, 3, 0) > Available(2, 3, 0)}$
                    
                    资源不足，让 P4 等待。
                    
            - ③ $\small \mathrm{P_0}$ 请求资源：$\small \mathrm{P_0}$ 发出请求向量 $\small \mathrm{Request_0(0, 2, 0)}$
                - 系统按银行家算法进行检查
                    - $\small \mathrm{Request_0(0, 2, 0) \le Need_0(7, 4, 3)}$
                    - $\small \mathrm{ Request_0(0, 2, 0) \le Available(2, 3, 0)}$
                - 系统暂时先假定可为 $\small \mathrm{P_0}$ 分配资源，并修改有关数据
                    - $\small \mathrm{Available = Available - Request_0 = (2, 1, 0) }$
                    - $\small \mathrm{Allocation_0 = Allocation_0 + Request_0 = (0, 3, 0)}$
                    - $\small \mathrm{Need_0 = Need_0 -Request_0 =(7, 2, 3)}$
                    
                    结果如下表所示。
                    
                    ![为 $\small \mathrm{P_0}$ 分配资源后的有关资源数据](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2014.png)
                    
                    为 $\small \mathrm{P_0}$ 分配资源后的有关资源数据
                    
                - 进行安全性检查：可用资源 $\small \mathrm{Available(2, 1, 0)}$ 已不能满足任何进程的需要，系统进入不安全状态，因此拒绝 $\small \mathrm{P_0}$ 的请求，让 $\small \mathrm{P_0}$ 等待，并将 $\small \mathrm{Available}$，$\small \mathrm{Allocation_0}$，$\small \mathrm{Need_0}$ 恢复为之前的值。
    
    ---
    
- **2.5.4 死锁检测与解除、死锁定理**
    
    前面介绍的死锁预防和避免算法，都是在为进程分配资源时施加限制条件或进行检测，若系统为进程分配资源时不采取任何措施，则应该提供死锁检测和解除的手段。
    
    1. 资源分配图
        
        系统死锁可利用资源分配图来描述。如下图所示，用圆圈代表一个进程，用框代表一类资源。由于一种类型的资源可能有多个，因此用框中的一个圆代表一类资源中的一个资源。从进程到资源的有向边称为请求边，表示该进程申请一个单位的该类资 源；从资源到进程的边称为分配边，表示该类资源已有一个资源分配给了该进程。
        
        ![资源分配示例](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2015.png)
        
        资源分配示例
        
        在上图所示的资源分配图中，进程 $\small P_1$ 已经分得了两个 $\small R_1$ 资源，并又请求一个 $\small R_2$ 资源；进程 $\small P_2$ 分得了一个 $\small R_1$ 资源和一个 $\small R_2$ 资源，并又请求一个 $\small R_1$ 资源。
        
    2. 死锁定理
        
        简化资源分配图可检测系统状态 S 是否为死锁状态。简化方法如下：
        
        - ① 在资源分配图中，找出一个既不阻塞（所请求的资源充足）又非独立的进程结点 $\small P_i$。在顺利的情况下，$\small P_i$ 可获得所需资源而继续运行，直至运行完毕，再释放其所占有的全部资源，这相当于消去 $\small P_i$ 的请求边和分配边，使之成为孤立的结点。在下图 (a) 中，将 $\small P_1$ 的两个分配边和一个请求边消去，便形成图 (b) 所示的情况。
            
            ![资源分配图的简化](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2016.png)
            
            资源分配图的简化
            
        - ② $\small P_1$ 释放资源后，便可使 $\small P_2$ 获得资源而继续运行，直至 $\small P_2$ 完成后又释放出它所占有的全部资源，形成图 (c) 所示的情况，即将 $\small P_2$ 的一条请求边和两条分配边消去。
        - ③ 在进行一系列的简化后，若能消去图中所有的边，使所有的进程结点都成为孤立结点，则称该图是可完全简化的；若不能通过任何过程使该图完全简化，则称该图是不可完全简化的。
        
        S 为死锁的条件是当且仅当状态的资源分配图是不可完全简化的，该条件为死锁定理。
        
    3. 死锁解除
        
        一旦检测出死锁，就应立即采取相应的措施来解除死锁。死锁解除的主要方法有：
        
        - 资源剥夺法。挂起某些死锁进程，并抢占它的资源，将这些资源分配给其他的死锁进程。但应防止被挂起的进程长时间得不到资源而处于资源匮乏的状态。
        - 撤销进程法。强制撤销部分甚至全部死锁进程并剥夺这些进程的资源。撤销的原则可以按进程优先级和撤销进程代价的高低进行。
        - 进程回退法。让一（或多）个进程回退到足以回避死锁的地步，进程回退时自愿释放资源而非被剥夺。要求系统保持进程的历史信息，设置还原点。
    
    ---
    

### 2.6 综合应用：生产者消费者问题、读者和写者问题、哲学家进餐问题等 ★★★★★

注：信号量机制及 P、V 操作参考 [2.2.3 信号量机制及 P、V 操作](https://www.notion.so/d6789a488c934a009190634bba8e1811)

- **2.6.1 生产者-消费者问题**
    
    TODO：
    
    ---
    
- **2.6.2 读者-写者问题**
    1. 读写公平
    2. 读者优先
    3. 写者优先
    
    ```c
    // 写者优先算法
    // 有写者正在写时，若等待队列中有写者到来时，当前写者退出后依然先进行写，即使有读者先到
    // 有读者正在读时，写者到来后会使得后续的读者阻塞，读结束后先进行写
    int read_count = 0;  // 记录读者数量
    int write_count = 0; // 记录写者数量
    semaphore rc_mutex = 1; // 保证对 read_count 的互斥访问
    semaphore wc_mutex = 1; // 保证对 write_count 的互斥访问
    semaphore rw = 1; // 保证对文件的互斥访问
    semaphore writer_first = 1; // 保证写者优先
    
    writer(){
        while(1){
            // 这里与读写公平法不同，当有写者到来时，第一个写者 P(writer_first)
            // 使得后来的 reader 阻塞，直到最后一个写者退出
            P(wc_mutex);
            if(write_count == 0)
                P(writer_first);
            write_count++;
            V(wc_mutex);
            
            P(rw);
            writing;
            V(rw);
            
            // 最后一个写者才 V(writer_first) 保证了写者优先
            P(wc_mutex);
            write_count--;
            if(write_count == 0)
                V(writer_first);
            V(wc_mutex);
        }
    }
    
    reader(){
        while(1){
            // 只要有写者在进行写，读者就会阻塞在 P(writer_first)，直到最后一个写者退出
            // 若没有写者写，可直接进行读
            P(writer_first);
            P(rc_mutex);
            if(read_count == 0)
                P(rw);
            read_count++;
            V(rc_mutex);
            V(writer_first);
            
            reading;
            
            P(rc_mutex);
            read_count--;
            if(read_count == 0)
                V(rw);
            V(rc_mutex);
        }
    }
    ```
    
    ---
    
- **2.6.3 哲学家进餐问题**
    
    问题描述：一张圆桌边上坐着 5 名哲学家，每两名哲学家之间的桌上摆一根筷子，两根筷子中间是一碗米饭，如图所示。哲学家们倾注毕生精力用于思考和进餐，哲学家在思考时，并不影响他人。只有当哲学家饥饿时，才试图拿起左、右两根筷子（一根一根地 拿起）。若筷子已在他人手上，则需要等待。饥饿的哲学家只有同时拿到了两根筷子才可以开始进餐，进餐完毕后，放下筷子继续思考。
    
    ![5 名哲学家进餐](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2017.png)
    
    5 名哲学家进餐
    
    问题分析:
    
    - ① 关系分析。5 名哲学家与左右邻居对其中间筷子的访问是互斥关系。
    - ② 整理思路。显然，这里有5个进程。本题的关键是如何让一名哲学家拿到左右两根筷子而不造成死锁或饥饿现象。解决方法有两个：一是让他们同时拿两根筷子；二是对每名哲学家的动作制定规则，避免饥饿或死锁现象的发生。
    - ③ 信号量设置。定义互斥信号量数组 $\small \mathrm{chopstick[5] = \{1, 1, 1, 1, 1\}}$，用于对 5 个筷子的互斥访问。哲学家按顺序编号为 0〜4，哲学家 $\small i$ 左边筷子的编号为 $\small i$，哲学家右边筷子的编号为 $\small (i + 1) \% 5$。
    
    ```c
    // 该算法会出现死锁！
    semaphore chopstick[5] = {1, 1, 1, 1, 1}; //定义信号量数组 chopstick [5],并初始化
    
    Pi() {                         // i号哲学家的进程
        do{ 
            P(chopstick[i]);       // 取左边筷子
            P(chopstick[(i+1)%5]); // 取右边筷子
            eat;                   // 进餐
            V(chopstick[i]);       // 放回左边筷子
            V(chopstick[(i+1)%5]); // 放回右边筷子
            think;                 // 思考
        } while(1);
    }
    ```
    
    该算法存在以下问题：当 5 名哲学家都想要进餐并分别拿起左边的筷子时（都恰好执行完 `P(chopstick[i])`）筷子已被拿光，等到他们再想拿右边的筷子时（执行 `P(chopstick[(i + 1)%5])`）就全被阻塞，因此出现了死锁。
    
    为防止死锁发生，可对哲学家进程施加一些限制条件，有以下几种方法：
    
    - ① 至多允许 4 名哲学家同时进餐；
    - ② 仅当一名哲学家左右两边的筷子都可用时，才允许他抓起筷子；
    - ③ 对哲学家顺序编号，要求奇数号 哲学家先拿左边的筷子，然后拿右边的筷子，而偶数号哲学家刚好相反。
    
    假设采用第二种方法，当一名哲学家左右两边的筷子都可用时 ，才允许他抓起筷子。算法如下：
    
    ```c
    // 通过记录型信号量实现
    // 新增 mutex 互斥信号量，每次只允许一个哲学家去拿筷子
    semaphore chopstick[5] = {1, 1, 1, 1, 1}; // 初始化信号量 
    semaphore mutex = 1; // 设置取筷子的信号量
    
    Pi() { // i 号哲学家的进程 
        do { 
            P(mutex);               // 在取筷子前获得互斥量 
            P(chopstick[i]);        // 取左边筷子 
            P(chopstick[(i+1)%5]);  // 取右边筷子 
    				V(mutex);               // 释放取筷子的信号量 
            eat;                    // 进餐 
            V(chopstick[i]);        // 放回左边筷子 
            V(chopstick[(i+1)%5]);  // 放回右边筷子 
            think;                  // 思考 
        } while(1);
    }
    
    // 通过记录型信号量和 AND 型信号量机制实现
    semaphore chopstick[5] = {1, 1, 1, 1, 1}; // 初始化信号量 
    Pi() { // i 号哲学家的进程 
        do { 
            Sswait(chopstick[(i+1)%5], chopstick[i]);  // 同时取左右两根筷子 
            eat;                                       // 进餐 
            Ssignal(chopstick[(i+1)%5], chopstick[i]); // 释放两根筷子 
            think;                                     // 思考 
        } while(1);
    }
    ```
    
    ---
    
- **2.6.4 吸烟者问题**
    
    ---
    
- **2.6.5 P、V 操作典型考题**
    
    ---
    

## 3. 内存管理

### 3.1 内存管理的需求：重定位、内存保护、内存共享 ★

- **3.1.1 重定位（地址转换）**
    
    重定位是把程序的逻辑地址空间变换成内存中的实际物理地址空间的过程。它是实现多道程序在内存中同时运行的基础。重定位有两种，分别是静态重定位和动态重定位。详见 [3.2.1 程序的装入](https://www.notion.so/d6789a488c934a009190634bba8e1811)。
    
    ---
    
- **3.1.2 内存保护**
    
    防止某个进程去访问不是操作系统配置给它的寻址空间 ，确保每个进程都有一个单独的内存空间。内存分配前，需要保护操作系统不受用户进程的影响，同时保护用户进程不受其他用户进程的影响 。内存保护可采取两种方法：
    
    1. 在 CPU 中设置一对上、下限寄存器，存放用户作业在主存中的下限和上限地址，每当 CPU 要访问一个地址时，分别和两个寄存器的值相比，判断有无越界。
        
        注意：该方案适用于分区式存储管理。
        
    2. 采用重定位寄存器（又称基地址寄存器）和界地址寄存器（又称限长寄存器）来实现这种保护。重定位寄存器含最小的物理地址值，界地址寄存器含逻辑地址的最大值。内存管理机构动态地将逻辑地址与界地址寄存器进行比较 ，若未发生地址越界，则加上重定位寄存器的值后映射成物理地址，再送交内存单元。
        
        ![重定位寄存器和界地址寄存器的硬件支持](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2018.png)
        
        重定位寄存器和界地址寄存器的硬件支持
        
    
    ---
    
- **3.1.3 内存共享**
    
    允许多个进程访问内存的相同部分。例如，多个合作进程可能需要访问同一块数据，因此必须支持对内存共享区域进行受控访问。
    
    并不是所有的进程内存空间都适合共享，只有那些只读的区域才可以共享。但在实际执行时，也可以为每个进程配以局部数据区，把在执行中可能改变的部分复制到该数据区。这样，程序在执行 时只需对该私有数据区中的内存进行修改，并不去改变共享的代码。
    
    注意：可重入代码又称纯代码，是一种允许多个进程同时访问但不允许被任何进程修改的代码 。
    
    ---
    

### 3.2 程序的装入和链接：静态装入和可重定位装入、静态链接、动态链接、运行时动态链接 ★★

用户程序要在系统中运行，必须先将它装入内存，然后再将其转变为一个可以执行的程序，通常都要经过以下几个步骤：

- 编译：由编译程序将用户源代码编译成若干个目标模块。
- 链接：由链接程序将编译后形成的一组目标模块及它们所需的库函数链接在一起 ，形成一 个完整的装入模块。
- 装入：由装入程序将装入模块装入内存。

![将用户程序变为可在内存中执行的程序的步骤](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2019.png)

将用户程序变为可在内存中执行的程序的步骤

- **3.2.1 程序的装入**
    1. 绝对装入
        
        绝对装入方式只适用于单道程序环境。在编译时，若知道程序将驻留在内存的某个位置，则编译程序将产生绝对地址的目标代码。绝对装入程序按照装入模块中的地址，将程序和数据装入内存。由于程序中的逻辑地址与实际内存地址完全相同 ，因此不需对程序和数据的地址进行修改。
        
        另外，程序中所用的绝对地址，可在编译或汇编时给出，也可由程序员直接赋予。而通常情况下在程序中采用的是符号地址，编译或汇编时再转换为绝对地址。
        
    2. 可重定位装入（静态重定位）
        
        在多道程序环境下，多个目标模块的起始地址通常都从 0 开始，程序中的其他地址都是相对于起始地址的，此时应釆用可重定位装入方式。根据内存的当前情况，将装入模块装入内存的适当位置。在装入时对目标程序中指令和数据地址的修改过程称为重定位，又因为地址变换通常是在进程装入时一次完成的，故称为静态重定位，如图 (a) 所示。
        
        当一个作业装入内存时，必须给它分配要求的全部内存空间，若没有足够的内存，则无法装入。此外，作业一旦进入内存，整个运行期间就不能在内存中移动，也不能再申请内存空间。
        
        简言之，静态重定位在程序装入内存的过程中完成地址转换。
        
    3. 动态运行时装入（动态重定位）
        
        也称动态重定位。程序在内存中若发生移动，则需要采用动态的装入方式。装入程序把装入模块装入内存后，并不立即把装入模块中的相对地址转换为绝对地址，而是把这种地址转换推迟到程序真正要执行时才进行。因此，装入内存后的所有地址均为相对地址。这种方式需要一个重定位寄存器的支持，如图 (b) 所示。
        
        动态重定位的优点：可以将程序分配到不连续的存储区；在程序运行之前可以只装入部分代码即可投入运行，然后在程序运行期间，根据需要动态申请分配内存；便于程序段的共享。
        
        ![重定位](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/%25E9%259D%2599%25E6%2580%2581%25E4%25B8%258E%25E5%258A%25A8%25E6%2580%2581%25E9%2587%258D%25E5%25AE%259A%25E4%25BD%258D.png)
        
        重定位
        
        简言之，动态重定位在 CPU 每次访问时，由动态地址变换机构进行地址转换。
        
    
    ---
    
- **3.2.2 程序的链接**
    1. 静态链接
        
        是指在程序运行之前，将各目标模块及它们所需的库函数，链接成一个完整的装入模块，以后不再拆开的方式。
        
    2. 装入时动态链接
        
        指链接在装入时进行，即在装入一个目标模块时，若发生一个外部模块调用事件，则由装入程序找出相应的外部模块，将它装入内存，并把它链接到调用者模块上去。这种链接方式的优点是便于对程序模块进行修改和更新，并且可以对外存中的目标模块实现共享。
        
    3. 运行时动态链接
        
        指链接在运行时进行，即在执行过程中，若发现一个被调用模块尚未装入内存，便立即由 OS 找出该模块，将它装入内存，并把它链接到调用者模块上。这种链接方式除了便于实现目标模块的修改、更新和共享外，还会加快程序的装入过程；由于它不会将本次执行过程中调用不到的模块装入内存，因此能提高内存的利用率。
        
    
    ---
    

### 3.3 分区存储管理：分区方式（单一连续分区、固定分区、可变式分区）、分区分配算法（首次适应算法、循环首次适应算法、最佳适应法、最坏适应法等）★★★★

- **3.3.1 分区方式**
    
    连续分配方式是指为一个用户程序分配一个连续的内存空间，譬如某用户需要 100MB 的内 存空间，连续分配方式就在内存空间中为用户分配一块连续的 100MB 空间。连续分配方式主要包括单一连续分配、固定分区分配和动态分区分配（可变式分区分配）。
    
    1. 单一连续分配
        
        内存在此方式下分为系统区和用户区 ，系统区仅供操作系统使用，通常在低地址部分；在用户区内存中，仅有一道用户程序，即整个内存的用户空间由该程序独占。
        
        这种方式的优点是简单、无外部碎片，无须进行内存保护，因为内存中永远只有一道程序。 缺点是只能用于单用户、单任务的操作系统中，有内部碎片，存储器的利用率极低。
        
    2. 固定分区分配
        
        固定分区分配是最简单的一种多道程序存储管理方式，它将用户内存空间划分为若干固定大小的区域，每个分区只装入一道作业。当有空闲分区时，便可再从外存的后备作业队列中选择适当大小的作业装入该分区，如此循环。在划分分区时有两种不同的方法：
        
        - 分区大小相等：程序太小会造成浪费（产生内部碎片），程序太大又无法装入，缺乏灵活性。
        - 分区大小不等：划分为多个较小的分区、适量的中等分区和少量大分区。
        
        为便于内存分配，通常将分区按大小排队，并为之建立一张分区说明表，其中各表项包括每个分区的始址、大小及状态。当有用户程序要装入时，便检索该表，以找到合适的分区给予分配并将其状态置为“已分配”；未找到合适分区时，则拒绝为该程序分配内存。
        
    3. 动态分区分配（可变式分区分配）
        
        又称可变分区分配，它是在进程装入内存时，根据进程的实际需要，动态地为之分配内存， 并使分区的大小正好适合进程的需要。因此，系统中分区的大小和数目是可变的。
        
        ![动态分区分配](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2020.png)
        
        动态分区分配
        
        如图所示，系统有 64MB 内存空间，其中低 8MB 固定分配给操作系统，其余为用户可 用内存。开始时装入前三个进程，它们分别分配到所需的空间后，内存仅剩4MB，进程 4 无法装入。在某个时刻，内存中没有一个就绪进程，CPU出现空闲，操作系统就换出进程 2，换入进程 4。由于进程4比进程 2 小，这样在主存中就产生了一个 6MB 的内存块。之后 CPU 又出现空闲， 需要换入进程 2，而主存无法容纳进程2，操作系统就换出进程 1，换入进程 2。
        
        动态分区在开始时是很好的，但随着时间的推移，内存中会产生越来越多小的内存块，内存的利用率也随之下降。这些小的内存块称为外部碎片，它存在于所有分区的外部，这与固定分区中的内部碎片正好相对。
        
        克服外部碎片可以通过紧凑技术来解决，即操作系统不时地对进程进行移动和整理。紧凑后的用户程序在内存中的位置发生了变化，此时若不对程序和数据的地址加以修改（变换），程序将无法执行，故需要重定位，相对费时。还可以通过重定位寄存器进行动态重定位，将能很好地解决此问题。
        
        在进程装入或换入主存时，若内存中有多个足够大的空闲块，则操作系统必须确定分配哪个内存块给进程使用，这就是动态分区的分配策略，详见：[3.3.2 分区分配算法](https://www.notion.so/d6789a488c934a009190634bba8e1811)。
        
        在动态分区分配中，与固定分区分配类似，设置一张空闲分区链（表），并按始址排序。分配内存时，检索空闲分区链，找到所需的分区，若其大小大于请求大小，便从该分区中按请求大小分割一块空间分配给装入进程（若剩余部分小到不足以划分，则无须分割），余下部分仍留在空闲分区链中。回收内存时，系统根据回收分区的始址，从空闲分区链中找到相应的插入点，此时可能出现四种情况：
        
        - 回收区与插入点的前一空闲分区相邻，将这两个分区合并，并修改前一 分区表项的大小为两者之和；
        - 回收区与插入点的后一空闲分区相邻，将这两个分区合并，并修改后一分区表项的始址和大小；
        - 回收区同时与插入点的前、后两个分区相邻，此时将这三个分 区合并，修改前一分区表项的大小为三者之和，取消后一分区表项；
        - 回收区没有相邻的空闲分 区，此时应为回收区新建一个表项，填写始址和大小，并插入空闲分区链。
    
    ---
    
- **3.3.2 分区分配算法（动态分区分配算法）**
    1. 首次适应（First Fit，FF）算法。
        
        空闲分区以地址递增的次序链接。分配内存时，从链首开始顺序查找，找到大小能满足要求的第一个空闲分区分配给作业。 
        
        首次适应算法最简单，通常也是最好和最快的。不过，首次适应算法会使得内存的低地址部分出现很多小的空闲分区，而每次分配查找时都要经过这些分区，因此增加了开销。
        
    2. 循环首次适应（Next Fit，NF）算法
        
        由首次适应算法演变而成。与首次适应算法不同之处是，分配内存时不是从链首而是从上次查找结束的位置开始继续查找。 
        
        循环首次适应算法试图避免低地址部分出现很多小空闲分区的问题。但它常常导致在内存空间的尾部（因为在一遍扫描中，内存前面部分使用后再释放时，不会参与分配）分裂成小碎片。通常比首次适应算法要差。 
        
    3. 最佳适应（Best Fit，BF）算法
        
        空闲分区按容量递增的次序形成空闲分区链，找到第一个能满足要求且最小的空闲分区分配给作业，避免“大材小用”。 
        
        最佳适应算法虽然称为“最佳”，但是性能通常很差，因为每次最佳的分配会留下很小的难以利用的内存块，会产生最多的外部碎片。
        
    4. 最坏适应（Worst Fit，WF）算法
        
        空闲分区以容量递减的次序链接，找到第一个能满足要求的，即最大的分区，从中分割一部分存储空间给作业。 
        
        最坏适应算法，与最佳适应算法相反，它选择最大的可用块，这看起来最不容易产生碎片，但是却把最大的连续内存划分开，会很快导致没有可用的大内存块，因此性能也非常差。
        
    5. 伙伴系统（buddy system）
        
        该算法规定，无论已分配分区或空闲分区，其大小均为 2 的 k 次幕（k 为整数，1 ≤ k ≤ m）。通常 $\small 2^m$ 是整个可分配内存的大小（也就是最大分区的大小）。假设系统的可利用空间容量为 $\small 2^m$ 个字，则系统开始运行时，整个内存区是一个大小为产的空闲分区。在系统运行过程 中，由于不断地划分，将会形成若干个不连续的空闲分区，将这些空闲分区按分区的大小进行分类。对于具有相同大小的所有空闲分区，单独设立一个空闲分区双向链表，这样， 不同大小的空闲分区形成了 k 个空闲分区链表。
        
        在伙伴系统中，其分配和回收的时间性能取决于查找空闲分区的位置和分割、合并空闲分区所花费的时间。其空间性能，由于对空闲分区进行合并，减少了小的空闲分区，提高了空闲分区的可使用率。
        
    
    ---
    

### 3.4 段式管理与页式管理：段、页、碎片等基本概念、段式管理与页式管理机制 ★★★★

非连续分配方式根据分区的大小是否固定，分为分页存储管理和分段存储管理。将分页与分段结合，便形成了段页式存储管理方式。

- **3.4.1 页式管理机制**
    
    分页的方法从形式上看，像分区相等的固定分区技术，分页管理不会产生外部碎片。但它又有本质的不同点：块的大小相对分区要小很多，而且进程也按照块进行划分，进程运行时按块申请主存可用空间并执行。这样，进程只会在为最后一个不完整的块申请一个主存块空间时，才产生主存碎片，所以尽管会产生内部碎片，但这种碎片相对于进程来说也是很小的，每个进程平均只产生半个块大小的内部碎片（也称页内碎片）。
    
    1. 页面和页面大小
        
        进程中的块称为页或页面（Page），内存中的块称为页框或页帧（Page Frame）。外存也以同样的单位进行划分，直接称为块或盘块（Block）。进程在执行时需要申请主存空间，即要为每个页面分配主存中的可用页框，这就产生了页和页框的一一对应。
        
        为方便地址转换，页面大小应是 2 的整数幕。同时页面大小应该适中，页面太小会使进程的页面数过多，这样页表就会过长，占用大量内存，而且也会增加硬件地址转换的开销，降低页面换入/换出的效率；页面过大又会使页内碎片增多，降低内存的利用率。
        
    2. 地址结构
        
        分页存储管理的逻辑地址结构如图所示。
        
        ![分页存储管理的逻辑地址结构](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2021.png)
        
        分页存储管理的逻辑地址结构
        
        地址结构包含两部分：前一部分为页号 P，后一部分为页内偏移量 W。地址长度为 32 位，其中 0 ~ 11 位为页内地址，即每页大小为 4KB（$\small 2^{12}\mathrm{B}$）；12 ~ 31位为页号，即最多允许 1M（$\small 2^{20}$） 页。 
        
        注意，地址结构决定了虚拟内存的寻址空间有多大 。在实际问题中，页号、页内偏移、逻辑地址可能是用十进制数给出的 ，若题目用二进制地址的形式给出时,要会转换。
        
    3. 页表
        
        为了便于在内存中找到进程的每个页面所对应的物理块，系统为每个进程建立一张页表，它记录页面在内存中对应的物理块号，页表一般存放在内存中。 在配置页表后，进程执行时，通过查找该表，即可找到每页在内存中的物理块号。可见，页表的作用是实现从页号到物理块号的地址映射，如图所示。
        
        ![页表的作用](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2022.png)
        
        页表的作用
        
        页表是由页表项组成的，第一部分是页号，第二部分是物理内存中的块号。
        
    4. 基本地址变换机构
        
        地址变换机构的任务是将逻辑地址转换为内存中的物理地址 。地址变换是借助于页表实现的。下图给出了分页存储管理系统中的地址变换机构。
        
        ![分页存储管理系统中的地址变换机构](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2023.png)
        
        分页存储管理系统中的地址变换机构
        
        在系统中通常设置一个页表寄存器（PTR），存放页表在内存的起始地址 F 和页表长度 M。 平时，进程未执行时，页表的始址和页表长度存放在本进程的 PCB 中，当进程被调度执行时，才将页表始址和页表长度装入页表寄存器中。设页面大小为 L，逻辑地址 A 到物理地址 E 的变换过程如下（假设逻辑地址、页号、每页的长度都是十进制数）：
        
        - ① 计算页号 P （P = A / L）和页内偏移量 W（W = A % L）。
        - ② 比较页号 P 和页表长度 M，若 P ≥ M，则产生越界中断，否则继续执行。
        - ③ 页表中页号 P 对应的页表项地址 = 页表始址 F + 页号 P × 页表项大小，取出该页表项内容 b，即为物理块号。
        - ④  计算 E = b × L + W，用得到的物理地址 E 去访问内存。
        
        ![分页存储管理系统地址变换过程](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2024.png)
        
        分页存储管理系统地址变换过程
        
        页式管理只需给出一个整数就能确定对应的物理地址，因为页面大小 L 是固定的。因此，页式管理中地址空间是一维的。
        
        页表项的作用是找到该页在内存中的位置。以 32 位逻辑地址空间、字节编址单位、一页 4KB（即页内偏移地址 W 占 12 位）为例。因为页号 P 占 20 位（32 - 12 = 20），为了保证页表项能够指向所有页面，又因为以字节作为编址单位，故页表项的大小 $\small \ge \left \lceil 20/8 \right \rceil = 3 \mathrm{B}$。当然，也可选择更大的页表项让一个页面能够正好容下整数个页表项，进而方便存储（如取成 4B，这样一页正好可以装下 1K  个页表项），或增加一些其他信息。
        
    5. 具有快表的地址变换机构
        
        由上面介绍的地址变换过程可知，若页表全部放在内存中，则存取一个数据或一条指令至少要访问两次内存：第一次是访问页表，确定所存取的数据或指令的物理地址；第二次是根据该地址存取数据或指令。显然，这种方法使计算机的处理速度降低近 1/2。
        
        为此，在地址变换机构中增设一个具有并行查找能力的高速缓冲存储器 —— 快表，又称相联存储器（TLB），用来存放当前访问的若干页表项，以加速地址变换的过程。与此对应，主存中的页表常称为慢表。具有快表的地址变换机构如图所示。
        
        ![具有快表的地址变换机构](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2025.png)
        
        具有快表的地址变换机构
        
        在具有快表的分页机制中，地址的变换过程如下： 
        
        - ① CPU给出逻辑地址后，由硬件进行地址转换，将页号送入高速缓存寄存器，并将此页号与快表中的所有页号进行比较。
        - ② 若找到匹配的页号，说明所要访问的页表项在快表中，则直接从中取出该页对应的页框号，与页内偏移量拼接形成物理地址。这样，存取数据仅一次访存便可实现。
        - ③ 若未找到匹配的页号，则需要访问主存中的页表，读出页表项后，应同时将其存入快表，以便后面可能的再次访问。若快表已满，则须按特定的算法淘汰一个旧页表项。
        
        注意：有些处理机设计为快表和慢表同时查找，若在快表中查找成功则终止慢表的查找。
        
        快表的有效性基于局部性原理，一般命中率可达 90% 以上，这样分页带来的速度损失就可降低至 10% 以下。
        
        ![引入快表前后访存时间的计算](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2026.png)
        
        引入快表前后访存时间的计算
        
    6. 两级页表 & 多级页表
        
        引入分页管理后需要考虑页表的大小。以 32 位逻辑地址空间、页面大小 4KB、页表项大小 4B 为例，若要实现进程对全部逻辑地址空间的映射，则每个进程需要 $\small 2^{20}$ 即约 100 万个页表项。也就是说，每个进程仅页表这一项就需要 4MB 主存空间，而且还要求是连续的，显然这是不切实际的。即便不考虑对全部逻辑地址空间进行映射的情况，一个逻辑地址空间稍大的进程，其页表大小也可能是过大的。以一个 40MB 的进程为例，页表项共 40KB（40MB / 4KB × 4B），若将所有页表项内容保存在内存中，则需要 10 个内存页框来保存整个页表。整个进程大小约为 1 万个页面，而实际执行时只需要几十个页面进入内存页框就可运行，但若要求 10 个页面大小的页表必须全部进入内存，则相对实际执行时的几十个进程页面的大小来说，肯定降低了内存利用率；从另一方面来说，这 10 页的页表项也并不需要同时保存在内存中，因为在大多数情况下， 映射所需要的页表项都在页表的同一个页面中。
        
        为了压缩页表，我们进一步延伸页表映射的思想，就可得到二级分页，即使用层次结构的页表：将页表的 10 页空间也进行地址映射，建立上一级页表，用于存储页表的映射关系。这里对页表的 10 个页面进行映射只需要 10 个页表项，所以上一级页表只需要 1 页就已足够。在进程执行时，只需要将这一页的上一级页表调入内存即可，进程的页表和进程本身的页面可在后面的执行中再调入内存。为查询方便，顶级页表最多只能有1个页面（一定要记住这个规定），因此顶级页表总共可以容纳4KB / 4B = 1K 个页表项，它占用的地址位数为 10 位，而之前已经计算出页内偏移地址占用了 12 位，因此一个 32 位的逻辑地址空间就剩下了 10 位，正好使得二级页表的大小在一页之内，这样就得到了逻辑地址空间的格式，如图所示。
        
        ![逻辑地址空间的格式](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2027.png)
        
        逻辑地址空间的格式
        
        若分为两级页表后，页表依然很长，则可以采用更多级页表，一般来说各级页表的大小不能超过一个页面。建立多级页表的目的在于建立索引，以便不用浪费主存空间去存储无用的页表项，也不用盲目地顺序式查找页表项。
        
        ![多级页表](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2028.png)
        
        多级页表
        
        Q：知道地址空间、页面大小、页表项大小，求解使用几级页表。
        
    
    ---
    
- **3.4.2 段式管理机制**
    
    分页管理方式是从计算机的角度考虑设计的 ，目的是提高内存的利用率，提升计算机的性能。 分页通过硬件机制实现，对用户完全透明。分段管理方式的提出则考虑了用户和程序员，以满足方便编程、信息保护和共享、动态增长及动态链接等多方面的需要。
    
    1. 分段
        
        段式管理方式按照用户进程中的自然段划分逻辑空间 。例如，用户进程由主程序段、两个子程序段、栈段和数据段组成，于是可以把这个用户进程划分为 5 段，每段从 0 开始编址，并分配一段连续的地址空间（段内要求连续，段间不要求连续，因此整个作业的地址空间是二维的）， 其逻辑地址由段号 S 与段内偏移量 W 两部分组成。在下图中，段号为 16 位，段内偏移量为 16 位，因此一个作业最多有 $\small 2^S = 2^{16} = 65536$ 段（由段号为 16 位得出），最大段长为 $\small 2^W = 2^{16} = 65536\mathrm{B} = 64\mathrm{KB}$（由偏移量为 16 位得出）。
        
        ![分段系统中的逻辑地址结构](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/%25E5%2588%2586%25E6%25AE%25B5%25E7%25B3%25BB%25E7%25BB%259F%25E4%25B8%25AD%25E7%259A%2584%25E9%2580%25BB%25E8%25BE%2591%25E5%259C%25B0%25E5%259D%2580%25E7%25BB%2593%25E6%259E%2584.png)
        
        分段系统中的逻辑地址结构
        
        在页式系统中，逻辑地址的页号和页内偏移量对用户是透明的，但在段式系统中，段号和段内偏移量必须由用户显式提供，在高级程序设计语言中，这个工作由编译程序完成。
        
    2. 段表
        
        每个进程都有一张逻辑空间与内存空间映射的段表，其中每个段表项对应进程的一段。段表项记录该段在内存中的起始地址（基址）和长度，段号是段表项在段表中的序号，是隐含给出的。配置段表后，执行中的进程可通过查找段表，找到每段所对应的内存区。可见，段表用于实现从逻辑段到物理内存区的映射，如图所示。
        
        ![利用段表实现物理内存区映射](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/%25E5%2588%25A9%25E7%2594%25A8%25E6%25AE%25B5%25E8%25A1%25A8%25E5%25AE%259E%25E7%258E%25B0%25E7%2589%25A9%25E7%2590%2586%25E5%2586%2585%25E5%25AD%2598%25E5%258C%25BA%25E6%2598%25A0%25E5%25B0%2584.png)
        
        利用段表实现物理内存区映射
        
    3. 地址变换机构
        
        分段系统的地址变换过程如图所示。为了实现进程从逻辑地址到物理地址的变换功能， 在系统中设置了段表寄存器，用于存放段表始址 F 和段表长度 M。从逻辑地址 A 到物理地址 E 之间的地址变换过程如下：
        
        ![分段系统的地址变换过程](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/%25E5%2588%2586%25E6%25AE%25B5%25E7%25B3%25BB%25E7%25BB%259F%25E7%259A%2584%25E5%259C%25B0%25E5%259D%2580%25E5%258F%2598%25E6%258D%25A2%25E8%25BF%2587%25E7%25A8%258B-1.png)
        
        分段系统的地址变换过程
        
        - ① 从逻辑地址 A 中取出前几位为段号 S，后几位为段内偏移量 W。（注意 ，在地址变换的题目中，要注意逻辑地址是用二进制数还是用十进制数给出的）
        - ② 比较段号 S 和段表长度 M，若 S ≥ M，则产生越界中断，否则继续执行。
        - ③ 段表中段号 S 对应的段表项地址 = 段表始址 F + 段号 S × 段表项长度，取出该段表项的前几位得到段长 C。
        - ④ 若段内偏移量 W ≥ C，则产生越界中断，否则继续执行。
        - ⑤ 取出段表项中该段的起始地址（基址）b，计算 E = b + W，用得到的物理地址 E 去访问内存。
        
        ![分段系统的地址变换过程](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2029.png)
        
        分段系统的地址变换过程
        
        像分页系统一样，当段表放在内存中时，每要访问一个数据，都须访问两次内存，从而成倍地降低了计算机的速率。解决的方法和分页系统类似，也增设一个联想存储器，用于保存最近常用的段表项。
        
    4. 段的共享与保护
        
        在分段系统中，段的共享是通过两个作业的段表中相应表项指向被共享的段的同一个物理副本来实现的。当一个作业正从共享段中读取数据时 ，必须防止另一个作业修改此共享段中的数据。不能修改的代码称为纯代码或可重入代码（它不属于临界资源），这样的代码和不能修改的数据可以共享，而可修改的代码和数据不能共享。
        
        与分页管理类似，分段管理的保护方法主要有两种：一种是存取控制保护，另一种是地址越界保护。地址越界保护将段表寄存器中的段表长度与逻辑地址中的段号比较，若段号大于段表长度，则产生越界中断；再将段表项中的段长和逻辑地址中的段内偏移进行比较，若段内偏移大于段长，也会产生越界中断。分页管理只需要判断页号是否越界，页内偏移是不可能越界的。
        
    5. 分页和分段的主要区别
        
        由上所述不难看出，分页和分段系统有许多相似之处。比如，两者都采用离散（非连续）分配方 式，且都是通过地址映射机构实现地址变换。但在概念上两者完全不同，主要表现在下述三个方面：
        
        - 页是信息的物理单位。采用分页存储管理方式是为实现离散分配方式，以消减内存的外零头（外部碎片），提高内存的利用率。或者说，分页仅仅只是系统管理上的需要，完全是系统的行为，对用户是不可见的。分段存储管理方式中的段则是信息的逻辑单位，它通常包含的是一组意义相对完整的信息。分段的目的主要在于能更好地满足用户的需要。
        - 页的大小固定且由系统决定。在采用分页存储管理方式的系统中，在硬件结构上， 就把用户程序的逻辑地址划分为页号和页内地址两部分，也就是说是直接由硬件实现的， 因而在每个系统中只能有一种大小的页面。而段的长度却不固定，决定于用户所编写的程序，通常由编译程序在对源程序进行编译时，根据信息的性质来划分。
        - 分页的用户程序地址空间是一维的。分页完全是系统的行为，故在分页系统中， 用户程序的地址是属于单一的线性地址空间，程序员只需利用一个记忆符即可表示一个地址。而分段是用户的行为，故在分段系统中，用户程序的地址空间是二维的，程序员在标识一个地址时，既需给出段名，又需给出段内地址。
    
    ---
    
- **3.4.3 段页式管理机制**
    
    分页存储管理能有效地提高内存利用率，而分段存储管理能反映程序的逻辑结构并有利于段的共享和保护。将这两种存储管理方法结合起来，便形成了段页式存储管理方式。 在段页式系统中，作业的地址空间首先被分成若干逻辑段，每段都有自己的段号，然后将每段分成若干大小固定的页。对内存空间的管理仍然和分页存储管理一样，将其分成若干和页面大小相同的存储块，对内存的分配以存储块为单位，如图所示。
    
    ![段页式管理方式](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2030.png)
    
    段页式管理方式
    
    在段页式系统中，作业的逻辑地址分为三部分：段号、页号和页内偏移量。
    
    ![段页式系统的逻辑地址结构](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2031.png)
    
    段页式系统的逻辑地址结构
    
    为了实现地址变换，系统为每个进程建立一张段表，每个分段有一张页表。段表表项中至少包括段号、页表长度和页表始址，页表表项中至少包括页号和块号。此外，系统中还应有一个段表寄存器，指出作业的段表始址和段表长度（段表寄存器和页表寄存器的作用都有两个，一是在段表或页表中寻址，二是判断是否越界）。
    
    注意：在一个进程中，段表只有一个，而页表可能有多个。
    
    在进行地址变换时，首先通过段表查到页表始址，然后通过页表找到页帧号，最后形成物理地址。如图所示，进行一次访问实际需要三次访问主存，这里同样可以使用快表来加快查找速度，其关键字由段号、页号组成，值是对应的页帧号和保护码。
    
    ![段页式系统的地址变换机构](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2032.png)
    
    段页式系统的地址变换机构
    
    结合上面对段式和页式管理地址空间的分析，可知段页式管理的地址空间是二维的。
    
    ![段页式系统的地址变换过程](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2033.png)
    
    段页式系统的地址变换过程
    
    ---
    

### 3.5 虚拟内存：局部性原理、虚拟内存概念、请求分段与请求分页、段页式管理、段页式地址结构与地址转换、页面置换算法（OPT、先进先出、LRU、Clock、改进型 Clock 置换）、 抖动 ★★★★★

注：为便于对比学习，段页式管理、段页式地址结构与地址转换移至 [3.4.3 段页式管理机制](https://www.notion.so/d6789a488c934a009190634bba8e1811)

- **3.5.1 虚拟存储概念**
    1. 传统存储管理方式的特征
        
        上一节讨论的各种内存管理策略（分区、段式、页式、段页式管理等）为传统存储器管理方式，它们都是为了同时将多个进程保存在内存中，以便允许进行多道程序设计。都具有以下两个共同的特征：
        
        - 一次性：作业必须一次性全部装入内存后，才能开始运行。这会导致当作业很大而不能全部被装入内存时，将使该作业无法运行；当大量作业要求运行时，由于内存不足以容纳所有作业，只能使少数作业先运行，导致多道程序度的下降。
        - 驻留性：作业被装入内存后，就一直驻留在内存中，其任何部分都不会被换出，直至作业运行结束。运行中的进程会因等待 I/O 而被阻塞，可能处于长期等待状态。
        
        由以上分析可知，许多在程序运行中不用或暂时不用的程序（数据）占据了大量的内存空间，而一些需要运行的作业又无法装入运行，显然浪费了宝贵的内存资源。
        
    2. 局部性原理
        
        要真正理解虚拟内存技术的思想，首先须了解著名的局部性原理。从广义上讲，快表、页高速缓存及虚拟内存技术都属于高速缓存技术，这个技术所依赖的原理就是局部性原理，即程序在执行时将呈现出局部性规律，即在一较短的时间内，程序的执行仅局限于某个部分，相应地，它所访问的存储空间也局限于某个区域。
        
        局部性原理表现在以下两个方面：
        
        - 时间局部性：程序中的某条指令一旦执行，不久后该指令可能再次执行；某数据被访问过，不久后该数据可能再次被访问。产生的原因是程序中存在着大量的循环操作。
        - 空间局部性：一旦程序访问了某个存储单元，在不久后，其附近的存储单元也将被访问，即程序在一段时间内所访问的地址，可能集中在一定的范围之内，因为指令通常是顺序存放、顺序执行的，数据也一般是以向量、数组、表等形式簇聚存储的。
        
        时间局部性通过将近来使用的指令和数据保存到高速缓存中，并使用高速缓存的层次结构实现。空间局部性通常使用较大的高速缓存，并将预取机制集成到高速缓存控制逻辑中实现。虚拟内存技术实际上建立了 “内存-外存”的两级存储器结构，利用局部性原理实现高速缓存。
        
    3. 虚拟存储器（内存）的定义和特征
        
        基于局部性原理，在程序装入时，仅须将程序当前要运行的少数页面或段先装入内存，其余部分暂留在外存。程序在运行时， 如果它所要访问的页（段）已调入内存，便可继续执行下去；但如果程序所要访问的页（段） 尚未调入内存（称为缺页或缺段），便发出缺页（段）中断请求，此时 OS 将利用请求调页（段） 功能将它们调入内存，以使进程能继续执行下去。如果此时内存已满，无法再装入新的页（段）， OS 还须再利用页（段）的置换功能，将内存中暂时不用的页（段）调至外存，腾出足够的内存空间后，再将要访问的页（段）调入内存，使程序继续执行下去。这样，便可使一个大的用户程序在较小的内存空间中运行，也可在内存中同时装入更多的进程，使它们并发执行。
        
        当用户看到自己的程序能在系统中正常运行时，他会认为，该系统所具有的内存容量 一定比自己的程序大，或者说，用户所感觉到的内存容量会比实际内存容量大得多。但用户所看到的大容量只是一种错觉，是虚的，故人们把这样的存储器称为虚拟存储器。
        
        综上所述，所谓虚拟存储器，是指具有请求调入功能和置换功能，能从逻辑上对内存容量加以扩充的一种存储器系统。其逻辑容量由内存容量和外存容量之和所决定，其运行速度接近于内存速度，而每位的成本却又接近于外存。可见，虚拟存储技术是一种性能非常优越的存储器管理技术。
        
        虚拟存储器有以下三个主要特征：
        
        - 多次性：是指无须在作业运行时一次性地全部装入内存，而允许被分成多次调入内存运行，即只需将当前要运行的那部分程序和数据装入内存即可开始运行。以后每当要运行到尚未调入的那部分程序时，再将它调入。多次性是虚拟存储器最重要的特征。
        - 对换性：是指无须在作业运行时一直常驻内存，在进程运行期间，允许将那些暂不使用的程序和数据或不运行的进程从内存调至外存的对换区 （换出），待以后需要时再将它们从外存调至内存 （换进）。正是由于对换性，才使得虚拟存储器得以正常运行 。
        - 虚拟性：是指从逻辑上扩充内存的容量，使用户所看到的内存容量远大于实际的内存容量。这是虚拟存储器所表现岀的最重要特征，也是实现虚拟存储器的最重要目标。
        - Q：虚拟内存空间的大小由什么因素决定？
            
            虚拟内存的容量要满足以下两个条件： 
            
            - ① 虚拟内存实际容量 < 内存容量和外存容量之和。这是硬件的硬性条件规定的，若虚存的实际容量超过了这个容量，则没有相应的空间来供虚存使用。
            - ② 虚拟内存的最大容量 < 计算机的地址位数能容纳的最大容量。假设地址是 32 位的，按字节编址，一个地址代表 1B 存储空间，则虚存的最大容量 $\small \le 4\mathrm{GB}~(2^{32}\mathrm{B})$。这是因为若虚存的最大容量超过 4GB，则 32 位的地址将无法访问全部虚存，也就是说 4GB 以后的空间被浪费了，相当于没有一样，没有任何意义。
            
            实际虚存的容量是取条件 ① 和 ② 的交集，即取最小值。
            
    4. 虚拟内存技术的实现
        
        虚拟内存技术允许将一个作业分多次调入内存，其实现建立在离散分配的内存管理方式的基础上，有以下三种方式： 
        
        - [请求分页存储管理](https://www.notion.so/d6789a488c934a009190634bba8e1811)
        - [请求分段存储管理](https://www.notion.so/d6789a488c934a009190634bba8e1811)
        - 请求段页式存储管理
        
        不管哪种方式，都需要有一定的硬件支持。一般需要的支持有以下几个方面 ：
        
        - 一定容量的内存和外存。
        - 请求分页的页表机制（或请求分段的段表机制），作为主要的数据结构。
        - 缺页中断机构，当用户程序要访问的部分尚未调入内存时，则产生中断。
        - 地址变换机构，逻辑地址到物理地址的变换。
    
    ---
    
- **3.5.2 请求分页管理**
    
    请求分页系统建立在基本分页系统基础之上，为了支持虚拟存储器功能而增加了请求调页功能和页面置换功能。请求分页是目前最常用的一种实现虚拟存储器的方法。
    
    在请求分页系统中，只要求将当前需要的一部分页面装入内存，便可以启动作业运行。在作 业执行过程中，当所要访问的页面不在内存中时，再通过调页功能将其调入，同时还可通过置换功能将暂时不用的页面换岀到外存上，以便腾出内存空间。
    
    为了实现请求分页，系统必须提供一定的硬件支持。除了需要一定容量的内存及外存的计算机系统，还需要有页表机制、缺页中断机构和地址变换机构。
    
    1. 页表机制
        
        请求分页系统的页表机制不同于基本分页系统，请求分页系统在一个作业运行之前不要求全部一次性调入内存，因此在作业的运行过程中，必然会出现要访问的页面不在内存中的情况，如何发现和处理这种情况是请求分页系统必须解决的两个基本问题。为此，在请求页表项中增加了 4个字段，如图所示。
        
        ![请求分页系统中的页表项](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2034.png)
        
        请求分页系统中的页表项
        
        增加的 4 个字段说明如下：
        
        - 状态位 P：用于指示该页是否已调入内存，供程序访问时参考。
        - 访问字段 A：用于记录本页在一段时间内被访问的次数，或记录本页最近已有多长时间未被访问，供置换算法换出页面时参考。
        - 修改位 M：标识该页在调入内存后是否被修改过，以确定页面置换时是否写回外存。
        - 外存地址：用于指出该页在外存上的地址，通常是物理块号，供调入该页时参考。
    2. 缺页中断机构
        
        在请求分页系统中，每当所要访问的页面不在内存中时，便产生一个缺页中断，请求操作系统将所缺的页调入内存。此时应将缺页的进程阻塞（调页完成唤醒），若内存中有空闲块，则分配一个块，将要调入的页装入该块，并修改页表中的相应页表项，若此时内存中没有空闲块，则要淘汰某页（若被淘汰页在内存期间被修改过，则要将其写回外存）。
        
        缺页中断作为中断，同样要经历诸如保护 CPU 环境、分析中断原因、转入缺页中断处理程 序、恢复 CPU 环境等几个步骤。但与一般的中断相比，它有以下两个明显的区别：
        
        - 在指令执行期间而非一条指令执行完后产生和处理中断信号，属于内部异常。
        - 一条指令在执行期间，可能产生多次缺页中断。
    3. 地址变换机构
        
        请求分页系统中的地址变换机构，是在分页系统地址变换机构的基础上，为实现虚拟内存， 又增加了某些功能而形成的，如产生和处理缺页中断，及从内存中换出一页的功能等等。
        
        如图所示，在进行地址变换时，先检索快表：
        
        ![请求分页中的地址变换过程](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2035.png)
        
        请求分页中的地址变换过程
        
        - 若找到要访问的页，则修改页表项中的访问位（写指令还需要重置修改位），然后利用页表项中给出的物理块号和页内地址形成物理地址。
        - 若未找到该页的页表项，则应到内存中去查找页表，再对比页表项中的状态位 P，看该页是否已调入内存，若页面已调入，则将该页的页表写入快表，若快表已满，则需采用某种算法替换。若页面未调入，则产生缺页中断，请求从外存把该页调入内存。
    4. 页框分配
        1. 驻留集大小
            
            对于分页式的虚拟内存，在进程准备执行时，不需要也不可能把一个进程的所有页都读入主存。因此，操作系统必须决定读取多少页，即决定给特定的进程分配几个页框。给一个进程分配的物理页框的集合就是这个进程的驻留集。需要考虑以下几点： 
            
            - ① 分配给一个进程的页框越少，驻留在主存中的进程就越多，从而可提高 CPU 的利用率。
            - ② 若一个进程在主存中的页面过少 ，则尽管有局部性原理，缺页率仍相对较高。
            - ③ 若分配的页框过多，则由于局部性原理，对该进程的缺页率没有太明显的影响。
        2. 内存分配策略
            
            在请求分页系统中，可采取固定和可变分配策略。在进行置换时，可采取全局置换和局部置换。于是可组合出下面三种适用的策略。
            
            - 固定分配局部置换
                
                为每个进程分配一定数目的物理块，在进程运行期间都不改变。所谓局部置换，是指如果进程在运行中发生缺页，则只能从分配给该进程在内存的页面中选出一页换出，然后再调入一页， 以保证分配给该进程的内存空间不变。实现这种策略时，难以确定应为每个进程分配的物理块数目：太少会频繁出现缺页中断，太多又会降低CPU和其他资源的利用率。
                
            - 可变分配全局置换
                
                先为每个进程分配一定数目的物理块，在进程运行期间可根据情况适当地增加或减少 。所谓全局置换，是指如果进程在运行中发生缺页，系统从空闲物理块队列中取出一块分配给该进程， 并将所缺页调入。这种方法比固定分配局部置换更加灵活，可以动态增加进程的物理块，但也存在弊端，如它会盲目地给进程增加物理块，从而导致系统多道程序的并发能力下降。
                
            - 可变分配局部置换
                
                为每个进程分配一定数目的物理块，当某进程发生缺页时，只允许从该进程在内存的页面中 选出一页换出，因此不会影响其他进程的运行。若进程在运行中频繁地发生缺页中断，则系统再为该进程分配若干物理块，直至该进程的缺页率趋于适当程度；反之，若进程在运行中的缺页率特别低，则可适当减少分配给该进程的物理块，但不能引起其缺页率的明显增加。这种方法在保证进程不会过多地调页的同时，也保持了系统的多道程序并发能力 。当然它需要更复杂的实现， 也需要更大的开销，但对比频繁地换入/换出所浪费的计算机资源，这种牺牲是值得的。
                
        3. 物理块调入算法
            
            采用固定分配策略时，将系统中的空闲物理块分配给各个进程，可采用下述几种算法。 
            
            - 平均分配算法：将系统中所有可供分配的物理块平均分配给各个进程。
            - 按比例分配算法：根据进程的大小按比例分配物理块。
            - 优先权分配算法：为重要和紧迫的进程分配较多的物理块。通常采取的方法是把所有可分配的物理块分成两部分，一部分按比例分配给各个进程；一部分则根据优先权分配。
        4. 调入页面的时机
            
            为确定系统将进程运行时所缺的页面调入内存的时机，可采取以下两种调页策略： 
            
            - ① 预调页策略。根据局部性原理，一次调入若干相邻的页会比一次调入一页更高效。但若调入的一批页面中的大多数都未被访问，则又是低效的。因此，需要采用以预测为基础的预调页策略，将那些预计在不久之后便会被访问的页面预先调入内存。但目前预调页的成功率仅约 50%。因此这种策略主要用于进程的首次调入，由程序员指出应先调入哪些页。
            - ② 请求调页策略。进程在运行中需要访问的页面不在内存，便提出请求，由系统将其所需页面调入内存。由这种策略调入的页一定会被访问，且这种策略比较易于实现，因此目前的虚拟存储器大多采用此策略。其缺点是每次仅调入一页，增加了磁盘 I/O 开销。
            
            预调入实际上就是运行前的调入，请求调页实际上就是运行期间调入。
            
        5. 从何处调入页面
            
            请求分页系统中的外存分为两部分：用于存放文件的文件区和用于存放对换页面的对换区。 对换区采用连续分配方式，而文件区采用离散分配方式，因此对换区的磁盘 I/O 速度比文件区的更快。这样，当发生缺页请求时，系统从何处将缺页调入内存就分为三种情况：
            
            - ① 系统拥有足够的对换区空间。可以全部从对换区调入所需页面，以提高调页速度。为此， 在进程运行前，需将与该进程有关的文件从文件区复制到对换区。
            - ② 系统缺少足够的对换区空间。凡是不会被修改的文件都直接从文件区调入；而当换出这些页面时，由于它们未被修改而不必再将它们换出 。但对于那些可能被修改的部分，在将它们换出时须调到对换区，以后需要时再从对换区调入（因为读比写的速度快）。
            - ③ UNIX方式。与进程有关的文件都放在文件区，因此未运行过的页面都应从文件区调入。 曾经运行过但又被换出的页面，由于是放在对换区，因此在下次调入时应从对换区调入。 进程请求的共享页面若被其他进程调入内存，则无须再从对换区调入。
        6. 页面调入过程
            
            当进程所访问的页面不在内存中时（存在位为 0），便向 CPU 发出缺页中断，中断响应后便转入缺页中断处理程序。该程序通过查找页表得到该页的物理块，此时如果内存未满，则启动磁盘 I/O，将所缺页调入内存，并修改页表。如果内存已满，则先按某种置换算法从内存中选出一 页准备换出；如果该页未被修改过（修改位为 0），则无须将该页写回磁盘；但是，如果该页已被修改（修改位为 1），则必须将该页写回磁盘，然后将所缺页调入内存，并修改页表中的相应表项， 置其存在位为 1。调入完成后，进程就可利用修改后的页表形成所要访问数据的内存地址。
            
    - Q：什么是虚拟存储器？如何实现页式虚拟存储器？
        
        虚拟存储器，是指具有请求调入功能和置换功能，能从逻辑上对内存容量加以扩充的一种存储器系统。其逻辑容量由内存容量和外存容量之和所决定，其运行速度接近于内存速度，而每位的成本却又接近于外存。
        
        页式虚拟存储器在基本分页系统基础之上，增加请求调页功能和页面置换功能。为了实现请求分页，系统必须提供一定的硬件支持。除了需要一定容量的内存及外存，还需要有页表机制、缺页中断机构和地址变换机构。进行页面置换时，还需选取适合的页面置换算法，置换算法的好坏将直接影响到系统的性能。
        
    
    ---
    
- **3.5.3 页面置换算法（OPT、先进先出、LRU、Clock、改进型 Clock 置换）**
    
    进程运行时，若其访问的页面不在内存中而需将其调入，但内存已无空闲空间时，就需要从内存中调出一页程序或数据，送入磁盘的对换区。选择调出页面的算法就称为页面置换算法。好的页面置换算法应有较低的页面更换频率，也就是说，应将以后不会再访问或以后较长时间内不会再访问的页面先调出 。 常见的置换算法有以下 4 种。
    
    1. 最佳（OPT）置换算法
        
        最佳置换算法选择的被淘汰页面是以后永不使用的页面，或是在最长时间内不再被访问的页面，以便保证获得最低的缺页率。然而，由于人们目前无法预知进程在内存下的若干页面中哪个是未来最长时间内不再被访问的，因而该算法无法实现。但可利用该算法去评价其他算法。
        
        假定系统为某进程分配了三个物理块，并考虑有页面号引用串：$\small 7, 0, 1, 2, 0, 3, 0, 4, 2, 3, 0, 3, 2,1, 2, 0,1, 7, 0,1$。
        
        进程运行时，先将 7, 0, 1 三个页面依次装入内存。当进程要访问页面 2 时，产生缺页中断， 根据最佳置换算法，选择将第 18 次访问才需调入的页面7淘汰。然后，访问页面 0 时，因为它已在内存中，所以不必产生缺页中断。访问页面 3 时，又会根据最佳置换算法将页面 1 淘汰…… 以此类推，如图所示，从图中可以看出采用最佳置换算法时的情况。
        
        ![利用最佳置换算法时的置换图](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2036.png)
        
        利用最佳置换算法时的置换图
        
        可以看到，发生缺页中断的次数为 9，页面置换的次数为6。
        
    2. 先进先出（FIFO）页面置换算法
        
        优先淘汰最早进入内存的页面 ，即淘汰在内存中驻留时间最久的页面。该算法实现简单，只需把已调入内存的页面根据先后次序链接成队列，设置一个指针总是指向最老的页面 。但该算法与进程实际运行时的规律不适应，因为在进程中，有的页面经常被访问。 
        
        这里仍用上面的例子采用 FIFO 算法进行页面置换。当进程访问页面 2 时，把最早进入内存的页面 7 换出。然后访问页面 3 时，把 2, 0,1 中最先进入内存的页面 0 换出。由下图可以看出，利用 FIFO 算法时进行了 12 次页面置换，比最佳置换算法正好多一倍。
        
        ![FIFO 置换算法时的置换图](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2037.png)
        
        FIFO 置换算法时的置换图
        
        FIFO 算法还会产生所分配的物理块数增大而缺页率不减反增的异常现象，称为 Belady 异常。只有 FIFO 算法可能出现Belady异常，LRU 和 OPT 算法永远不会出现 Belady 异常。
        
        如下图所示,页面访问顺序为 $\small 3, 2, 1, 0, 3, 2, 4, 3, 2, 1, 0, 4$。若采用 FIFO 置换算法，当分配的物理块为 3 个时，缺页次数为 9 次；当分配的物理块为 4 个时，缺页次数为 10 次。分配给进程的物理块增多，但缺页次数不减反增。
        
        ![Belady 异常](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2038.png)
        
        Belady 异常
        
    3. 最近最久未使用（LRU）置换算法
        
        选择最近最长时间未访问过的页面予以淘汰，它认为过去一段时间内未访问过的页面，在最近的将来可能也不会被访问。该算法为每个页面设置一个访问字段，用来记录页面自上次被访问以来所经历的时间，淘汰页面时选择现有页面中值最大的予以淘汰。
        
        再对上面的例子采用 LRU 算法进行页面置换，如图所示。进程第一次对页面 2 访问时，将最近最久未被访问的页面 7 置换岀去。然后在访问页面 3 时，将最近最久未使用的页面 1 换出。
        
        ![LRU 页面置换算法时的置换图](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2039.png)
        
        LRU 页面置换算法时的置换图
        
        在上图中，前 5 次置换的情况与最佳置换算法相同，但两种算法并无必然联系。实际上， LRU 算法根据各页以前的使用情况来判断，是“向前看”的（根据过去预测未来），而最佳置换算法则根据各页以后的使用情况来判断，是“向后看”的（根据未来选择最佳）。而页面过去和未来的走向之间并无必然联系。
        
        LRU 算法的性能较好，但需要寄存器和栈的硬件支持。LRU 是堆栈类的算法。理论上可以证明，堆栈类算法不可能出现 Belady 异常。而 FIFO 算法基于队列实现，不是堆栈类算法。
        
    4. 时钟（CLOCK）置换算法
        
        LRU 算法的性能接近 OPT 算法，但实现起来的开销大。因此，操作系统的设计者尝试了很多算法，试图用比较小的开销接近 LRU 算法的性能，这类算法都是 CLOCK 算法的变体。
        
        - 简单的 CLOCK 置换算法
            
            当使用简单的 CLOCK 置换算法时，为每页设置一位访问位。当某页被装入或被访问时，其访问位被置为 1。算法将内存中的所有页面视为一个循环队列，并有一个替换指针与之相关联，当某一页被替换后，该指针被设置指向被替换页面的下一页。在选择一页淘汰时，只需检查页的访问位。若为 0，就选择该页换出；若为 1，则将它置为 0，暂不换出，给予该页第二次驻留内存的机会，再依次顺序检查下一个页面。当检查到队列中的最后一个页面时，若其访问位仍为 1，则返回到队首去循环检查。由于该算法是循环地检查各个页面的使用情况，故称 CLOCK 算法。但是，因为该算法只有一位访问位，而置换时将未使用过的页面换出，故又称最近未用（NRU）算法。
            
            假设页面访问顺序为 $\small 7, 0, 1, 2, 0, 3, 0, 4, 2, 3, 0, 3,  2, 1, 3, 2$，采用简单CLOCK置换算法，分配 4 个页帧，每个页对应的结构为（页面号，访问位），置换过程如图所示。
            
            ![CLOCK 算法时的置换图](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2040.png)
            
            CLOCK 算法时的置换图
            
            - 首次访问 7, 0, 1, 2 时，产生缺页中断，依次调入主存，访问位都置为 1。
            - 访问 0 时，已存在，访问位置为 1。
            - 访问 3 时，产生第 5 次缺页中断，替换指针初始指向帧 1，此时所有帧的访问位均为 1，则替换指针完整地扫描一周，把所有帧的访问位都置为 0，然后回到最初的位置（帧1），换出帧 1 中的页 7，换入页 3 并将其访问位置 1，同时替换指针指向下一个页帧（帧 2），如下图 (a) 所示。
            - 访问 0 时，已存在，访问位置为 1。
            - 访问 4 时，产生第 6 次缺页中断，替换指针指向帧 2 （上次替换位置的下一帧），帧 2 的访问位为 1，将其修改为 0，继续扫描，帧 3 的访问位为 0，替换帧 3 中的页，替换指针指向下一个页帧（帧 4）如下图 (b) 所示。
            - 然后依次访问 2, 3, 0, 3, 2 均已存在，每次访问都将其访问位置为 1。
            - 访问 1 时，产生缺页中断，替换指针指向帧 4，此时所有帧的访问位均为 1，又完整扫描一周并置访问位为 0，回到帧 4 将其替换，然后替换指针指向下一个页帧（帧 1）。
            - 访问 3 时，已存在，访问位置 1。
            - 访问 2 时，产生缺页中断，替换指针指向帧 1，帧 1 的访问位为 1，将其修改为 0，继续扫描，帧 2 的访问位为 0，替换帧 2 中的页。
            
            ![缺页中断时替换指针扫描示意图](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2041.png)
            
            缺页中断时替换指针扫描示意图
            
    5. 改进型 CLOCK 置换算法
        
        将一个页面换出时，若该页已被修改过，则要将该页写回磁盘，若该页未被修改过，则不必将它写回磁盘。可见，对于修改过的页面，替换代价更大。在改进型 CLOCK 算法中，除考虑页面使用情况外，还增加了置换代价——修改位。在选择页面换出时，优先考虑既未使用过又未修改过的页面。由访问位 A 和修改位 M 可以组合成下面四种类型的页面：
        
        - 1 类 A = 0，M = 0：最近未被访问且未被修改，是最佳淘汰页。
        - 2 类 A = 0，M = 1：最近未被访问，但已被修改，不是很好的淘汰页。
        - 3 类 A = 1，M = 0：最近已被访问，但未被修改，可能再被访问。
        - 4 类 A = 1，M = 1：最近已被访问且已被修改，可能再被访问。
        
        内存中的每页必定都是这四类页面之一。在进行页面置换时，可采用与简单 CLOCK 算法类似的算法，差别在于该算法要同时检查访问位和修改位。算法执行过程如下：
        
        - ① 从指针的当前位置开始，扫描循环队列，寻找 A = 0 且 M = 0 的 1 类页面，将遇到的第一个 1 类页面作为选中的淘汰页。在第一次扫描期间不改变访问位 A。
        - ② 若第 1 步失败，则进行第二轮扫描，寻找 A = 0 且 M = 1 的 2 类页面。将遇到的第一个 2 类页面作为淘汰页。在第二轮扫描期间，将所有扫描过的页面的访问位都置为 0。
        - ③ 若第 2 步也失败，此时指针将返回到开始的位置，并且所有帧的访问位都为 0。重复第 1 步，并且若有必要，重复第 2 步，此时一定能找到被淘汰的页。
        
        改进型 CLOCK 算法优于简单 CLOCK 算法的地方在于，可减少磁盘的 I/O 操作次数。但为了找到一个可置换的页，可能要经过几轮扫描，即实现算法本身的开销将有所增加。
        
        操作系统中的页面置换算法都有一个原则，即尽可能保留访问过的页面，而淘汰未访问的页面。简单的 CLOCK 算法只考虑页面是否被访问过；改进型 CLOCK 算法对这两类页面做了细分，分为修改过的页面和未修改的页面。因此，若有未使用过的页面，则当然优先将其中未修改过的页面换出。若全部页面都用过，还是优先将其中未修改过的页面换出。
        
    
    ---
    
- **3.5.4 抖动和工作集**
    1. 抖动
        
        在页面置换过程中，一种最糟糕的情形是，刚刚换出的页面马上又要换入主存，刚刚换入的页面马上又要换出主存，这种频繁的页面调度行为称为抖动或颠簸。
        
        系统发生抖动的根本原因是，系统中同时运行的进程太多，由此分配给每个进程的物理块太少，不能满足进程正常运行的基本要求，致使每个进程在运行时频繁地出现缺页，必须请求系统将所缺页面调入内存。这会使得在系统中排队等待页面调入 /调出的进程数目增加。显然，对磁盘的有效访问时间也随之急剧增加，造成每个进程的大部分时间都用于页面的换入/换出，而几乎不能再去做任何有效的工作，进而导致发生处理机的利用率急剧下降并趋于零的情况。 
        
        抖动是进程运行时出现的严重问题，必须采取相应的措施解决它。由于抖动的发生与系统为进程分配物理块的多少有关，于是又提出了关于进程工作集的概念。
        
        减少抖动的方法：撤销部分进程，为某些进程添加适当的页帧，选择更适合的页面置换算法。
        
    2. 工作集
        
        工作集是指在某段时间间隔内，进程要访问的页面集合。基于局部性原理，可以用最近访问过的页面来确定工作集。一般来说，工作集 W 可由时间 t 和工作集窗口大小 Δ 来确定。例如，某进程对页面的访问次序如下：
        
        ![Untitled](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2042.png)
        
        假设系统为该进程设定的工作集窗口大小 Δ 为5，则在 $\small t_1$ 时刻，进程的工作集为｛2, 3, 5｝，在 $\small t_2$ 时刻，进程的工作集为｛1, 2, 3, 4｝。
        
        实际应用中，工作集窗口会设置得很大，即对于局部性好的程序，工作集大小一般会比工作 集窗口 Δ 小很多。工作集反映了进程在接下来的一段时间内很有可能会频繁访问的页面集合 ，因此，若分配给进程的物理块小于工作集大小，则该进程就很有可能频繁缺页，所以为了防止这种抖动现象，一般来说分配给进程的物理块数（即驻留集大小）要大于工作集大小。
        
        工作集模型的原理是，让操作系统跟踪每个进程的工作集，并为进程分配大于其工作集的物理块。落在工作集内的页面需要调入驻留集中，而落在工作集外的页面可从驻留集中换出。若还有空闲物理块，则可再调一个进程到内存。若所有进程的工作集之和超过了可用物理块总数，则操作系统会暂停一个进程，将其页面调出并将物理块分配给其他进程，防止出现抖动现象。
        
    
    ---
    
- **3.5.5 请求分段管理**
    
    在分页基础上建立的请求分页式虚拟存储器系统，是以页面为单位进行换入、换出的。而在分段基础上所建立的请求分段式虚拟存储器系统，则是以分段为单位进行换入、换出的。它们在实现原理以及所需要的硬件支持上都是十分相似的。在请求分段系统中，程序运行之前，只需先调入少数几个分段（不必调入所有的分段）便可启动运行。当所访问的段不在内存中时，可请求 OS 将所缺的段调入内存。像请求分页系统一样，为实现请求分段存储管理方式，同样需要一定的硬件支持和相应的软件。
    
    为了实现请求分段式存储管理，应在系统中配置多种硬件机构，以支持快速地完成请求分段功能。与请求分页系统相似，在请求分段系统中所需的硬件支待有段表机制、缺段中断机构，以及地址变换机构。
    
    1. 请求段表机制
        
        在请求分段式管理中所需的主要数据结构是请求段表。在该表中除了具有请求分页机制中有的访问字段 A、修改位 M、存在位 P 和外存始址四个字段外，还增加了存取方式字段和增补位。这些字段供程序在调进、调出时参考。下面给出请求分段的段表项。
        
        ![请求分段系统中的段表项](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2043.png)
        
        请求分段系统中的段表项
        
        在段表项中，除了段名（号）、段长、段在内存中的起始地址（段基址）外，还增加了以下字段：
        
        - 存取方式。由于应用程序中的段是信息的逻辑单位，可根据该信息的属性对它实施保护，故在段表中增加存取方式字段，如果该字段为两位，则存取属性是只执行、只读和允许读／写。
        - 访问字段 A 。其含义与请求分页的相应字段相同，用于记录该段被访问的频繁程度。提供给置换算法选择换出页面时参考。
        - 修改位 M 。该字段用于表示该页在进入内存后是否已被修改过，供置换页面时 参考。
        - 存在位 P 。该字段用于指示本段是否已调入内存，供程序访问时参考。
        - 增补位。这是请求分段式管理中所特有的字段，用于表示本段在运行过程中是否做过动态增长。
        - 外存始址。指示本段在外存中的起始地址，即起始盘块号。
    2. 缺段中断机构
        
        在请求分段系统中采用的是请求调段策略。每当发现运行进程所要访问的段尚未调入内存时，便由缺段中断机构产生一缺段中断信号，进入 OS 后，由缺段中断处理程序将所需的段调入内存。与缺页中断机构类似，缺段中断机构同样需要在一条指令的执行期间产生和处理中断，以及在一条指令执行期间，可能产生多次缺段中断。但由于分段是信息的逻辑单位，因而不可能出现一条指令被分割在两个分段中，和一组信息被分割在两个分段中的情况。缺段中断的处理过程如图所示。由于段不是定长的，这使对缺段中断的处理要比对缺页中断的处理复杂。
        
        ![请求分段系统中的中断处理过程](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2044.png)
        
        请求分段系统中的中断处理过程
        
    3. 地址变换机构
        
        请求分段系统中的地址变换机构是在分段系统地址变换机构的基础上形成的。因为被访问的段并非全在内存，所以在地址变换时，若发现所要访问的段不在内存，必须先将所缺的段调入内存，并修改段表，然后才能再利用段表进行地址变换。为此，在地址变换机构中又增加了某些功能，如缺段中断的请求及处理等。下图为请求分段系统的地址变换过程。
        
        ![请求分段系统的地址变换过程](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2045.png)
        
        请求分段系统的地址变换过程
        
    
    ---
    

## 4. 设备管理

### 4.1 I/O 系统的：基本概念、I/O 控制方式（程序 I/0、中断、DMA、通道）、相关数据结构、 缓冲管理（单缓冲、双缓冲、循环缓冲、缓冲池）★★

注：相关数据结构参考 [4.3.1 设备分配](https://www.notion.so/d6789a488c934a009190634bba8e1811)

- **4.1.1 基本概念**
    1. 设备的分类
        
        按信息交换的单位分类，I/O 设备可分为： 
        
        - ① 块设备。信息交换以数据块为单位。它属于有结构设备，如磁盘等。磁盘设备的基本特征是传输速率较高、可寻址，即对它可随机地读/写任一块。
        - ②  字符设备。信息交换以字符为单位。它属于无结构类型，如它们的基本特征是传输速率低、不可寻址，并且时常采用中断 I/O 方式。
        
        按传输速率分类，I/O设备可分为： 
        
        - ① 低速设备。传输速率仅为每秒几字节到数百字节的一类设备，如键盘、鼠标等。
        - ② 中速设备。传输速率为每秒数千字节至数万字节的一类设备，如激光打印机等。
        - ③ 高速设备。传输速率在数百千字节至千兆字节的一类设备，如磁盘机、光盘机等。
    2. I/O 接口
        
        I/O 接口（设备控制器）位于 CPU 与设备之间，它既要与 CPU 通信，又要与设备通信，还要具有按 CPU 发来的命令去控制设备工作的功能，主要由三部分组成，如图所示。
        
        ![设备控制器的组成](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2046.png)
        
        设备控制器的组成
        
        - ① 设备控制器与 CPU 的接口。该接口有三类信号线：数据线、地址线和控制线。数据线通常与两类寄存器相连：数据寄存器（存放从设备送来的输入数据或从 CPU 送来的输出数据）和控制/状态寄存器（存放从 CPU 送来的控制信息或设备的状态信息）。
        - ② 设备控制器与设备的接口。一个设备控制器可以连接一个或多个设备，因此控制器中有一个或多个设备接口。每个接口中都存在数据、控制和状态三种类型的信号。
        - ③ I/O 逻辑。用于实现对设备的控制。它通过一组控制线与 CPU 交互，对从 CPU 收到的 I/O 命令进行译码。CPU 启动设备时，将启动命令发送给控制器，同时通过地址线把地址发送给控制器，由控制器的 I/O 逻辑对地址进行译码，并相应地对所选设备进行控制。
        
        设备控制器的主要功能有：
        
        - ① 接收和识别 CPU 发来的命令，如磁盘控制器能接收读、写、 查找等命令；
        - ② 数据交换，包括设备和控制器之间的数据传输，以及控制器和主存之间的数据传 输；
        - ③ 标识和报告设备的状态，以供 CPU 处理；
        - ④ 地址识别；
        - ⑤ 数据缓冲；
        - ⑥ 差错控制。
    3. I/O 端口
        
        I/O 端口是指设备控制器中可被 CPU 直接访问的寄存器，主要有以下三类寄存器。
        
        - 数据寄存器：实现 CPU 和外设之间的数据缓冲。
        - 状态寄存器：获取执行结果和设备的状态信息，以让 CPU 知道是否准备好。
        - 控制寄存器：由 CPU 写入，以便启动命令或更改设备模式。
        
        为了实现 CPU 与 I/O 端口进行通信，有两种方法，如图所示。
        
        - ① 独立编址。为每个端口分配一个 I/O 端口号，所有 I/O 端口形成 I/O 端口空间，普通用户程序不能对其进行访问，只有操作系统使用特殊的 I/O 指令才能访问端口。
        - ② 统一编址。又称内存映射 I/O，每个端口被分配唯一的内存地址，且不会有内存被分配这 一地址，通常分配给端口的地址靠近地址空间的顶端。
        
        ![独立编址 I/O 和内存映射 I/O](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2047.png)
        
        独立编址 I/O 和内存映射 I/O
        
    
    ---
    
- **4.1.2 I/O 控制方式**
    1. 采用轮询的可编程 I/O 方式（程序 I/0，程序直接控制方式）
        
        计算机从外部设备读取每个字时，CPU 需要对外设状态进行循环检查（轮询），直到确定该字已经在 I/O 控制器的数据寄存器中。在程序直接控制方式中，由于 CPU 的高速性和 I/O 设备的低速性，致使 CPU 的绝大部分时间都处于等待 I/O 设备完成数据 I/O 的循环测试中，造成 CPU 资源的极大浪费。
        
        在该方式中，CPU 之所以要不断地测试 I/O 设备的状态，就是因为在 CPU 中未采用中断机构，使 I/O 设备无法向 CPU 报告它已完成了一个字符的输入操作。
        
        程序直接控制方式虽然简单且易于实现，但其缺点也显而易见，由于 CPU 和 I/O 设备只能串行工作，导致 CPU 的利用率相当低。
        
    2. 采用中断的可编程 I/O 方式（中断驱动方式）
        
        中断驱动方式的思想是，允许 I/O 设备主动打断 CPU 的运行并请求服务，从而“解放” CPU， 使得其向 I/O 控制器发送 I/O 命令后可以继续做其他有用的工作。
        
        从 I/O 控制器的角度来看，I/O 控制器从 CPU 接收一个读命令，然后从外部设备读数据。一旦数据读入 I/O 控制器的数据寄存器，便通过控制线给 CPU 发出中断信号，表示数据已准备好，然后等待 CPU 请求该数据。I/O 控制器收到 CPU 发出的取数据请求后，将数据放到数据总线上，传到 CPU 的寄存器中。至此，本次 I/O 操作完成，I/O 控制器又可开始下一次 I/O 操作。
        
        从 CPU 的角度来看，CPU 发出读命令，然后保存当前运行程序的上下文（现场，包括程序计数器及处理机寄存器），转去执行其他程序。在每个指令周期的末尾，CPU 检查中断。当有来自 I/O 控制器的中断时，CPU 保存当前正在运行程序的上下文，转去执行中断处理程序以处理该中断。这时，CPU 从 I/O 控制器读一个字的数据传送到寄存器，并存入主存。接着，CPU 恢复发岀 I/O 命令的程序（或其他程序）的上下文，然后继续运行。
        
        中断驱动方式比程序直接控制方式有效，但由于数据中的每个字在存储器与 I/O 控制器之间的传输都必须经过 CPU，这就导致了中断驱动方式仍然会消耗较多的 CPU 时间。
        
    3. 直接存储器访问方式（DMA 方式）
        
        在中断驱动方式中，I/O 设备与内存之间的数据交换必须要经过 CPU 中的寄存器，所以速度还是受限，而 DMA 方式的基本思想是在 I/O 设备和内存之间开辟直接的数据交换通路，彻底“解放”  CPU。DMA 方式的特点如下：
        
        - 基本单位是数据块。
        - 所传送的数据，是从设备直接送入内存的，或者相反。
        - 仅在传送一个或多个数据块的开始和结束时，才需 CPU 干预，整块数据的传送是在 DMA 控制器的控制下完成的。
        
        要在主机与控制器之间实现成块数据的直接交换 ，须在 DMA 控制器中设置如下 4 类寄存器: 
        
        - 命令/状态寄存器（CR）：接收从 CPU 发来的 I/O 命令、有关控制信息，或设备的状态。
        - 内存地址寄存器（MAR）：在输入时，它存放把数据从设备传送到内存的起始目标地址；在输出时，它存放由内存到设备的内存源地址。
        - 数据寄存器（DR）：暂存从设备到内存或从内存到设备的数据。
        - 数据计数器（DC）：存放本次要传送的字（节）数。
        
        ![DMA 控制器的组成](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/DMA%25E6%258E%25A7%25E5%2588%25B6%25E5%2599%25A8%25E7%259A%2584%25E7%25BB%2584%25E6%2588%2590.png)
        
        DMA 控制器的组成
        
        DMA方式的工作过程：CPU 接收到 I/O 设备的 DMA 请求时，它给 DMA 控制器发出一条命令，同时设置 MAR 和 DC 初值，启动 DMA 控制器进行数据传送，然后继续其他工作。整个数据传送过程由 DMA 控制器直接与存储器交互，每次传送一个字，DC 内容减 1，当 DC 为 0 时传送完成，DMA 控制器发送一个中断信号给 CPU。因此只有在传送开始和结束时才需要 CPU 的参与。
        
        DMA 方式与中断方式的主要区别是，中断方式在每个数据需要传输时中断 CPU，而 DMA 方式则是在所要求传送的一批数据全部传送结束时才中断 CPU；此外，中断方式的数据传送是在中断处理时由 CPU 控制完成的，而 DMA 方式则是在 DMA 控制器的控制下完成的。
        
    4. I/O 通道方式（通道控制方式）
        
        I/O 通道是指专门负责输入/输出的处理机。I/O 通道方式是 DMA 方式的发展，它可以进一步减少 CPU 的干预，即把对一个数据块的读（或写）为单位的干预，减少为对一组数据块的读（或写）及有关控制和管理为单位的干预。同时，又可以实现 CPU、通道和 I/O 设备三者的并行操作， 从而更有效地提高整个系统的资源利用率。
        
        例如，当 CPU 要完成一组相关的读（或写）操作及有关控制时，只需向 I/O 通道发送一条 I/O 指令，以给岀其所要执行的通道程序的首地址和要访问的 I/O 设备，通道接到该指令后，执行通道程序便可完成 CPU 指定的 I/O 任务，数据传送结束时向 CPU 发中断请求。
        
        I/O 通道与一般处理机的区别：通道指令的类型单一，主要局限于与 I/O 操作有关的指令；没有自己的内存，通道所执行的通道程序是放在主机的内存中的，也就是说通道与 CPU 共享内存。
        
        I/O 通道与 DMA 方式的区别：DMA 方式需要 CPU 来控制传输的数据块大小、传输的内存位置，而通道方式中这些信息是由通道控制的。另外，每个 DMA 控制器对应一台设备与内存传递数据，而一个通道可以控制多台设备与内存的数据交换。
        
    
    ---
    
- **4.1.3 I/O 软件层次结构**
    
    整个 I/O 软件可以视为具有 4 个层次的系统结构，各层次及其功能如下：
    
    1. 用户层 I/O 软件
        
        实现与用户交互的接口，用户可直接调用在用户层提供的、与 I/O 操作有关的库函数，对设备进行操作。一般而言，大部分的 I/O 软件都在操作系统内部，但仍有一小部分在用户层，包括与用户程序链接在一起的库函数。用户层软件必须通过一组系统调用来获取操作系统服务。 
        
    2. 设备独立性软件
        
        用于实现用户程序与设备驱动器的统一接口 、设备命令、设备的保护及设备的分配与释放等，同时为设备管理和数据传送提供必要的存储空间。
        
        设备独立性也称设备无关性，使得应用程序独立于具体使用的物理设备。为实现设备独立性而引入了逻辑设备和物理设备这两个概念。在应用程序中，使用逻辑设备名来请求使用某类设备；而在系统实际执行时，必须将逻辑设备名映射成物理设备名使用。
        
        使用逻辑设备名的好处是：① 增加设备分配的灵活性；② 易于实现I/O重定向，所谓I/O重定向，是指用于I/O操作的设备可以更换(即重定向)，而不必改变应用程序。
        
        为了实现设备独立性，必须再在驱动程序之上设置一层设备独立性软件 。总体而言，设备独立性软件的主要功能可分为以下两个方面：
        
        - ① 执行所有设备的公有操作，包括：对设备的分配与回收；将逻辑设备名映射为物理设备名；对设备进行保护，禁止用户直接访问设备；缓冲管理；差错控制；提供独立于设备的大小统一的逻辑块，屏蔽设备之间信息交换单位大小和传输速率的差异。
        - ② 向用户层(或文件层)提供统一接口 。无论何种设备，它们向用户所提供的接口 应是相同的。例如，对各种设备的读/写操作，在应用程序中都统一使用 read/write 命令等。
        - Q：什么是设备的独立性，应如何实现？
            
            设备独立性也称设备无关性，即应用程序独立于具体使用的物理设备。对用户而言，是指在编写程序时使用的设备与实际设备无关，在程序中只指明 I/O 使用的设备类型即可。
            
            为了实现设备的独立性，应引入逻辑设备和物理设备两个概念。在应用程序中，使用逻辑设备名称来请求使用某类设备；而系统执行时，是使用物理设备名称。鉴于驱动程序是一个与硬件（或设备）紧密相关的软件，必须在驱动程序之上设置一层软件，称为设备独立性软件，以执行所有设备的公有操作、完成逻辑设备名到物理设备名的转换（为此应设置一张逻辑设备表 Logical Unit Table, LUT）并向用户层（或文件层）软件提供统一接口，从而实现设备的独立性。
            
    3. 设备驱动程序
        
        与硬件直接相关，负责具体实现系统对设备发出的操作指令，驱动 I/O 设备工作的驱动程序。 通常，每类设备配置一个设备驱动程序，它是 I/O 进程与设备控制器之间的通信程序，常以进程形式存在。设备驱动程序向上层用户程序提供一组标准接口 ，设备具体的差别被设备驱动程序所封装，用于接收上层软件发来的抽象 I/O 要求，如 read 和 write 命令，转换为具体要求后，发送给设备控制器，控制 I/O 设备工作；它也将由设备控制器发来的信号传送给上层软件，从而为 I/O 内核子系统隐藏设备控制器之间的差异。
        
    4. 中断处理程序
        
        用于保存被中断进程的 CPU 环境，转入相应的中断处理程序进行处理，处理完毕再恢复被中断进程的现场后，返回到被中断进程。中断处理层的主要任务有：进行进程上下文的切换，对处理中断信号源进行测试，读取设备 状态和修改进程状态等。由于中断处理与硬件紧密相关，对用户而言，应尽量加以屏蔽，因此应放在操作系统的底层，系统的其余部分尽可能少地与之发生联系 。
        
    
    ---
    
- **4.1.4 缓冲管理**
    1. 缓冲的引入
        
        在设备管理子系统中，引入缓冲区的目的主要如下：
        
        - 缓和 CPU 与 I/O 设备间速度不匹配的矛盾。
        - 减少对 CPU 的中断频率，放宽对 CPU 中断响应时间的限制。
        - 解决基本数据单元大小（即数据粒度）不匹配的问题。
        - 提高 CPU 和 I/O 设备之间的并行性。
        
        其实现方法如下：
        
        - 采用硬件缓冲器，但由于成本太高，除一些关键部位外，一般不采用硬件缓冲器。
        - 采用缓冲区（位于内存区域）
    2. 缓冲分类
        1. 单缓冲
            
            在主存中设置一个缓冲区。当设备和处理机交换数据时，先将数据写入缓冲区，然后需要数据的设备或处理机从缓冲区取走数据，在缓冲区写入或取出的过程中，另一方需等待。
            
            ![单缓冲工作示意图](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2048.png)
            
            单缓冲工作示意图
            
            如图所示，在块设备输入时，假定从磁盘把一块数据输入到缓冲区的时间为 T，操作系统将该缓冲区中的数据传送到用户区的时间为 M，而 CPU 对这一块数据处理的时间为 C。
            
            在研究每块数据的处理时间时，有一个技巧：假设一种初始状态，然后计算下一次到达相同状态时所需要的时间，就是处理一块数据所需要的时间。在单缓冲中，这种初始状态为：工作区是满的，缓冲区是空的。如题目无明确说明，通常认为缓冲区的大小和工作区的大小相等。
            
            假设 T > C，从初始状态开始，当工作区数据处理完后，时间为 C，缓冲区还没充满，当缓冲区充满时，经历了 T 时间，停止再冲入数据，然后缓冲区向工作区传送数据，当工作区满了后，缓冲区的数据同时也为空，用时为 M，到达下一个开始状态，整个过程用时 M + T；若 T < C，同理，整个过程用时 M + C。故单缓冲区处理每块数据的用时为 max(C, T) + M。
            
        2. 双缓冲
            
            根据单缓冲的特点，CPU 在传送时间 M 内处于空闲状态，由此引入双缓冲。I/O 设备输入数据时先装填到缓冲区 1，在缓冲区 1 填满后才开始装填缓冲区 2，与此同时处理机可以从缓冲区 1 中取出数据送入用户进程，当缓冲区 1 中的数据处理完后，若缓冲区 2 已填满，则处理机又从缓冲区 2 中取岀数据送入用户进程，而 I/O 设备又可以装填缓冲区 1。注意，必须等缓冲区 2 充满 才能让处理机从缓冲区 2 取出数据。双缓冲机制提高了处理机和输入设备的并行程度 。
            
            为了研究双缓冲处理一块数据的用时 ，我们先规定一种初始状态：工作区是空的，其中一个 缓冲区是满的，另外一个缓冲区是空的；我们不妨假设缓冲区 1 是空的，缓冲区 2 是满的。
            
            ![双缓冲工作示意图](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2049.png)
            
            双缓冲工作示意图
            
            假设 T < C + M，缓冲区 2 开始向工作区传送数据，缓冲区 1 开始冲入数据。当工作区充满数据后，缓冲区为空，时间为 M，然后工作区开始处理数据，缓冲区 1 继续冲入数据，因为此时只有一个 I/O 设备，所以缓冲区 2 虽然为空，但不能冲入数据。当缓冲区 1 充满数据后，工作区的数据还未处理完毕，时间为 T。当工作区数据处理完毕后，此时工作区为空，缓冲区 1 满，缓冲区 2 为空（若还有数据需要输入，则缓冲区 2 不为空），达到下一个初始状态，用时 C + M；
            
            再来分析 T > C + M 的情况。缓冲区 2 开始向工作区传送数据，缓冲区 1 开始冲入数据，当工作区充满数据并处理完后，用时 C + M，但缓冲区 1 的数据还未充满；当时间为 T 时，缓冲区 1 的数据充满，到达下一个初始状态。
            
            总结：双缓冲区处理一块数据的用时为 max(C + M, T)。
            
            若 M+C < T，则可使块设备连续输入；若 C + M > T，则可使 CPU 不必等待设备输入。对于字符设备，若采用行输入方式，则采用双缓冲可使用户在输入第一行后，在 CPU 执行第一行中的命令的同时，用户可继续向第二缓冲区输入下一行数据。而单缓冲情况下则必须等待一行数据被提取完毕才可输入下一行的数据。 
            
        3. 循环缓冲
            
            包含多个大小相等的缓冲区，每个缓冲区中有一个链接指针指向下一个缓冲区，最后一个缓冲区指针指向第一个缓冲区，多个缓冲区构成一个环形。
            
            循环缓冲用于输入/输出时，还需要有两个指针 in 和 out。对输入而言，首先要从设备接收数据到缓冲区中，in 指针指向可以输入数据的第一个空缓冲区；当运行进程需要数据时，从循环缓冲区中取一个装满数据的缓冲区，并从此缓冲区中提取数据，out 指针指向可以提取数据的第一 个满缓冲区。输出则正好相反。
            
        4. 缓冲池
            
            由多个系统公用的缓冲区组成，缓冲区按其使用状况可以形成三个队列：空缓冲队列、装满输入数据的缓冲队列（输入队列）和装满输出数据的缓冲队列（输出队列）。还应具有 4 种缓冲 区：用于收容输入数据的工作缓冲区、用于提取输入数据的工作缓冲区、用于收容输出数据的工作缓冲区及用于提取输出数据的工作缓冲区，如图所示。
            
            ![缓冲池的工作方式](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2050.png)
            
            缓冲池的工作方式
            
            - 收容输入：当输入进程需要输入数据时，便从空缓冲队列的队首摘下一个空缓冲区，把它作为收容输入工作缓冲区，然后把输入数据输入其中，装满后再将它挂到输入队列队尾。
            - 提取输入：当计算进程需要输入数据时，便从输入队列取得一个缓冲区作为提取输入工作缓冲区，计算进程从中提取数据，数据 用完后再将它挂到空缓冲队列尾。
            - 收容输出：当计算进程需要输岀数据时，便从空缓冲队列的队首取得一个空缓冲区，作为收容输出工作缓冲区，当其中装满输出数据后，再将它挂到输出队列队尾。
            - 提取输出：当要输出时，由输出进程从输出队列中取得一个装满输出数据的缓冲区，作为提取输出工作缓冲区，当数据提取完后，再将它挂到空缓冲队列的队尾。
    
    ---
    

### 4.2 磁盘管理与磁盘调度算法：SSTF 算法，SCAN 算法，CSCAN 算法，N-STEP-SCAN 算法，FSCAN 算法 ★★★★

- **4.2.1 磁盘访问时间**
    
    一次磁盘读写操作的时间由寻找（寻道）时间、旋转延迟时间和传输时间决定。
    
    1. 寻道时间 $\small T_s$
        
        活动头磁盘在读写信息前，将磁头移动到指定磁道所需要的时间。这个时间除跨越 n 条磁道的时间外，还包括启动磁臂的时间 s，即
        
        $$
        T_s = m × n + s
        $$
        
        式中 m 是与磁盘驱动器速度有关的常数，约为 0.2ms，磁臂的启动时间约为 2ms。
        
    2. 旋转延迟时间 $\small T_r$
        
        磁头定位到某一磁道的扇区所需要的时间，设磁盘的旋转速度为 r，则
        
        $$
        T_r = \frac{1}{2r}
        $$
        
        对于硬盘，典型的旋转速度为 5400 转/分，相当于一周 11.1ms，则 $\small T_r$ 为 5.55ms（$\small \frac{60 × 1000}{2 × 5400} ≈ 5.55$）。
        
    3. 传输时间 $\small T_t$
        
        从磁盘读出或向磁盘写入数据所经历的时间，这个时间取决于每次所读/写的字节数 b 和磁盘的旋转速度：
        
        $$
        T_t = \frac{b}{rN}
        $$
        
        式中，r 为磁盘每秒的转数，N 为一个磁道上的字节数。
        
    
    在磁盘存取时间的计算中，寻道时间与磁盘调度算法相关；而延迟时间和传输时间都与磁盘旋转速度相关，且为线性相关，所以在硬件上，转速是磁盘性能的一个非常重要的参数。
    
    总平均存取时间 $\small T_a$ 可以表示为：
    
    $$
    T_a = T_s + \frac{1}{2r} + \frac{b}{rN}
    $$
    
    ---
    
- **4.2.2 磁盘调度算法**
    1. 先来先服务（FCFS）算法
        
        FCFS 算法根据进程请求访问磁盘的先后顺序进行调度，这是一种最简单的调度算法，如下图所示。该算法的优点是具有公平性。若只有少量进程需要访问，且大部分请求都是访问簇聚的文件扇区，则有望达到较好的性能；若有大量进程竞争使用磁盘，则这种算法在性能上往往接近于随机调度。所以，实际磁盘调度中会考虑一些更为复杂的调度算法。
        
        ![FCFS 磁盘调度算法](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2051.png)
        
        FCFS 磁盘调度算法
        
        例如，磁盘请求队列中的请求顺序分别为 $\small 55, 58, 39, 18, 90, 160, 150, 38, 184$，磁头的初始位置是磁道 100，采用 FCFS 算法时磁头的运动过程如图所示。磁头共移动了 $\small 45 + 3 + 19 + 21 + 72 + 70+ 10+ 112+ 146 = 498$ 个磁道，平均寻道长度为 $\small 498/9 = 55.3$。
        
    2. 最短寻道时间优先（SSTF）算法
        
        SSTF 算法选择调度处理的磁道是与当前磁头所在磁道距离最近的磁道，以便使每次的寻道时间最短。当然，总是选择最小寻找时间并不能保证平均寻找时间最小，但能提供比 FCFS 算法更好的性能。这种算法会产生“饥饿”现象。如下图所示，若某时刻磁头正在 18 号磁道，而在 18 号磁道附近频繁地增加新的请求，则 SSTF 算法使得磁头长时间在 18 号磁道附近工作，将使 184 号磁道的访问被无限期地延迟，即被“饿死”。
        
        ![SSTF 磁盘调度算法](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2052.png)
        
        SSTF 磁盘调度算法
        
        例如，磁盘请求队列中的请求顺序分别为 $\small 55, 58, 39, 18, 90, 160, 150,38, 184$，磁头初始位置是磁道 100，采用 SSTF 算法时磁头的运动过程如图所不。磁头共移动了 $\small 10 + 32 + 3+ 16 + 1 + 20+ 132 + 10 + 24 = 248$ 个磁道，平均寻找长度为 $\small 248/9 = 27.5$。
        
    3. 扫描（SCAN）算法
        
        SCAN 算法在磁头当前移动方向上选择与当前磁头所在磁道距离最近的请求作为下一次服务的对象，实际上就是在最短寻找时间优先算法的基础上规定了磁头运动的方向，如下图所示。 由于磁头移动规律与电梯运行相似，因此又称电梯调度算法。SCAN 算法对最近扫描过的区域不公平，因此它在访问局部性方面不如 FCFS 算法和 SSTF 算法好。
        
        ![SCAN 磁盘调度算法](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2053.png)
        
        SCAN 磁盘调度算法
        
        例如，磁盘请求队列中的请求顺序分别为 $\small 55, 58, 39, 18, 90, 160, 150,38, 184$，磁头初始位置是磁道 100。采用 SCAN 算法时，不但要知道磁头的当前位置，而且要知道磁头的移动方向，假设磁头沿磁道号增大的顺序移动，则磁头的运动过程如图所示。移动磁道的顺序为 $\small 100,150, 160, 184, 200, 90, 58, 55, 39, 38, 18$。磁头共移动了 $\small 50 + 10 + 24 + 16 + 110 + 32 + 3 + 16 + 1 + 20 = 282$ 个磁道，平均寻道长度为 $\small 282/9 = 31.33$。
        
    4. 循环扫描（CSCAN）算法
        
        在扫描算法的基础上规定磁头单向移动来提供服务 ，回返时直接快速移动至起始端而不服务任何请求。由于 SCAN 算法偏向于处理那些接近最里或最外的磁道的访问请求，所以使用改进型的 C-SCAN 算法来避免这个问题，如下图所示。
        
        ![C-SCAN磁盘调度算法](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2054.png)
        
        C-SCAN磁盘调度算法
        
        例如，磁盘请求队列中的请求顺序为 $\small 55, 58, 39, 18, 90, 160, 150, 38, 184$，磁头初始位置是磁道 100。采用 C-SCAN 算法时，假设磁头沿磁道号增大的顺序移动，则磁头的运动过程如上图所示。移动磁道的顺序为 $\small 100, 150, 160, 184, 200, 0, 18,38, 39, 55, 58, 90$。 磁头共移动 $\small 50+ 10 + 24+ 16 + 200+ 18 + 20+ 1 + 16 + 3 + 32 = 390$ 个磁道，平均寻道长度为 $\small 390/9 = 43.33$。
        
        采用 SCAN 算法和 C-SCAN 算法时，磁头总是严格地遵循从盘面的一端到另一端，显然，在实际使用时还可以改进，即磁头移动只需要到达最远端的一个请求即可返回，不需要到达磁盘端点。 这种形式的 SCAN 算法和 C-SCAN 算法称为 LOOK 调度和 C-LOOK 调度，因为它们在朝一个给定方向移动前会查看是否有请求。
        
        ![SCAN LOOK 磁盘调度算法](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2055.png)
        
        SCAN LOOK 磁盘调度算法
        
        注意，若无特别说明，也可以默认 SCAN 算法和 C-SCAN 算法为 LOOK 和 C-LOOK 调度。
        
        ![CSAN C-LOOK 磁盘调度算法](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2056.png)
        
        CSAN C-LOOK 磁盘调度算法
        
    5. NStepSCAN 算法 
        
        在 SSTF、SCAN 及 CSCAN 几种调度算法中，都可能出现磁臂停留在某处不动的情况。例如，有一个或几个进程对某一磁道有较高的访问频率，即这个（些）进程反复请求对某一磁道的 I/O 操作，从而垄断了整个磁盘设备。我们把这一现象称为“磁臂粘着”（Armstickiness） 。 在高密度磁盘上容易出现此情况。 N 步 SCAN 算法是将磁盘请求队列分成若干个长度为 N 的子队列，磁盘调度将按 FCFS 算法依次处理这些子队列。而每处理一个队列时又是按 SCAN 算法，对一个队列处理完后，再处理其他队列。当正在处理某子队列时，如果又出现新的磁盘 I/O 请求，便将新请求进程放入其他队列，这样就可避免出现粘着现象。当 N 值取得很大时，会使 N 步扫描法的性能接近于 SCAN 算法的性能；当 N = 1 时，N 步 SCAN 算法便蜕化为 FCFS 算法。
        
    6. FSCAN 算法
        
        FSCAN 算法实质上是 N 步 SCAN 算法的简化，即 FSCAN 只将磁盘请求队列分成两个子队列。一个是由当前所有请求磁盘 I/O 的进程形成的队列，由磁盘调度按 SCAN 算法进行处理。另一个是在扫描期间，将新出现的所有请求磁盘 I/O 的进程放入等待处理的请求队列。这样，所有的新请求都将被推迟到下一次扫描时处理。
        

### 4.3 设备分配、设备处理、虚拟设备，Spooling 系统 ★★★

- **4.3.1 设备分配**
    1. 设备分配概述
        
        设备分配为用户分配完成 I/O 请求所需的设备和设备控制器，在配置有通道的系统中，还需为用户分配通道。分配的总原则是充分发挥设备的使用效 率，尽可能地让设备忙碌，又要避免由于不合理的分配方法造成进程死锁。从设备的特性来看，采用下述三种使用方式的设备分别称为独占设备、共享设备和虚拟设备。
        
        - 独占式使用设备。进程分配到独占设备后，便由其独占，直至该进程释放该设备。
        - 分时式共享使用设备。对于共享设备，可同时分配给多个进程，通过分时共享使用。
        - 以 SPOOLing 方式使用外部设备。SPOOLing 技术实现了虚拟设备功能，可以将设备同时分配给多个进程。这种技术实质上就是实现了对设备的 I/O 操作的批处理。
    2. 设备分配的数据结构
        
        设备分配依据的主要数据结构有设备控制表（DCT）、控制器控制表（COCT）、通道控制表（CHCT）和系统设备表（SDT），各数据结构功能如下。
        
        - 设备控制表（DCT）： 一个设备控制表就表征一个设备，而这个控制表中的表项就是设备的各个属性，如下图所示。
            - 设备队列队首指针：凡因请求本设备而未得到满足的进程，应将其 PCB 按某种策略排成一个设备请求队列，设备队列的队首指针指向该请求队列队首 PCB。
            - 指向控制器表的指针：该指针指向该设备所连接的控制器的控制表。
            
            ![设备控制表 DCT](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2057.png)
            
            设备控制表 DCT
            
        - 控制器控制表（COCT）：系统为每一个控制器都设置了用于记录控制器情况的控制器控制表。
        - 通道控制表（CHCT）：每个通道都有—张通道控制表。
            
            设备控制器控制设备与内存交换数据，而设备控制器又需要请求通道为它服务 ，因此每个控制器控制表（COCT）有一个表项存放指向相应通道控制表（CHCT）的指针，而一个通道可为多个设备控制器服务，因此 CHCT 中必定有一个指针，指向一个表，这个表上的信息表达的是 CHCT 提供服务的那几个设备控制器。CHCT 与 COCT 的关系是一对多的关系。
            
        
        ![控制器控制表 COCT、通道控制表 CHCT](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2058.png)
        
        控制器控制表 COCT、通道控制表 CHCT
        
        - 系统设备表（SDT）：整个系统只有一张 SDT。它记录已连接到系统中的所有物理设备的情况，每个物理设备占一个表目。
            
            ![系统设备表 SDT](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2059.png)
            
            系统设备表 SDT
            
    3. 设备分配的策略
        
        在多道程序系统中，进程数多于资源数，因此要有一套合理的分配原则，主要考虑的因素有：I/O 设备的固有属性、I/O 设备的分配算法、I/O 设备分配的安全性以及 I/O 设备的独立性。
        
        - 设备分配原则。设备分配应根据设备特性、用户要求和系统配置情况。既要充分发挥设备的使用效率，又要避免造成进程死锁，还要将用户程序和具体设备隔离开 。
        - 设备分配方式。设备分配方式有静态分配和动态分配两种。
            - ① 静态分配主要用于对独占设备的分配，它在用户作业开始执行前，由系统一次性分配该作业所要求的全部设备、 控制器。一旦分配，这些设备、控制器就一直为该作业所占用，直到该作业被撤销。静态分配方式不会出现死锁，但设备的使用效率低。
            - ② 动态分配在进程执行过程中根据执行需要进行。当进程需要设备时，通过系统调用命令向系统提出设备请求，由系统按某种策略给进程分配所需要的设备 、控制器，一旦用完，便立即释放。这种方式有利于提高设备利用率，但若分配算法使用不当，则有可能造成进程死锁。
        - 设备分配算法。常用的动态设备分配算法有先请求先分配、优先级高者优先等。
            
            对于独占设备，既可以釆用动态分配方式，又可以采用静态分配方式，但往往采用静态分配方式。共享设备可被多个进程所共享，一般采用动态分配方式，但在每个 I/O 传输的单位时间内只被一个进程所占有，通常采用先请求先分配和优先级高者优先的分配算法 。
            
    4. 设备分配的安全性
        
        设备分配的安全性是指设备分配中应防止发生进程死锁。 
        
        - 安全分配方式。每当进程发出 I/O 请求后便进入阻塞态，直到其 I/O 操作完成时才被唤醒。这样，一旦进程已经获得某种设备后便阻塞，不能再请求任何资源，而在它阻塞时也不保持任何资源。其优点是设备分配安全，缺点是 CPU 和 I/O 设备是串行工作的。
        - 不安全分配方式。进程在发出 I/O 请求后仍继续运行，需要时又发出第二个、第三个 I/O 请求等。仅当进程所请求的设备已被另一进程占用时，才进入阻塞态。优点是一个进程可同时操作多个设备，使进程推进迅速，缺点是有可能造成死锁。
    5. 逻辑设备名到物理设备名的映射
        
        为了提高设备分配的灵活性和设备的利用率，方便实现 I/O 重定向，引入了设备独立性。设备独立性是指应用程序独立于具体使用的物理设备。
        
        为了实现设备独立性，在应用程序中使用逻辑设备名来请求使用某类设备 ，在系统中设置一 张逻辑设备表（Logical Unit Table, LUT），用于将逻辑设备名映射为物理设备名。LUT 表项包括逻辑设备名、物理设备名和设备驱动程序入口地址；当进程用逻辑设备名来请求分配设备时，系统为它分配一台相应的物理设备，并在 LUT 中建立一个表目，当以后进程再利用该逻辑设备名请求 I/O 操作时，系统通过查找 LUT 来寻找对应的物理设备和驱动程序。
        
        在系统中可釆取两种方式设置逻辑设备表： 
        
        - ① 在整个系统中只设置一张 LUT。这样，所有进程的设备分配情况都记录在同一张 LUT 中， 因此不允许 LUT 中具有相同的逻辑设备名，主要适用于单用户系统。
        - ② 为每个用户设置一张 LUT。每当用户登录时，系统便为该用户建立一个进程，同时也为之建立一张 LUT，并将该表放入进程的 PCB 中。
    
    ---
    
- **4.3.2 设备处理**
    
    设备处理启动设备进行真正的 I/O 操作，响应并处理设备控制器发来的中断请求。
    
    设备处理程序又称为设备驱动程序，它是 I/0 系统的高层与设备控制器之间的通信程序。其基本任务是用于实现 CPU 和设备控制器之间的通信，即由 CPU 向设备控制器发出 I/O 命令，要求它完成指定的 I/O 操作。反之，由 CPU 接收从控制器发来的中断请求，并给予迅速的响应和相应的处理。由于驱动程序与硬件密切相关，故通常应为每一类设备配置一种驱动程序。
    
    设备处理过程是：首先检查 I/O 请求的合法性，了解设备状态是否是空闲的，读取有关的传递参数及设置设备的工作方式。然后向设备控制器发出 I/O 命令，启动 I/O 设备完成指定的 I/O 操作。此外设备驱动程序还应能及时响应由控制器发来的中断请求，并根据该中断请求的类型，调用相应的中断处理程序进行处理。对于设置了通道的计算机系统， 设备处理程序还应能根据用户的 I/O 请求自动地构成通道程序。
    
    ---
    
- **4.3.3 Spooling 系统、虚拟设备**
    
    为了缓和 CPU 的高速性与 I/O 设备低速性之间的矛盾，引入了脱机输入/输出技术，它是操作系统中采用的一项将独占设备改造成共享设备的技术。该技术利用专门的外围控制机，将低速  I/O 设备上的数据传送到高速磁盘上，或者相反。
    
    SPOOLing 技术是对脱机输入/输出系统的模拟，它不需要外围控制机，但必须建立在具有多道程序功能的操作系统上，而且还需要得到高速随机外存（通常采用磁盘）的支持。
    
    1. Spooling 系统的组成
        - 输入井和输出井：这是在磁盘上开辟的两个大存储空间。
            - 输入井是模拟脱机输入时的磁盘，用于暂存 I/0 设备输入的数据；
            - 输出井是模拟脱机输出时的磁盘，用来暂存用户程序的输出数据。
        - 输入缓冲区和输出缓冲区：这是在内存中开辟的两个缓冲区。
            - 输入缓冲区用于暂存由输入设备送来的数据，以后再传送到输入井。
            - 输出缓冲区用来暂存从输出井送来的数据，以后再传送给输出设备。
        - 输入进程和输出进程：输入/输出进程用于模拟脱机输入/输出时的外围控制机。
            - 输入进程，也称为预输入进程，将用户要求的数据从输入设备传送到输入缓冲区，再存放到输入井。当 CPU 需要输入设备时，直接从输入井读入内存。
            - 输出进程，也称为缓输出进程，把用户要求输出的数据，先从内存送到输出井，待输出设备空闲时， 再将输出井中的数据经过输出缓冲区送到输出设备上。
        - 井管理程序：用于控制作业与磁盘井之间信息的交换。当作业执行过程中向某台设备发出启动输入或输出操作请求时，由操作系统调用井管理程序，由其控制从输入井读取信息或将信息输出至输出井。
        
        ![SPOOLing 系统的组成](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2060.png)
        
        SPOOLing 系统的组成
        
    2. Spooling 系统的特点
        - 提高了 I/O 的速度。这里对数据所执行的 I/O 操作，已从对低速 I/O 设备执行的 I/O 操作演变为对磁盘缓冲区中数据的存取，如同脱机输入输出一样，提高了 I/O 速度，缓和了 CPU 与低速 I/O 设备之间速度不匹配的矛盾。
        - 将独占设备改造为共享设备。因为在[假脱机打印机系统](https://www.notion.so/d6789a488c934a009190634bba8e1811)中，实际上并没为任何进程分配设备，而只是在磁盘缓冲区中为进程分配一个空闲盘块和建立一张 I/O 请求表。这样，便把独占设备改造为共享设备。
        - 实现了虚拟设备功能。宏观上，虽然是多个进程在同时使用一台独占设备，而对于每一个进程而言，它们都会认为自己是独占了一个设备。当然，该设备只是逻辑上的设备。假脱机打印机系统实现了将独占设备变换为若干台对应的逻辑设备的功能。
    3. 假脱机打印机系统
        
        共享打印机是使用 SPOOLing 技术的实例。当用户进程请求打印输出时，SPOOLing 系统同意打印，但是并不真正立即把打印机分配给该进程，而由假脱机管理进程完成两项任务：
        
        - 在磁盘缓冲区中为之申请一个空闲盘块，并将要打印的数据送入其中暂存。
        - 为用户进程申请一张空白的用户请求打印表，并将用户的打印要求填入其中，再将该表挂到假脱机文件队列上。
        
        这两项工作完成后，虽然还没有任何实际的打印输岀，但是对于用户进程而言，其打印任务已完成。对用户而言，系统并非立即执行真实的打印操作，而只是立即将数据输岀到缓冲区，真正的打印操作是在打印机空闲且该打印任务已排在等待队列队首时进行的。
        
    
    ---
    

## 5. 文件系统

### 5.1 基本概念：文件和文件系统、目录、文件结构的物理结构和逻辑结构（顺序文件、索引顺序文件、索引文件、HASH 文件）、文件共享（基于索引节点、基于符号链接实现文件共享）★★★★

- **5.1.1 文件和文件系统**
    1. 文件的基本概念
        
        首先了解文件的结构，我们通过自底向上的方式来定义。 
        
        - 数据项：是文件系统中最低级的数据组织形式，可分为以下两种类型：
            - 基本数据项：用于描述一个对象的某种属性的一个值。是数据组织中可以命 名的最小逻辑数据单位，又称为字段。例如，用于描述一个学生的基本数据项有：学号、姓名、年龄、所在班级等。
            - 组合数据项：由多个基本数据项组成。
        - 记录：是一组相关的数据项的集合，用于描述一个对象在某方面的属性。
        - 文件：是指由创建者所定义的、具有文件名的一组相关元素的集合，可分为有结构文件和无结构文件两种。在有结构的文件中，文件由若干个记录组成，如一个班的学生记录；而无结构文件则被视为一个字符流，比如一个二进制文件或字符文件。文件在文件系统中是一个最大的数据单位，它描述了一个对象集。
            
            除了文件数据，操作系统还会保存与文件相关的信息等，这些附加信息称为文件属性或文件元数据。文件属性在不同系统中差别很大 ，但通常都包括如下属性：
            
            - 名称：（同一目录下）文件名称唯一，以容易读取的形式保存。
            - 类型：可以从不同的角度来规定文件的类型，如源文件、目标文件及可执行 文件等。
            - 长度（大小）：指文件的当前长度（大小），长度的单位可以是字节、字或块，也可能是最大允许的长度。
            - 物理位置：通常用于指示文件所在的设备及所在设备中地址的指针。
            - 创建者：文件创建者的 ID（身份标识符）。
            - 所有者：文件当前所有者的 ID。
            - 创建时间、最后一次修改时间和最后一次存取时间。
            - 保护：对文件进行保护的访问控制信息。
            
            操作系统通过[文件控制块（FCB）](https://www.notion.so/d6789a488c934a009190634bba8e1811)来维护文件元数据。
            
    2. 文件系统的层次结构
        
        如图所示，文件系统的模型可分为三个层次：最底层是对象及其属性，中间层是对对象进行操纵和管理的软件集合，最高层是文件系统提工给用户的接口。
        
        ![文件系统模型](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2061.png)
        
        文件系统模型
        
        1. 对象及其属性
            
            文件管理系统管理的对象如下：
            
            - ① 文件。在文件系统中有着各种不同类型的文件， 它们都作为文件管理的直接对象。
            - ② 目录。为了方便用户对文件的存取和检索，在文件系统中必须配置目录，在目录的每个目录项中，必须含有文件名、对文件属性的说明，以及该文件所在的物理地址（或指针）。对目录的组织和管理，是方便用户和提高对文件存取速度的关键。
                
                注意：站在用户的角度看，文件系统的目的主要实现“按名存取”。
                
            - ③ 外存存储空间。文件和目录必定占用存储空间，对这部分空间的有效管理，不仅能提高外存的利用率，而且能提高对文件的存取速度。
                
                注意：对于目录和外存的有效管理，是为了实现文件系统的主要目标：提高存储空间利用率；提高文件存取速度，减少存取时间。
                
        2. 对对象操纵和管理的软件集合
            
            该层是文件管理系统的核心部分。文件系统的功能大多是在这一层实现的，其中包括有： 
            
            - ① 对文件存储空间的管理；
            - ② 对文件目录的管理；
            - ③ 用于将文件的逻辑地址转换为物理地址的机制；
            - ④ 对文件读和写的管理；
            - ⑤ 对文件的共享与保护等功能。
            
            在实现这些功能 时， OS 通常都采取了层次组织结构，即在每一层中都包含了一定的功能，处于某个层次的软件，只能调用同层或更低层次中的功能模块。
            
            一般地，把与文件系统有关的软件分为四个层次：
            
            - ① I/O 控制层，是文件系统的最低层，主要由磁盘驱动程序等组成，也可称为设备驱动程序层。
            - ② 基本文件系统层，主要用于处理内存与磁盘之间数据块的交换。
            - ③ 基本 I/O 管理程序，该层用于完成与磁盘 I/O 有关的事务，如将文件逻辑块号转换为物理块号，管理磁盘中的空闲盘块，I/O 缓冲的指定等。
            - ④ 逻辑文件系统，用于处理与记录和文件相关的操作，如允许用户和应用程序使用符号文件名访问文件及记录，实现对文件和记录的保护等。
        3. 文件系统的接口
            
            为方便用户的使用，文件系统以接口的形式提供了一组对文件和记录操作的方法和手段。通常是下面两种类型的接口：
            
            - 命令接口，是指作为用户与文件系统直接交互的接口，用户可通过键盘终端键入命令取得文件系统的服务。
            - 程序接口，是指作为用户程序与文件系统的接口，用户程序可通过系统调用取得文件系统的服务。例如，用于创建文件的系统调用 Creat，用于打开一个文件的系统调用 Open 等。
    
    ---
    
- **5.1.2 文件的逻辑结构**
    
    文件的逻辑结构是从用户观点出发看到的文件的组织形式，即文件是由一系列的逻辑记录组成的，是用户可以直接处理的数据及其结构，它独立于文件的物理特性，又称为文件组织（File Organization）。
    
    按逻辑结构，文件可划分为无结构文件和有结构文件两大类。
    
    1. 无结构文件（流式文件）
        
        无结构文件是最简单的文件组织形式。无结构文件将数据按顺序组织成记录并积累、保存， 它是有序相关信息项的集合，以字节（Byte）为单位。由于无结构文件没有结构，因而对记录的访问只能通过穷举搜索的方式，因此这种文件形式对大多数应用不适用。但字符流的无结构文件管理简单，用户可以方便地对其进行操作。所以，那些对基本信息单位操作不多的文件较适于采用字符流的无结构方式，如源程序文件、目标代码文件等。
        
    2. 有结构文件（记录式文件）
        
        有结构文件按记录的组织形式可以分为如下几种：
        
        1. 顺序文件
            
            文件中的记录一个接一个地顺序排列，记录通常是定长的，可以顺序存储或以链表形式存储。 顺序文件有以下两种结构：第一种是串结构，记录之间的顺序与关键字无关，通常是按存入时间的先后进行排列，对串结构文件进行检索必须从头开始顺序依次查找，比较费时。第二种是顺序结构，文件中的所有记录按关键字顺序排列，可采用折半查找法提高检索效率。
            
            在对记录进行批量操作，即每次要读或写一大批记录时，顺序文件的效率是所有逻辑文件中最高的。此外，对于顺序存储设备（如磁带），也只有顺序文件才能被存储并能有效地工作。在经常需要查找、修改、增加或删除单个记录的场合，顺序文件的性能比较差。
            
        2. 索引文件
            
            变长记录文件只能顺序查找，效率较低。为此，可以建立一张索引表，为主文件的每个记录在索引表中分别设置一个表项，包含指向变长记录的指针（即逻辑起始地址）和记录长度，索引表按关键字排序，因此其本身也是一个定长记录的顺序文件。这样就把对变长记录顺序文件的检索转变为对定长记录索引文件的随机检索，从而加快了记录的检索速度。同时，利用索引文件插入和删除记录也非常方便。只是它除了有主文件外，还须配置一张索引表，而且每个记录都要有一个索引项，因此增加了存储开销。
            
        3. 索引顺序文件
            
            索引顺序文件是顺序文件和索引文件的结合，基本上克服了变长记录的顺序文件不能随机访问，以及不便于记录的删除和插入的缺点。它仍保留了顺序文件的关键特征，即记录是按关键字的顺序组织起来的。它又增加了两个新特征：一个是引入了文件索引表，通过该表可以实现对索引顺序文件的随机访问；另一个是增加了溢出（overflow）文件，用它来记录新增加的、删除的和修改的记录。
            
            最简单的索引顺序文件只使用了一级索引。索引顺序文件将顺序文件中的所有记录分为若干组，为顺序文件建立一张索引表，在索引表中为每组中的第一条记录建立一个索引项，其中含有该记录的关键字值和指向该记录的指针。
            
            ![索引顺序文件示意图](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2062.png)
            
            索引顺序文件示意图
            
            如图所示，主文件名包含姓名和其他数据项。姓名为关键字，索引表中为每组第一条记录（不是每条记录）的关键字值，用指针指向主文件中该记录的起始位置。索引表只包含关键字和指针两个数据项，所有姓名关键字递增排列。主文件中记录分组排列，同一个组中的关键字可以无序，但组与组之间的关键字必须有序。查找一条记录时，首先通过索引表找到其所在的组，然后在该组中使用顺序查找，就能很快地找到记录。
            
            对于含有 N 条记录的顺序文件，查找某关键字的记录时，平均需要查找 $\small \mathrm{N/2}$ 次。在索引顺序 文件中，假设 N 条记录分为 $\small \mathrm{\sqrt{N}}$ 组，则索引表中有 $\small \mathrm{\sqrt{N}}$ 个表项，每组有 $\small \mathrm{\sqrt{N}}$ 条记录，在查找某关键字的记录时，先顺序查找索引表，需要查找 $\small \mathrm{\sqrt{N} / 2}$ 次，然后在主文件中对应的组中顺序查找，也需要查找 $\small \mathrm{\sqrt{N} / 2}$ 次，因此共需查找 $\small \mathrm{\sqrt{N}}$ 次。显然，索引顺序文件提高了查找效率。若记录数很多，则可采用两级或多级索引。
            
            索引文件和索引顺序文件都提高了存取的速度，但因为配置索引表而增加了存储空间。
            
        4. 直接文件和哈希（Hash）文件
            
            对于直接文件，可根据给定的关键字直接获得指定记录的物理地址，具有有很高的存取速度。换而言之，关键字本身就决定了记录的物理地址。
            
            哈希文件是目前应用最为广泛的一种直接文件。它利用 Hash 函数（或称散列函数）将关键字转换为相应记录的地址。但为了能实现文件存储空间的动态分配，通常由 Hash 函数所求得 的并非是相应记录的地址，而是指向某一目录表相应表目的指针，该表目的内容指向相应记录所在的物理块。不同于顺序文件或索引文件，哈希文件结构没有顺序的特性。
            
    
    ---
    
- **5.1.3 文件的物理结构**
    
    文件的物理结构就是研究文件的实现，即文件数据在物理存储设备上是如何分布和组织的，又称为文件的存储结构。其不仅与存储介质的存储性能有关，而且与所采用的外存分配（组织）方式有关。目前常用的外存分配方式有：连续分配、链接分配和索引分配。详见：[5.2 外存分配方法：连续分配、链接分配、索引分配](https://www.notion.so/d6789a488c934a009190634bba8e1811)
    
    无论是文件的逻辑结构，还是其物理结构，都会影响对文件的检索速度。
    
    - Q：单个文件的逻辑结构和物理结构之间是否存在某些制约关系？
        
        文件的逻辑结构是用户可见的结构，即从用户角度看到的文件的全貌。文件的物理结构是文件在存储器上的组织结构，它表示一个文件在辅存上安置、链接、编目的方法。它和文件的存取方法以及存储设备的特性等都有着密切的联系 。单个文件的逻辑结构和物理结构之间虽无明显的制约或关联关系，但是如果物理结构选择不慎，也很难体现出逻辑结构的特点 ，比如一个逻辑结构是顺序结构，而物理结构是隐式链接结构的文件，即使理论上可以很快找出某条记录的地址， 而实际找时仍然需要在磁盘上一块一块地找。
        
    
    ---
    
- **5.1.4 文件目录**
    
    文件目录也是一种数据结构，用于标识系统中的文件及其物理地址，供检索时使用。对目录管理的要求：实现“按名存取”、提高对目录的检索速度、文件共享、允许文件重名。
    
    1. 文件控制块（FCB）
        
        文件控制块（FCB）与文件一一对应，是用来存放描述和控制文件需要的各种信息的数据结构 ，以实现文件的“按名存取”。一个 FCB 就是一个文件目录项，FCB 的有序集合称为文件目录。如图是一个典型的 FCB。为了创建一个新文件，系统将分配一个 FCB 并存放在文件目录中，称为目录项。
        
        ![一个典型的FCB（左图）与 UNIX 的文件目录结构（右图）](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/%25E4%25B8%2580%25E4%25B8%25AA%25E5%2585%25B8%25E5%259E%258B%25E7%259A%2584FCBUNIX%25E7%259A%2584%25E6%2596%2587%25E4%25BB%25B6%25E7%259B%25AE%25E5%25BD%2595%25E7%25BB%2593%25E6%259E%2584.png)
        
        一个典型的FCB（左图）与 UNIX 的文件目录结构（右图）
        
        FCB主要包含以下信息：
        
        - 基本信息：如文件名、文件的物理位置、文件的逻辑结构、文件的物理结构等。
        - 存取控制信息：包括文件主的存取权限、核准用户的存取权限以及一般用户的存取权限。
        - 使用信息：如文件建立时间、上次修改时间以及当前使用信息等。
        
        一个文件目录也被视为一个文件，称为目录文件。
        
    2. 索引节点
        1. 索引节点的引入
            
            文件目录通常存放在磁盘上，当文件很多时，文件目录会占用大量的盘块。在查找目录的过程中，要先将存放目录文件的第一个盘块中的目录调入内存，然后用给定的文件名逐一比较，若未找到指定文件，就还需要不断地将下一盘块中的目录项调入内存，逐一比较。通过分析发现，在检索目录的过程中，只用到了文件名，仅当找到一个目录项（其中的文件名与要查找的文件名匹配）时，才需从该目录项中读出该文件的物理地址。也就是说，在检索目录时，文件的其他描述信息不会用到，也不需要调入内存。因此，有的系统（如 UNIX）便采用了文件名和文件描述信息分开的方法，使文件描述信息单独形成一个称为索引结点的数据结构，简称 i 结点（inode）。在文件目录中的每个目录项仅由文件名和指向该文件所对应的 i 结点的指针构成，如上图右图所示。
            
            假设一个 FCB 为 64B，盘块大小是 1KB，则每个盘块中可以存放 16 个FCB（FCB 必须连续存放）。若一个文件目录共有 640 个 FCB，需占用 40 个盘块（40KB），则查找文件平均需要启动磁盘 20 次。而在 UNIX 系统 中，一个目录项仅占 16B，其中 14B 是文件名，2B 是 i 结点指针。在 1KB 的盘块中可存放 64 个目录项。这样，可使查找文件的平均启动磁盘次数减少到原来的 1/4，大大节省了系统开销。
            
            Q：知道文件控制块 FCB 大小、单个盘快大小和文件目录项数目，计算查找一个文件需要访问几次硬盘。
            
        2. 磁盘索引结点
            
            存放在磁盘上的索引结点。每个文件有一个唯一的磁盘索引结点。
            
            主要内容包括：文件主标识符（拥有该文件的个人或小组的标识符）、文件类型、文件存取权限、文件物理地址、文件长度、文件链接计数（文件系统中所有指向该文件的文件名的指针计数）、文件存取时间。
            
        3. 内存索引结点
            
            存放在内存中的索引结点。当文件被打开时，要将磁盘索引结点拷贝到内存的索 引结点中，便于以后使用。
            
            在内存索引结点中又增加了：索引结点编号（用于标识内存索引结点）、状态（指示 i 结点是否上锁或被修改）、访问计数（每当有一进程要访问此 i 结点时，将该访问计数加 1 ，访问完再减 1）、文件所属文件系统的逻辑设备号、链接指针（分别指向空闲链表和散列队列的指针）。
            
    3. 目录结构分为：单级目录、二级目录、多级（树形）目录。详见：[5.3 目录管理：单级目录、二级目录、多级目录](https://www.notion.so/d6789a488c934a009190634bba8e1811)
    
    ---
    
- **5.1.5 文件共享**
    
    文件共享使多个用户共享同一个文件，系统中只需保留该文件的一个副本。现代常用的两种文件共享方法如下。
    
    1. 基于索引结点实现文件共享（硬链接）
        
        在树形结构的目录中，当有两个或多个用户要共享一个子目录或文件时，必须将共享文件或子目录链接到两个或多个用户的目录中，才能方便地找到该文件，如图所示。
        
        ![基于索引结点的共享方式](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2063.png)
        
        基于索引结点的共享方式
        
        在这种共享方式中，诸如文件的物理地址及其他的文件属性等信息，不再放在目录项中，而是放在索引结点中。在文件目录中只设置文件名及指向相应索引结点的指针。在索引结点中还应有一个链接计数 count，用于表示链接到本索引结点（即文件）上的用户目录项的数目。当 count = 2 时，表示有两个用户目录项链接到本文件上，或者说有两个用户共享此文件。
        
        用户 A 创建一个新文件时，他便是该文件的所有者，此时将 count 置为 1。用户 B 要共享此文 件时，在 B 的目录中增加一个目录项，并设置一个指针指向该文件的索引结点。此时，文件主仍然是用户 A，而 count = 2。如果用户 A 不再需要此文件，能否直接将文件删除呢？答案是否定的。因为若删除了该文件，也必然删除了该文件的索引结点，这样便会使用户 B 的指针悬空，而 B 可能正在此文件上执行写操作，此时将因此半途而废。因此用户 A 不能删除此文件，只是将该文件的 count 减 1，然后删除自己目录中的相应目录项。用户 B 仍可以使用该文件。当 count = 0 时，表示没有用户使用该文件，才会删除该文件。如图给出了用户 B 链接到文件上的前、后情况。
        
        ![文件共享中的链接计数](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2064.png)
        
        文件共享中的链接计数
        
    2. 基于符号链接实现文件共享（软链接）
        
        为使用户 B 能共享用户 A 的一个文件 F，可以由系统创建一个 LINK 类型的新文件（即符号链接本身是一个文件），也取名为 F，并将该文件写入用户 B 的目录中，以实现用户 B 的目录与文件 F 的链接。在新文件中只包含被链接文件 F 的路径名。当用户 B 要访问被链接的文件 F 且正要读 LINK 类新文件时，操作系统查看到要读的文件是 LINK 类型，则根据该文件中的路径名去找到文件 F，然后对它进行读， 从而实现用户 B 对文件 F 的共享。这样的链接方法被称为符号链接（软链接）。
        
        在利用符号链方式实现文件共享时，只有文件主才拥有指向其索引结点的指针。而共享该文件的其他用户只有该文件的路径名，并不拥有指向其索引结点的指针（即无论多少个符号链接指向源文件，源文件索引节点的计数值都不会变）。这样，也就不会发生在文件主删除一共享文件后留下一悬空指针的情况。当文件主把一个共享文件删除后，若其他用户又试图通过符号链去访问它时，则会访问失败，于是将符号链接删除，此时不会产生任何影响。
        
        在符号链接的共享方式中，当其他用户读共享文件时，系统根据文件路径名逐个查找目录，直至找到该文件的索引结点。因此，每次访问共享文件时，都可能要多次地读盘。使得访问文件的开销甚大，且增加了启动磁盘的频率。此外，符号链接的索引结点也要耗费一定的磁盘空间。
        
        利用符号链接实现网络文件共享时，只需提供该文件所在机器的网络地址及文件路径名。
        
    
    硬链接和软链接都是文件系统中的静态共享方法，在文件系统中还存在着另外的共享需求， 即两个进程同时对同一个文件进行操作，这样的共享称为动态共享。
    
    可以这样说：文件共享，“软” “硬”兼施。硬链接就是多个指针指向一个索引结点，保证只要还有一个指针指向索引结点，索引结点就不能删除；软链接就是把到达共享文件的路径记录下来，当要访问文件时，根据路径寻找文件。可见，硬链接的查找速度要比软链接的快 。
    
    ---
    
- **5.1.6 文件保护**
    
    为了防止文件共享可能会导致文件被破坏或未经核准的用户修改文件，文件系统必须控制用户对文件的存取，即解决对文件的读、写、执行的许可问题。为此，必须在文件系统中建立相应的文件保护机制。文件保护通过口令保护、加密保护和访问控制等方式实现。其中，口令和加密是为了防止用户文件被他人存取或窃取 ，而访问控制则用于控制用户对文件的访问方式。
    
    1. 访问类型
        
        对文件的保护可从限制对文件的访问类型中出发。可加以控制的访问类型主要有以下几种。
        
        - 读。从文件中读。
        - 写。向文件中写。
        - 执行。将文件装入内存并执行。
        - 添加。将新信息添加到文件结尾部分。
        - 删除。删除文件，释放空间。
        - 列表清单。列出文件名和文件属性。
        
        此外还可以对文件的重命名、复制、编辑等加以控制。这些高层的功能可以通过系统程序调用低层系统调用来实现。保护可以只在低层提供。例如，复制文件可利用一系列的读请求来完成, 这样，具有读访问权限的用户同时也就具有了复制和打印权限。
        
    2. 访问控制
        
        解决访问控制最常用的方法是根据用户身份进行控制。而实现基于身份访问的最为普通的方法是，为每个文件和目录增加一个访问控制列表（Access-Control List, ACL），以规定每个用户 名及其所允许的访问类型。这种方法的优点是可以使用复杂的访问方法，缺点是长度无法预计并且可能导致复杂的空间管理，使用精简的访问列表可以解决这个问题。 
        
        精简的访问列表采用拥有者、组和其他三种用户类型。
        
        - ① 拥有者。创建文件的用户。
        - ② 组。一组需要共享文件且具有类似访问的用户。
        - ③ 其他。系统内的所有其他用户。
        
        这样，只需用三个域即可列出访问表中这三类用户的访问权限 。文件拥有者在创建文件时， 说明创建者用户名及所在的组名，系统在创建文件时也将文件主的名字、所属组名列在该文件的 FCB 中。用户访问该文件时，按照拥有者所拥有的权限访问文件，若用户和拥有者在同一个用户组，则按照同组权限访问，否则只能按其他用户权限访问。UNIX 操作系统即采用此种方法。
        
    3. 口令保护
        
        口令指用户在建立一个文件时提供一个口令，系统为其建立 FCB 时附上相应口令。用户请求访问时必须提供相应的口令。这种方法时间和空间的开销不多，缺点是口令直接存在系统内部，不够安全。 
        
    4. 加密保护
        
        加密保护指用户对文件进行加密，文件被访问时需要使用密钥。这种方法保密性强，节省了存储空间，不过编码和译码要花费一定的时间。 
        
        口令和密码都是防止用户文件被他人存取或窃取 ，并没有控制用户对文件的访问类型 。 
        
    
    ---
    

### 5.2 外存分配方法：连续分配、链接分配、索引分配 ★★★★

如前所述，[文件的物理结构](https://www.notion.so/d6789a488c934a009190634bba8e1811)直接与外存的分配（组织）方式有关。对于不同的外存分配方式， 将形成不同的文件物理结构。目前常用的外存分配方式有：连续分配、链接分配和索引分配。

- **5.2.1 连续分配（连续组织方式）**
    
    在对文件采取连续分配组织方式时，为每个文件分配一片连续的磁盘空间，由此所形成的文件物理结构将是顺序式的文件结构。
    
    连续分配方法要求每个文件在磁盘上占有一组连续的块，如图所示。磁盘地址定义了磁盘上的一个线性排序，这种排序使作业访问磁盘时需要的寻道数和寻道时间最小 。
    
    ![连续分配](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2065.png)
    
    连续分配
    
    采用连续分配时，逻辑文件中的记录顺序也存储在相邻接的块中。一个文件的目录项中“文件物理地址”字段应包括第一块的地址和该文件所分配区域的长度，若文件长 n 块并从位置 b 开始，则该文件将占有块 b，b + l，b + 2，…，b + n-1。
    
    连续分配支持顺序访问和直接访问。优点是实现简单、存取速度快。缺点是：
    
    - 文件长度不宜动态增加，因为一个文件末尾后的盘块可能已分配给其他文件，一旦需要增加，就需要大量移动盘块。
    - 为保持文件的有序性，删除和插入记录时，需要对相邻的记录做物理上的移动，还会动态改变文件的长度。
    - 反复增删文件后会产生外部碎片（与内存管理分配方式中的碎片相似）。
    - 很难确定一个文件需要的空间大小，因而只适用于长度固定的文件。
- **5.2.2 链接分配（链接组织方式）**
    
    在对文件采取链接组织方式时，可以为每个文件分配不连续的磁盘空间，通过链接指针将一个文件的所有盘块链接在一起，由此所形成的将是链接式文件结构。
    
    链接分配是一种釆用离散分配的方式。它消除了磁盘的外部碎片，提高了磁盘的利用率。可以动态地为文件分配盘块，因此无须事先知道文件的大小。此外，对文件的插入、删除和修改也非常方便。链接分配又可分为隐式链接和显式链接两种形式。
    
    1. 隐式链接
        
        隐式链接方式如图所示。目录项中含有文件第一块和最后一块的指针。每个文件对应一个磁盘块的链表；磁盘块分布在磁盘的任何地方，除最后一个盘块外，每个盘块都含有指向文件下一个盘块的指针，这些指针对用户是透明的。
        
        ![隐式链接分配](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2066.png)
        
        隐式链接分配
        
        隐式链接的缺点是只适合顺序访问，若要访问文件的第 i 个盘块，则只能从文件第一个盘块开始通过盘块指针顺序查找到第 i 块，随机访问效率很低。隐式链接的稳定性也是一个问题，系统在运行过程中由于软件或硬件错误导致链表中的指针丢失或损坏，会导致文件数据的丢失。
        
        通常的解决方案是，将几个盘块组成簇（cluster），按簇而不按块来分配，可以成倍地减少查找时间。比如一簇为 4 块，这样，指针所占的磁盘空间比例也要小得多。这种方法的代价是增加了内部碎片。簇可以改善许多算法的磁盘访问时间，因此应用于大多数操作系统。
        
    2. 显式链接
        
        显式链接是指把用于链接文件各物理块的指针，从每个物理块的末尾中提取岀来，显式地存放在内存的一张链接表中。该表在整个磁盘中仅设置一张，称为文件分配表（File Allocation Table，FAT）。每个表项中存放链接指针，即下一个盘块号。文件的第一个盘块号记录在目录项“物理地址”字段中，后续的盘块可通过查 FAT 找到。例如，某磁盘共有 100 个磁盘块，存放了两个文件：文件“aaa”占三个盘块，依次是 2→8→5；文件“bbb”占两个盘块，依次是 7→1。其余盘块都是空闲盘块，则该磁盘的 FAT 表如图所示。
        
        ![显式链接分配 & 文件分配表](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2067.png)
        
        显式链接分配 & 文件分配表
        
        不难看出，文件分配表FAT的表项与全部磁盘块一一对应，并且可以用一个特殊的数字 `-1` 表示文件的最后一块，可以用 `-2` 表示这个磁盘块是空闲的（当然也可指定为 -3，-4）。因此，FAT 不仅记录了文件各块之间的先后链接关系，同时还标记了空闲的磁盘块， 操作系统也可以通过 FAT 对文件存储空间进行管理。 当某进程请求操作系统分配一个磁盘块时，操作系统 FAT（文件分配表）只需从 FAT 中找到 `-2` 的表项，并将对应的磁盘块分配给进程即可。
        
        FAT 表在系统启动时就会被读入内存，因此查找记录的过程是在内存中进行的，因而不仅显著地提高了检索速度，而且明显减少了访问磁盘的次数。
        
- **5.2.3 索引分配（索引组织方式）**
    
    在对文件采取索引组织方式时，所形成的将是索引式文件结构。在传统的文件系统中，通常仅采用其中的一种组织方式来组织文件。在现代 OS 中，由于存在着多种类型的、特别是实时类型的多媒体文件，因此，对文件可能采取了多种类型的组织形式。
    
    链接分配解决了连续分配的外部碎片和文件大小管理的问题。但依然存在问题：链接分配不能有效支持直接访问（FAT 除外）；FAT 需要占用较大的内存空间。事实上，在打开某个文件时，只需将该文件对应盘块的编号调入内存即可，完全没有必要将整个 FAT 调入内存。为此，索引分配将每个文件所有的盘块号都集中放在一起构成索引块（表），如图所示。
    
    ![索引分配](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2068.png)
    
    索引分配
    
    每个文件都有其索引块，这是一个磁盘块地址的数组。索引块的第 i 个条目指向文件的第 i 个块。要读第 i 块，通过索引块的第 i 个条目的指针来查找和读入所需的块。
    
    索引分配的优点是支持直接访问，且没有外部碎片问题。缺点是由于索引块的分配，增加了系统存储空间的开销。索引块的大小是一个重要的问题，每个文件必须有一个索引块，因此索引块应尽可能小，但索引块太小就无法支持大文件。可以采用以下机制来处理这个问题：
    
    1. 链接方案
        
        如果所分配出去的盘块的盘块号已经装满一个索引块 时， OS 会为该文件分配另一个索引块，用于将以后继续为之分配的盘块号记录于其中。依此类推，再通过链指针将各索引块按序链接起来。显然，当文件太大，其索引块太多时，这种 方法是低效的。
        
        ![链接方案](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2069.png)
        
        链接方案
        
    2. 多级索引
        
        通过第一级索引块指向一组第二级的索引块 ，第二级索引块再指向文件块。查找时，通过第一级索引查找第二级索引，再采用这个第二级索引查找所需数据块。这种方法根据最大文件大小，可以继续到第三级或第四级。例如，4KB 的块，能在索引块中存 入 1024 个 4B 的指针。两级索引支持 1048576 （$\small 2^{20}$）个数据块，即支持最大文件为 4GB。
        
        ![多级索引](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2070.png)
        
        多级索引
        
        多级索引的主要优点是：大大加快了对大型文件的查找速度。其主要缺点是，在访问一个盘块时，其所需启动磁盘的次数随着索引级数的增加而增多，即使是对于小文件也是如此。实际情况是，通常总是以中、小文件居多，而大文件是较少的。因此可见，如果在文件系统中仅采用了多级索引组织方式，并不能获得理想的效果。
        
    3. 混合索引（增量式索引）
        
        将多种索引分配方式相结合的分配方式。例如，系统既采用直接地址，又采用单级索引分配方式或两级索引分配方式。 此外，访问文件需两次访问外存，先读取索引块的内容，然后访问具体的磁盘块，因而降低 了文件的存取速度。为了解决这一问题，通常将文件的索引块读入内存，以提高访问速度。
        
        为了能够较全面地照顾到小型、中型、大型和特大型文件，可采用混合索引分配方式。对于小文件，为了提高对众多小文件的访问速度，最好能将它们的每个盘块地址直接放入 FCB，这样就可以直接从 FCB 中获得该文件的盘块地址，即为直接寻址。对于中型文件，可以采用单级索引方式，需要先从 FCB 中找到该文件的索引表，从中获得该文件的盘块地址，即为一次间址。对于大型或特大型文件，可以采用两级和三级索引分配方式。UNIX 系统采用的就是这种分配方式，在其索引结点中，共设有 13 个地址项，即 i.addr(0)〜i.addr(12)，如图所示。
        
        ![混合索引 - UNIX系统的 inode 结构示意图](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2071.png)
        
        混合索引 - UNIX系统的 inode 结构示意图
        
        1. 直接地址：为了提高对文件的检索速度，在索引结点中可设置 10 个直接地址项，即用 i.addr(0) ~ i.addr(9) 来存放直接地址，即文件数据盘块的盘块号。假如每个盘块的大小为 4KB，当文件不大于 40KB 时，便可直接从索引结点中读出该文件的全部盘块号。
        2. —次间接地址：对于中、大型文件，只采用直接地址并不现实的。为此，可再利用索引结点中的地址项 i.addr(10) 来提供一次间接地址。这种方式的实质就是一级索引分配方式。 图中的一次间址块也就是索引块，系统将分配给文件的多个盘块号记入其中。在一次间址块中可存放 1024 个盘块号，因而允许文件长达 4MB。
        3. 多次间接地址：当文件长度大于 4MB + 40KB（一次间接地址与 10 个直接地址项）时， 系统还需采用二次间接地址分配方式。这时，用地址项 i.addr(11) 提供二次间接地址。该方式的实质是两级索引分配方式。系统此时在二次间址块中记入所有一次间址块的盘号。 地址项 i.addr(11) 作为二次间址块，允许文件最大长度可达4GB。同理，地址项 iaddr(12) 作为三次间址块，其允许的文件最大长度可达 4TB。
        - Q：文件的物理结构是指一个文件在外存上的存储组织方式，那么何谓文件的混合索引结构？
            
            混合索引结构是将多种索引分配方式相结合的分配方式。系统既采用直接地址，又采用单级索引分配方式或两级索引分配方式。 
            
- **5.2.4 分配方式对比**
    
    
    | 分配方式 | 实现 | 目录项内容 | 优点 | 缺点 |
    | --- | --- | --- | --- | --- |
    | 连续分配 | 为文件分配的必须是连续 的磁盘块 | 起始块号、文件长 度 | 顺序访问容易；
    顺序存取速度快；
    支持随机访问。 | 要求为一个文件分配连续的存储空间；
    会产生碎片，磁盘利用率低；
    不利于文件拓展；
     |
    | 链接分配 - 隐式链接 | 除文件的最后一个盘块之 外，每个盘块中都存有指 向下一个盘块的指针 | 起始块号、结束块号 | 可解决碎片问题，外存利用率高；
    文件拓展实现方便。 | 只能顺序访问，不能随机访问。 |
    | 链接分配 - 显式链接 | 建立一张文件分配表（FAT）， 显式记录盘块的先后关系 （开机后 FAT 常驻内存） | 起始块号 | 除了拥有隐式链接的优点之外，还可通过查询内存中的 FAT 实现随机访问 | FAT 需要占用一定的存储空间 |
    | 索引分配 | 为文件数据块建立索引表。 若文件太大，可采用链接方案、多级索引、混合索引 | 链接方案记录的是第一个索引块的块号，多级/混合索引记录的是顶级索引块的块号 | 支持顺序访问和随机访问；
    查找效率高；
    易于实现文件的拓展；
    不会产生外部碎片。 | 索引表需占用一定的存储空间。访问数据块前需要先读入索引块。若采用链接方案，查找索引块时可能需要很多次读磁盘操作。 |

### 5.3 目录管理：单级目录、二级目录、多级目录 ★★

- **5.3.1 单级目录结构**
    
    在整个文件系统中只建立一张目录表，每个文件占一个目录项。当访问一个文件时，先按文件名在该目录中查找到相应的 FCB，经合法性检查后执行相应的操作。当建立一个新文件时，必须先检索所有目录项，以确保没有“重名”的情况，然后在该目 录中增设一项，把新文件的属性信息填入到该项中。当删除一个文件时，先从该目录中找到该文件的目录项，回收该文件所占用的存储空间，然后清除该目录项。
    
    单级目录结构实现了 “按名存取”，但是存在查找速度慢、文件不允许重名、不便于文件共享等缺点，而且对于多用户的操作系统显然是不适用的。
    
    ---
    
- **5.3.2 两级目录结构**
    
    为了克服单级目录所存在的缺点，可以将文件目录分成主文件目录（Master File Directory, MFD）和用户文件目录（User File Directory, UFD）两级。主文件目录项记录用户名及相应用户文件目录所在的存储位置。用户文件目录项记录该用户文件的 FCB 信息。当某用户欲对其文件进行访问时，只需搜索该用户对应的 UFD，这既解决了不同用户文件的“重名”问题，又在一定程度上保证了文件的安全。
    
    两级目录结构提高了检索的速度，解决了多用户之间的文件重名问题，不同用户还可使用不同的文件名访问系统中的同一个共享文件，文件系统也可以在目录上实现访问限制。但是两级目录结构缺乏灵活性，不能对文件分类，不便于用户之间共享彼此的文件等。
    
    ---
    
- **5.3.2 多级（树形）目录结构**
    
    将两级目录结构加以推广，就形成了树形目录结构。它可以明显地提高对目录的检索速度和文件系统的性能。当用户要访问某个文件时，用文件的路径名标识文件，文件路径名是个字符串，由从根目录出发到所找文件通路上所有目录名与数据文件名用分隔符“/”链 接而成。从根目录出发的路径称为绝对路径。当层次较多时，每次从根目录查询会浪费时间，于是加入了当前目录（又称工作目录）加快检索速度，进程对各文件的访问都是相对于当前目录进行的 。当用户要访问某个文件时，使用相对路径标识文件，相对路径由从当前目录岀发到所找文件通路上所有目录名与数据文件名用分隔符“/”链接而成。
    
    树形目录结构可以很方便地对文件进行分类，层次结构清晰，也能够更有效地进行文件的管理和保护。在树形目录中，不同性质、不同用户的文件，分别呈现在系统目录树的不同层次或不同子树中，可以容易地赋予不同的存取权限。但是，在树形目录中查找一个文件，需要按路径名逐级访问中间结点，增加了磁盘访问次数，这无疑会影响查询速度。目前，大多数操作系统如 UNIX、 Linux 和 Windows 系统都采用了树形文件目录。
    
    ---
    

### 5.4 文件存储空间的管理技术：位示图、空闲链表、索引 ★★★

- **5.4.1 空闲表法**
    
    空闲表法属于连续分配方式，它与内存的动态分配方式类似，为每个文件分配一块连续的存储空间。系统为外存上的所有空闲区建立一张空闲盘块 表，每个空闲区对应于一个空闲表项，其中包括表项序号、 该空闲区的第一个盘块号、该区的空闲盘块数等信息。再将所有空闲区按其起始盘块号递增的次序排列，如表所示。
    
    ![空闲盘块表](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2072.png)
    
    空闲盘块表
    
    空闲盘区的分配与内存的动态分配类似，同样采用首次适应算法和最佳适应算法等。例如，在系统为某新创建的文件分配空闲盘块时，先顺序地检索空闲盘块表的各表项，直至找到第一个其大小能满足要求的空闲区，再将该盘区分配给用户，同时修改空闲盘块表。
    
    系统在对用户所释放的存储空间进行回收时，也采取类似于内存回收的方法，即要考虑回收区是否与空闲盘块表中插入点的前区和后区相邻接，对相邻接者应予以合并。
    
    ---
    
- **5.4.2 空闲链表法**
    
    将所有空闲盘区拉成一条空闲链。根据构成链所用基本元素的不同，分为两种形式：
    
    - 空闲盘块链
        
        将磁盘上的所有空闲空间以盘块为单位拉成一条链。当用户因创建文件而请求分配存储空间时，系统从链首开始，依次摘下适当数目的空闲盘块分配给用户。当用户因删除文件而释放存储空间时 ，系统将回收的盘块依次插入空闲盘块链的末尾。这种方法的优点是分配和回收一个盘块的过程非常简单，但在为一个文件分配盘块时可能 要重复操作多次，效率较低。又因它是以盘块为单位的，空闲盘块链会很长。
        
    - 空闲盘区链
        
        将磁盘上的所有空闲盘区（每个盘区可包含若干个盘块）拉成一条链。每个盘区除含有用于指示下一个空闲盘区的指针外，还应有能指明本盘区大小（盘块数） 的信息。分配盘区的方法与内存的动态分区分配类似，通常采用首次适应算法。在回收盘区时，同样也要将回收区与相邻接的空闲盘区合并。这种方法的优缺点刚好与第一种方法的相反，即分配与回收的过程比较复杂，但效率通常较高，且空闲盘区链较短。
        
    
    ---
    
- **5.4.3 位示图法**
    
    位示图是利用二进制的一位来表示磁盘中一个盘块的使用情况，磁盘上所有的盘块都有一个二进制位与之对应。当其值为“0”时，表示对应的盘块空闲；为“1”时，表示已分配。这样，一个 $\small m×n$ 位组成的位示图就可用来表示 $\small m×n$ 个盘块的使用情况，如图所示。
    
    ![位示图法示意图](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2073.png)
    
    位示图法示意图
    
    注意：这里的位示图行和列都从 1 开始编号，若题目中指明从 0 开始编号，则下述计算方法要进行相应调整。
    
    1. 盘块的分配
        - ① 顺序扫描位示图，从中找出一个或一组其值为“0”的二进制位。
        - ② 将找到的一个或一组二进制位，转换成与之对应的盘块号。若找到的其值为“0”的二进制位位于位示图的第 i 行、第 j 列，则其相应的盘块号 b 为：$\small b = n(i-1)+j$（n 为每行位数）。
        - ③ 修改位示图，令  $\small map[i,j] = 1$。
    2. 盘块的回收
        - ① 将回收盘块的盘块号转换成位示图中的行号和列号。转换公式为：
            - $\small i  = (b - 1) / n + 1$
            - $\small j = (b - 1) \% n + 1$
        - ② 修改位示图，令  $\small map[i,j] = 0$。
    
    这种方法的主要优点是从位示图中很容易找到一个或一组相邻接的空闲盘块。此外，由于位示图很小，占用空间少，因而可将它保存在内存中，进而使在每次进行盘区分配时，无需首先把盘区分配表读入内存，从而节省了许多磁盘的启动操作。
    
    ---
    
- **5.4.4 成组链接法**
    
    在 UNIX 系统中采用的是成组链接法，这种方法结合了空闲表和空闲链表两种方法，它具有上述两种方法的优点，克服了两种方法均有的表太长的缺点。
    
    用来存放一组空闲盘块号（空闲盘块的块号）的盘块称为成组链块。成组链接法的大致思想是：把顺序的 n 个空闲盘块号保存在第一个成组链块中 ，其最后一个空闲盘块（作为成组链块） 则用于保存另一组空闲盘块号，如此继续，直至所有空闲盘块均予以链接。系统只需保存指向第一个成组链块的指针。假设磁盘最初全为空闲盘块，其成组链接如图所示。
    
    ![空闲盘块的成组链接法](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%80%83%E7%BA%B2%E7%9F%A5%E8%AF%86%E7%82%B9%20d6789a488c934a009190634bba8e1811/Untitled%2074.png)
    
    空闲盘块的成组链接法
    
    1. 盘块的分配
        
        根据第一个成组链块的指针，将其对应的盘块分配给用户，然后将指针下移一格。若该指针指向的是最后一个盘块（即成组链块），由于该盘块记录的是下一组空闲盘块号，因此要将该盘块读入内存，并将指针指向新的成组链块的第一条记录 ，然后执行上述分配操作。
        
    2. 盘块的回收
        
        成组链块的指针上移一格，再记入回收盘块号。当成组链块的链接数达到 n 时，表示已满， 便将现有已记录 n 个空闲盘块号的成组链块号记入新回收的盘块（作为新的成组链块）。
        
        表示空闲空间的位向量表或第一个成组链块，以及卷中的目录区、文件区划分信息都要存放在磁盘中，一般放在卷头位置，在 UNIX 系统中称为超级块。在对卷中的文件进行操作前，超级块需要预先读入系统空闲的主存，并且经常保持主存超级块与磁盘卷中超级块的一致性。
        
    
    ---
    

## 6. 补充知识点

注：补充知识点是指以往 820 考卷涉及到但未在考纲中列出的知识点

### 6.1 操作系统接口

- **6.1.2 Shell 命令语言**
    
    在 Linux 系统中，Shell 是命令语言、命令解释器（程序）及程序设计语言的统称，其特点如下：
    
    1. 作为命令语言，它拥有自己内建的 Shell 命令集，可以为用户提供使用操作系统的接口，用户利用该接口与机器交互。  
    2. 作为一种程序设计语言，它支持绝大多数在高级语言中能见到的程序元素，如函数、变量、数组和程序控制结构。
    3. 作为一个命令解释器（程序）， Shell 可对输入的命令解释执行。
    
    ---
    
- **6.1.3 系统调用**
    
    程序接口，是 OS 专门为用户程序设置的，提供给程序员在编程时使用，也是用户程序取得 OS 服务的唯一途径。它是由一组系统调用（system call)组成，因而，也可以说，系统调用提供了用户程序和操作系统内核之间的接口。系统调用不仅可供所有的应用程序使用，而且也可供 OS 自身使用。
    
    1. 系统态和用户态
        
        在计算机系统中设置了两种状态：系统态（或称为核心态、管态）和用户态（目态）。在实际运行过程中，处理机会在系统态和用户态间切换。相应地，现代多数 OS 将 CPU 的指令集分为特权指令和非特权指令两类。
        
        - ① 特权指令。特权指令是指在系统态运行的指令，它对内存空间的访问范围基本不受限 制，不仅能访问用户空间，也能访问系统空间。如启动外部设备、设置系统时钟时间、关中 断、转换执行状态等。特权指令只允许 OS 使用，不允许应用程序使用，以避免引起系统混乱。
        - ② 非特权指令。非特权指令是在用户态运行的指令。应用程序所使用的都是非特权 指令，它只能完成一般性的操作和任务，不能对系统中的硬件和软件直接进行访问，对内 存的访问范围也局限于用户空间。这样，可以防止应用程序的运行异常对系统造成破坏。 这种限制是由硬件实现的，如果在应用程序中使用了特权指令，就会发出权限出错信 号，操作系统捕获到这个信号后，将转入相应的错误处理程序，将停止该应用程序的运行， 重新调度。

### 6.1 中断和异常

### 6.2 内核态（管态）和用户态（目态）

TODO：已有试题考了用户态和内核态的切换

TODO：进程切换整个过程描述

进程上下文切换由以下4个步骤组成:

- ① 决定是否作上下文切换以及是否允许作上下文切换。包括对进程调度原因的检查分析，以及当前执行进程的资格和CPU执行方式的检查等。在操作系统中，上下文切换程序并不是每时每刻都在检查和分析是否可作上下文切换，它们设置有适当的时机。
- ② 保存当前执行进程的上下文。这里所说的当前执行进程，实际上是指调用[上下文切换](https://baike.baidu.com/item/%E4%B8%8A%E4%B8%8B%E6%96%87%E5%88%87%E6%8D%A2?fromModule=lemma_inlink)程序之前的执行进程。如果[上下文切换](https://baike.baidu.com/item/%E4%B8%8A%E4%B8%8B%E6%96%87%E5%88%87%E6%8D%A2?fromModule=lemma_inlink)不是被那个当前执行进程所调用，且不属于该进程，则所保存的上下文应是先前执行进程的上下文，或称为“老”[进程上下文](https://baike.baidu.com/item/%E8%BF%9B%E7%A8%8B%E4%B8%8A%E4%B8%8B%E6%96%87?fromModule=lemma_inlink)。显然，[上下文切换](https://baike.baidu.com/item/%E4%B8%8A%E4%B8%8B%E6%96%87%E5%88%87%E6%8D%A2?fromModule=lemma_inlink)程序不能破坏“老”进程的上下文结构。
- ③ 使用[进程调度](https://baike.baidu.com/item/%E8%BF%9B%E7%A8%8B%E8%B0%83%E5%BA%A6?fromModule=lemma_inlink)算法，选择一处于就绪状态的进程。
- ④ 恢复或装配所选进程的上下文，将CPU控制权交到所选进程手中。

<aside>
📚 参考书目

- 《820 计算机专业基础》电子科大知博书店
- 《计算机操作系统》
- 《王道操作系统》
</aside>